<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[minimaxir | Max Woolf's Blog]]></title>
  <link href="http://minimaxir.com/rss.xml" rel="self"/>
  <link href="http://minimaxir.com/"/>
  <updated>2016-05-27T08:02:26-07:00</updated>
  <id>http://minimaxir.com/</id>
  <author>
    <name><![CDATA[Max Woolf]]></name>
    <email><![CDATA[max@minimaxir.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Create a Network Graph Visualization of Reddit Subreddits]]></title>
    <link href="http://minimaxir.com/2016/05/reddit-graph/"/>
    <updated>2016-05-27T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2016/05/reddit-graph</id>
    <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Graph_theory">Network graphs</a> are pretty data visualizations, and I like pretty data visualizations. Recently, <a href="https://www.reddit.com">Reddit</a> user CuriousGnu <a href="http://www.curiousgnu.com/reddit-comments">posted a network graph</a> of the comment patterns of the top 50 Reddit subreddits:</p>

<p><img src="/img/reddit-graph/rd_comments_net_hd.png" alt=""></p>

<p>The <a href="https://www.reddit.com/r/dataisbeautiful/comments/4fsrjd/oc_redditors_who_commented_in_rx_also_commented/">visualization</a> was made with <a href="https://gephi.org">Gephi</a>, a very popular free and open-source network graph tool.</p>

<p><img src="/img/reddit-graph/gephi.png" alt=""></p>

<p>Gephi is <em>extremely</em> difficult to use, and most blog posts  about the software are in the form of Step 1: Gephi, Step 2: ???, Step 3: Profit. Even if you know <em>do</em> how to use it, most of the network design customizations must be done manually, which is not helped by software slowness even on high-end machines. My own attempts to use Gephi for nice-looking networks have had <a href="https://www.reddit.com/r/dataisbeautiful/comments/3z60z6/network_of_reddit_commenting_patterns_for_the_top/">mixed</a> <a href="https://www.reddit.com/r/magicTCG/comments/401hdq/graph_network_of_magic_the_gathering_creature/">results</a>.</p>

<p>Additionally, there is very little discussion on how to gather the data for large-scale network graph visualizations, and how to make them in a <em>reproducible</em> manner. It is time to fix that and create a Reddit network graph visualization with many more nodes, step by step.</p>

<h2>Getting Reddit Edge Data</h2>

<p>Network graphs are typically formed by getting the relationship data between two entities (the edges), then extrapolating the vertices of the graph (the nodes) from that data.</p>

<p>There are two common data structures for representing edge data. One is an <a href="https://en.wikipedia.org/wiki/Adjacency_matrix">adjacency matrix</a>, which is a 2D matrix where the rows/columns represent the entities, and the value at the intersection between a row/column represents the <em>weight</em> of the relationships. For the visualization above, CuriousGnu made an adjacency matrix by querying the relationships from <a href="https://cloud.google.com/bigquery/">BigQuery</a> for each subreddit manually. That requires adding a line of SQL for <em>each</em> subreddit you want to plot, which is time-consuming and I am lazy.</p>

<p>Let&rsquo;s try option #2: an <a href="https://reference.wolfram.com/language/ref/EdgeList.html">edge list</a>, which is a tabular dataset where each row contains the two entities and a weight. With clever use of BigQuery, we can query the edges for <em>every single subreddit</em> at the same time. And we can query on real-time Reddit data from approximately the past 6 months using Jason Baumgartner&rsquo;s <a href="https://pushshift.io/using-bigquery-with-reddit-data/">Reddit dataset</a> on BigQuery. </p>

<p>The process works like this:</p>

<ol>
<li>Determine active users of a subreddit by identifying the subreddits where a user has <strong>commented</strong> on at least <strong>5 different submissions</strong> within the past 6 months.</li>
<li>Perform a <a href="http://stackoverflow.com/questions/3362038/what-is-self-join-and-when-would-you-use-it">self-join</a> by joining the table on itself: this will create <strong>links</strong> between all subreddits where a given user is active. (e.g. an active user of /r/askreddit, /r/pics, and /r/gifs will form 9 links: askreddit → askreddit, askreddit → pics, askreddit → gifs, pics → askreddit, etc.)</li>
<li>Aggregate the counts of the number of links between two subreddits; this will become the edge <strong>Weight</strong>.</li>
<li>Filter the resulting dataset by removing self-loops and reverse-edges. (e.g. since we have askreddit → pics, remove pics → askreddit). Additionally, we should only retain edges with <strong>at least 200 active users</strong> to keep the resulting dataset a manageable size for this analysis.</li>
</ol>

<p>Putting it all together results in this query:</p>
<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">a</span><span class="p">.</span><span class="n">l_subreddit</span> <span class="k">as</span> <span class="k">Source</span><span class="p">,</span> <span class="n">b</span><span class="p">.</span><span class="n">l_subreddit</span> <span class="k">as</span> <span class="n">Target</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">as</span> <span class="n">Weight</span>
<span class="k">FROM</span> <span class="p">(</span>
  <span class="k">SELECT</span> <span class="n">author</span><span class="p">,</span> <span class="k">LOWER</span><span class="p">(</span><span class="n">subreddit</span><span class="p">)</span> <span class="k">as</span> <span class="n">l_subreddit</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="p">(</span><span class="n">link_id</span><span class="p">))</span> <span class="k">as</span> <span class="n">unique_threads</span>
  <span class="k">FROM</span> <span class="p">[</span><span class="n">pushshift</span><span class="p">:</span><span class="n">rt_reddit</span><span class="p">.</span><span class="n">comments</span><span class="p">]</span>
  <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">author</span><span class="p">,</span> <span class="n">l_subreddit</span>
  <span class="k">HAVING</span> <span class="n">unique_threads</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">)</span> <span class="n">a</span> <span class="k">JOIN</span> <span class="p">(</span>
  <span class="k">SELECT</span> <span class="n">author</span><span class="p">,</span> <span class="k">LOWER</span><span class="p">(</span><span class="n">subreddit</span><span class="p">)</span> <span class="k">as</span> <span class="n">l_subreddit</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="p">(</span><span class="n">link_id</span><span class="p">))</span> <span class="k">as</span> <span class="n">unique_threads</span>
  <span class="k">FROM</span> <span class="p">[</span><span class="n">pushshift</span><span class="p">:</span><span class="n">rt_reddit</span><span class="p">.</span><span class="n">comments</span><span class="p">]</span>
  <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">author</span><span class="p">,</span> <span class="n">l_subreddit</span>
  <span class="k">HAVING</span> <span class="n">unique_threads</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">)</span> <span class="n">b</span> <span class="k">ON</span> <span class="n">a</span><span class="p">.</span><span class="n">author</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="n">author</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="k">Source</span><span class="p">,</span> <span class="n">Target</span>
<span class="k">HAVING</span> <span class="k">Source</span> <span class="o">&lt;</span> <span class="n">Target</span> <span class="k">AND</span> <span class="n">Weight</span> <span class="o">&gt;=</span> <span class="mi">200</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">Weight</span> <span class="k">DESC</span>
</code></pre></div>
<p>Only 13 lines of code, with 3 of those lines repeated. Running the query only takes a few minutes. (which is actually <em>forever</em> in BigQuery time: when people talk about &ldquo;big data,&rdquo; this is <em>actually big data</em>!)</p>

<p>That query (at the time of analysis) returns <a href="https://docs.google.com/spreadsheets/d/1MFHno-sYR3MkWgntnieWobWQ2e3x4CAcNUdVIFjmlQI/edit?usp=sharing">this dataset</a> of 7,498 edges; more than enough. Now for the fun part.</p>

<h2>Visualizing the Reddit Data</h2>

<p>The edge list linked above can actually be imported into Gephi as-is. <strong>Don&rsquo;t</strong>.</p>

<p>Instead, let&rsquo;s use R and my favorite data visualization tool <code>ggplot2</code>, with a twist.</p>

<p>First, we load the edge list into R, and create an undirected network graph using the <code>igraph</code> package.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">net <span class="o">&lt;-</span> graph.data.frame<span class="p">(</span>df<span class="p">,</span> directed<span class="o">=</span><span class="bp">F</span><span class="p">)</span>
</code></pre></div>
<p><img src="/img/reddit-graph/igraph.png" alt=""></p>

<p>The imported edge list results in a network with 1,131 nodes/subreddits. After pruning nodes with only a few neighbors and removing the subsequently-orphaned edges, we get a network of 517 nodes/subreddits with 6,732 edges.</p>

<p>We can then add summary statistics for the nodes, such as the group/community each node belongs to, and the <a href="https://en.wikipedia.org/wiki/Centrality">eigenvector centrality</a> of the node.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">V<span class="p">(</span>net<span class="p">)</span><span class="o">$</span>group <span class="o">&lt;-</span> membership<span class="p">(</span>cluster_walktrap<span class="p">(</span>net<span class="p">,</span> weights<span class="o">=</span>E<span class="p">(</span>net<span class="p">)</span><span class="o">$</span>Weight<span class="p">))</span>
V<span class="p">(</span>net<span class="p">)</span><span class="o">$</span>centrality <span class="o">&lt;-</span> eigen_centrality<span class="p">(</span>net<span class="p">,</span> weights<span class="o">=</span>E<span class="p">(</span>net<span class="p">)</span><span class="o">$</span>Weight<span class="p">)</span><span class="o">$</span><span class="kt">vector</span>
</code></pre></div>
<p>Convert the network to a dataframe suitable for plotting using the <code>ggnetwork</code> library.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">df_net <span class="o">&lt;-</span> ggnetwork<span class="p">(</span>net<span class="p">,</span> layout <span class="o">=</span> <span class="s">&quot;fruchtermanreingold&quot;</span><span class="p">,</span> weights<span class="o">=</span><span class="s">&quot;Weight&quot;</span><span class="p">,</span> niter<span class="o">=</span><span class="m">50000</span><span class="p">)</span>
</code></pre></div>
<p>Now time for ggplot2/ggnetwork fun. In this case, we will color the nodes whether or not they are a default subreddit (orange if default, blue otherwise) and color the lines accordingly (orange if either end is a default subreddit, blue otherwise).</p>

<p>Yes, writing and optimizing all of this code is <em>significantly</em> easier than using Gephi, believe it or not.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">default_colors<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="s">&quot;#3498db&quot;</span><span class="p">,</span> <span class="s">&quot;#e67e22&quot;</span><span class="p">)</span>
default_labels<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="s">&quot;Not Default&quot;</span><span class="p">,</span> <span class="s">&quot;Default&quot;</span><span class="p">)</span>

ggplot<span class="p">(</span>df_net<span class="p">,</span> aes<span class="p">(</span>x <span class="o">=</span> x<span class="p">,</span> y <span class="o">=</span> y<span class="p">,</span> xend <span class="o">=</span> xend<span class="p">,</span> yend <span class="o">=</span> yend<span class="p">,</span> size <span class="o">=</span> centrality<span class="p">))</span> <span class="o">+</span>
    geom_edges<span class="p">(</span>aes<span class="p">(</span>color <span class="o">=</span> connectDefault<span class="p">),</span> size<span class="o">=</span><span class="m">0.05</span><span class="p">)</span> <span class="o">+</span>
    geom_nodes<span class="p">(</span>aes<span class="p">(</span>fill <span class="o">=</span> defaultnode<span class="p">),</span> shape <span class="o">=</span> <span class="m">21</span><span class="p">,</span> stroke<span class="o">=</span><span class="m">0.2</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;black&quot;</span><span class="p">)</span> <span class="o">+</span>
    geom_nodelabel_repel<span class="p">(</span>data<span class="o">=</span>df_net<span class="p">,</span> aes<span class="p">(</span>color <span class="o">=</span> defaultnode<span class="p">,</span> label <span class="o">=</span> vertex.names<span class="p">),</span>
                          fontface <span class="o">=</span> <span class="s">&quot;bold&quot;</span><span class="p">,</span> size<span class="o">=</span><span class="m">0.5</span><span class="p">,</span> box.padding <span class="o">=</span> unit<span class="p">(</span><span class="m">0.05</span><span class="p">,</span> <span class="s">&quot;lines&quot;</span><span class="p">),</span>
                          label.padding<span class="o">=</span> unit<span class="p">(</span><span class="m">0.1</span><span class="p">,</span> <span class="s">&quot;lines&quot;</span><span class="p">),</span> segment.size<span class="o">=</span><span class="m">0.1</span><span class="p">,</span> label.size<span class="o">=</span><span class="m">0.2</span><span class="p">)</span> <span class="o">+</span>
    scale_color_manual<span class="p">(</span>values<span class="o">=</span>default_colors<span class="p">,</span> labels<span class="o">=</span>default_labels<span class="p">,</span> guide<span class="o">=</span><span class="bp">F</span><span class="p">)</span> <span class="o">+</span>
    scale_fill_manual<span class="p">(</span>values<span class="o">=</span>default_colors<span class="p">,</span> labels<span class="o">=</span>default_labels<span class="p">)</span> <span class="o">+</span>
    ggtitle<span class="p">(</span><span class="s">&quot;Network Graph of Reddit Subreddits (by @minimaxir)&quot;</span><span class="p">)</span> <span class="o">+</span>
    scale_size<span class="p">(</span>range<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">0.1</span><span class="p">,</span> <span class="m">4</span><span class="p">))</span> <span class="o">+</span> 
    theme_blank<span class="p">()</span>
</code></pre></div>
<p><object data="/img/reddit-graph/subreddit-1.pdf" type="application/pdf" width="100%" height="400px"></object></p>

<p><span class="hidden-lg"><em>If you are on a smartphone or tablet, tap <a href="/img/reddit-graph/subreddit-1.pdf" target="_blank">this link</a> to view the network in a zoomable format.</em></span></p>

<p>The large networks in the blog post are rendered as a PDF, which allows for easy pan/zooming at a very low file size (284KB!), while SVG/<a href="https://d3js.org">d3</a>/<a href="http://sigmajs.org">sigma.js</a> approaches have very poor performance at large numbers of nodes/edges.</p>

<p>As we expect, the default subreddits are in the center of the network graph and have high centrality (although /r/art and /r/earthporn are oddly far separated from the other defaults). The large amounts of orange graph-wide illustrate the breadth of the defaults.</p>

<p>Now let&rsquo;s color the nodes and edges by group, just as you saw in the introductory visualization:</p>

<p><object data="/img/reddit-graph/subreddit-2.pdf" type="application/pdf" width="100%" height="400px"></object></p>

<p><span class="hidden-lg"><em>If you are on a smartphone or tablet, tap <a href="/img/reddit-graph/subreddit-2.pdf" target="_blank">this link</a> to view the network in a zoomable format.</em></span></p>

<p>If an edge links to a node of the same group, the edge is colored that group. Otherwise, the edge is colored gray. (the code that implements this is not shown because it is somewhat convoluted). This color scheme helps gauge the overall impact of the communities on Reddit. But why not look at specific groups?</p>

<h2>Subgraph Surprises</h2>

<p>As you can see plainly in the group-colored visualization, there is a giant green group at the center which includes the default subreddits. Analyzing that is not helpful. But we can filter the network on other specific groups and their subgraphs to see if we can define any Reddit subcultures. (note that the Group number is merely an ID; the value and order are not relevant).</p>

<p>The most notable Reddit groups are gaming groups. We have two distinct groups of gamers:</p>

<p><img src="/img/reddit-graph/group-006.png" alt=""></p>

<p><img src="/img/reddit-graph/group-008.png" alt=""></p>

<p>Plus Nintendo gamers? With a little Vita on the side?</p>

<p><img src="/img/reddit-graph/group-010.png" alt=""></p>

<p>Subreddits related to sports and sporting teams form a nice cluster:</p>

<p><img src="/img/reddit-graph/group-001.png" alt=""></p>

<p>PC-building has a distinct community:</p>

<p><img src="/img/reddit-graph/group-011.png" alt=""></p>

<p>The British make nice triangles!</p>

<p><img src="/img/reddit-graph/group-022.png" alt=""></p>

<p>Relationship and female-oriented subreddits have a relationship. </p>

<p><img src="/img/reddit-graph/group-007.png" alt=""></p>

<p>Lastly, DC Comics has their own sector, particularly with the corresponding CW television shows. (although some Marvel shows sneak in!)</p>

<p><img src="/img/reddit-graph/group-003.png" alt=""></p>

<p>Of course, Reddit itself has better data for identifying relationships between subreddits, as they can track user activity more intimately. Meanwhile, the output for this post turned out better than expected and I hope to include similar visualizations in future blog posts. Hopefully, it dispelled some of the mystery behind pretty network graphs. (if you do use the code or data visualization designs from this post, it would be greatly appreciated if proper attribution is given back to this post and/or myself. Thanks!).</p>

<hr>

<p><em>As always, the full code used to process the edge list and generate the visualizations is available in <a href="https://github.com/minimaxir/reddit-graph/blob/master/subreddit_network_pdf.ipynb">this Jupyter notebook</a>, open-sourced <a href="https://github.com/minimaxir/reddit-graph">on GitHub</a>.</em></p>

<p><em>Additionally, thanks to Professor James P. Curley of Columbia University for providing <a href="http://curleylab.psych.columbia.edu/netviz/netviz1.html#/">helpful slides</a> which have good code samples for getting started with igraph/ggnetwork.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating Stylish, High-Quality Word Clouds Using Python and Font Awesome Icons]]></title>
    <link href="http://minimaxir.com/2016/05/wordclouds/"/>
    <updated>2016-05-09T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2016/05/wordclouds</id>
    <content type="html"><![CDATA[<p>You&rsquo;ve probably seen word clouds around the internet. There are several popular free tools for creating them, such as <a href="http://www.wordle.net">Wordle</a>. I myself am a fan of them, and I have made them for previous posts using the <a href="http://www.inside-r.org/packages/cran/wordcloud/docs/wordcloud">wordcloud package</a> for R.</p>

<p><img src="/img/gif-unlimited/reddit-gif-wordcloud-e.png" alt=""></p>

<p>Word clouds are not the most scientific type of data visualization. However, they are a very <em>information-dense</em> representation of the frequency of all words in a given text. Word clouds are more effective than just using bar charts displaying the counts of words for large amounts of text, as the chart would be difficult to parse if there are too many bars.</p>

<p>The <a href="https://github.com/amueller/word_cloud">Python word_cloud package</a> by Andreas Mueller is relatively popular. A Reddit bot <a href="https://github.com/Winneon/makeswordclouds">makeswordclouds</a> by Jesse Bryan automatically <a href="https://www.reddit.com/user/makeswordcloudsagain">generates a word cloud</a> of comments on <a href="https://www.reddit.com">Reddit</a> submissions using this package. However, when I first saw the example output on the package, I was not impressed.</p>

<p><img src="/img/wordclouds/constitution.png" alt=""></p>

<p>I did more research into the <a href="http://amueller.github.io/word_cloud/">package documentation</a>. I found that there are two important perks present the Python implementation:</p>

<p>1) Python word_cloud allows the user to specify a mask to constrain the distribution of words.</p>

<p><img src="/img/wordclouds/a_new_hope_1.png" alt=""></p>

<p>2) In addition to the mask, Python word_cloud allows the user to use the original colors of the image to set the colors of the words.</p>

<p><img src="/img/wordclouds/colored_2.png" alt=""></p>

<p>The masks are the more interesting aspect for creating visualizations, but <em>where</em> do you get the masks? You can manually trace and extract objects from images, but that can be time consuming and the masks will likely be heavily aliased and at a low resolution (the size of the mask sets the size of the word cloud).</p>

<p>Enter <a href="https://fortawesome.github.io/Font-Awesome/">Font Awesome</a>, an icon font by Dave Gandy which is <em>very</em> widely used throughout the Internet (including this website). Icon fonts contain a wide variety of shapes and are vectorized, and therefore they can scale to any size.</p>

<p><img src="/img/wordclouds/fa_icons.png" alt=""></p>

<p>So why not use Font Awesome icons as masks for the word cloud? The font icons need to be extracted and rasterized as an image in order to be usable with the Python word_cloud package: cue the Python script <a href="https://github.com/Pythonity/icon-font-to-png">Icon Font to PNG</a> by Pythonity which does what the name implies.</p>

<p><img src="/img/wordclouds/fa_icons_finder.png" alt=""></p>

<p>Now <em>every</em> Font Awesome icon can be used as a word cloud mask! And the icons can be exported at any size: for this post, I render the word clouds at <strong>2048x2048px</strong>, larger than most desktop screens! After hacking the Python scripts included with the package which were used to create the default word clouds, I managed to create a few interesting examples.</p>

<h2>Reddit Data and Thematic Icons</h2>

<p>Font Awesome has <a href="http://fortawesome.github.io/Font-Awesome/icon/line-chart/">icons for charts</a>, which logically appeals to me as a data person. Why not make a word cloud which looks like a line chart?</p>

<p>Let&rsquo;s use the word counts of titles of submissions to the <a href="https://www.reddit.com/r/dataisbeautiful/">/r/dataisbeautiful subreddit</a> on Reddit which have scored at least 100 points (using the <a href="https://bigquery.cloud.google.com/table/fh-bigquery:reddit_posts.full_corpus_201512">Reddit data dump</a> located on <a href="https://cloud.google.com/bigquery/">BigQuery</a>).</p>

<p>Additionally, we can improve on the design of the default word cloud output by forcing all-caps text and by changing the text font. For word clouds, I prefer to use condensed font families, as they can allow for more information to be displayed in the word cloud. In this example, I will be using the <a href="https://www.myfonts.com/fonts/paratype/din-condensed/">DIN Condensed</a> font, a font native to OS X and a font you&rsquo;ve likely seen in media advertisements and website logos.</p>

<p>Putting it all together:</p>

<p><a href="https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/dataisbeautiful_wordcloud.png"><img src="/img/wordclouds/dataisbeautiful_wordcloud.png" alt=""></a></p>

<p><em>(All word clouds in this post are shrunk to 600x600px to reduce loading time: click on the image for the full 2048x2048px resolution.)</em></p>

<h2>GitHub Data and Brand Icons</h2>

<p>Font Awesome also contains icons representing the logos of popular internet brands, such as Facebook and Twitter.</p>

<p><a href="https://github.com">GitHub</a> is another such website. I will use BigQuery again with the <a href="https://bigquery.cloud.google.com/table/githubarchive:year.2014">2014 GitHub Archive dataset</a> to gather word counts of git commit messages during that year, and use the modern GitHub logo as the mask. This time, I will incorporate the freeware condensed monospaced font <a href="https://www.fontsquirrel.com/fonts/M-1m">M+ 1m</a> in order to create a more code-like aesthetic, which creates an interesting look when juxtaposed with the negative space of GitHub&rsquo;s logo.</p>

<p><a href="https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/github_wordcloud.png"><img src="/img/wordclouds/github_wordcloud.png" alt=""></a></p>

<h2>Yelp Data and Sentiment Icons</h2>

<p>One of the <a href="http://minimaxir.com/2014/09/one-star-five-stars/">earliest word clouds I made</a> was for the <a href="http://www.yelp.com">Yelp</a> reviews dataset from the <a href="https://www.yelp.com/dataset_challenge">Yelp Dataset Challenge</a> to compare and contrast verbiage between 1-star reviews and 5-star reviews. Let&rsquo;s remake those word clouds.</p>

<p>At this point I should mention appropriate color palettes for word clouds since the  rainbows of the stereotypical word clouds can be distracting. I strongly recommend using the <a href="http://colorbrewer2.org">ColorBrewer</a> palettes, helpfully provided for this use case with the <a href="https://github.com/jiffyclub/palettable">paletteable Python library</a> by Matt Davis. I particularly like the <a href="https://jiffyclub.github.io/palettable/colorbrewer/sequential/">sequential palettes</a>, which follow a clean gradient between white and another color (or between two or three colors), although I ignore some of the lighter colors as they may not be visible against white backgrounds.</p>

<p>The font choice this time is <a href="https://www.google.com/fonts/specimen/Open+Sans+Condensed">Open Sans Condensed</a>, a Google Font. <a href="https://www.google.com/fonts">Google Fonts</a> are free and open source. I strongly recommend using them for documents/websites to add some flair over default fonts.</p>

<p>Using the <strong>Greens</strong> palette and a smiley-face Font Awesome icon on 5-star reviews:</p>

<p><a href="https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/yelp_pos_wordcloud.png"><img src="/img/wordclouds/yelp_pos_wordcloud.png" alt=""></a></p>

<p>Makes sense, although the thin lines of the smiley-face causes the font sizes to become constrained. How about the inverse: <strong>Reds</strong>, thumbs-down, and 1-star reviews?</p>

<p><a href="https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/yelp_neg_wordcloud.png"><img src="/img/wordclouds/yelp_neg_wordcloud.png" alt=""></a></p>

<h2>Facebook Data and Etc. Icons</h2>

<p>I recently updated my <a href="https://github.com/minimaxir/facebook-page-post-scraper">Facebook Page Data Scraper</a>, which gathers public posts made by Facebook Pages, to now retrieve total Reaction counts on those posts instead of just Likes.</p>

<p>Why not create a word cloud of news headlines to get a zeitgeist of popular discussion? To do this, I scraped all the public posts from <a href="https://www.facebook.com/cnn/">CNN&rsquo;s Facebook page</a>, and created a word cloud of all CNN headlines to which the posts link. Let&rsquo;s use the Google Font  <a href="https://www.google.com/fonts/specimen/Amatic+SC">Amatic SC</a> which you&rsquo;ve likely seen before in ads, and let&rsquo;s try a <a href="https://jiffyclub.github.io/palettable/colorbrewer/qualitative/">qualitative palette</a>, <strong>Dark2</strong>, to get a &ldquo;rainbow&rdquo; effect without looking gawdy.</p>

<p>And use a flag icon, because why not?</p>

<p><a href="https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/cnn_wordcloud.png"><img src="/img/wordclouds/cnn_wordcloud.png" alt=""></a></p>

<p>Of course, CNN has had fun with the 2016 U.S. Presidential election.</p>

<p>Let&rsquo;s do one more word cloud. We have not yet done a word cloud using the colors-from-original-image technique. Using the underlying <a href="http://matplotlib.org/examples/color/colormaps_reference.html">matplotlib color map</a> for the <strong>Spectral</strong> palette, we can overlay a spatial rainbow which determines the color of the words displayed at that area of the mask:</p>

<p><img src="/img/wordclouds/spectral.jpg" alt=""></p>

<p>Additionally, in order to estimate the importance of the presence of each word in generating Reactions, we can create a word cloud of the words where the words are sized not by count, but by the <strong>average number of Reactions</strong> on Facebook posts referencing CNN headlines containing that word (where the word is used in at least 20 headlines for some statistical correction). And let&rsquo;s try a black background.</p>

<p><a href="https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/cnn_wordcloud_reactions.png"><img src="/img/wordclouds/cnn_wordcloud_reactions.png" alt=""></a></p>

<p>While the word cloud cannot be output in a vectorized format using this method, creating a word cloud at a super-high resolution (even larger than 2048x2048px) is more-than-enough for making typical wall posters and t-shirts.</p>

<p>Word clouds may not have as much explanatory value in the academic sense, but they have <em>persuasive</em> power, which is just as important. At the least, it&rsquo;s another visual technique in my fun bag of visualization tricks to spice up future blog posts. </p>

<p>Performing postprocessing on rendered word clouds can help create especially artsy art, but that discussion best-saved for <a href="https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/starry_night_cnn_weight_12_iterations_500_smooth_5.png">another time</a>.</p>

<hr>

<p><em>You can view the scripts to create the word clouds in this posts in this <a href="https://github.com/minimaxir/stylistic-word-clouds">GitHub repository</a>; the code is more hacky than usual, but it should be clear enough to demonstrate how the raw data was processed in each instance and how the word clouds were rendered. In the future, I hope to create a <a href="http://flask.pocoo.org">Flask app</a> based on these scripts to streamline the creation of word clouds.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Blockbuster Movies with Male Leads Earn More Than Those with Female Leads]]></title>
    <link href="http://minimaxir.com/2016/04/movie-gender/"/>
    <updated>2016-04-13T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2016/04/movie-gender</id>
    <content type="html"><![CDATA[<p>One of the more interesting revelations discovered during the <a href="https://en.wikipedia.org/wiki/Sony_Pictures_Entertainment_hack">2014 Sony Pictures Entertainment hack</a> was that actresses <a href="https://en.wikipedia.org/wiki/Jennifer_Lawrence">Jennifer Lawrence</a> and <a href="https://en.wikipedia.org/wiki/Amy_Adams">Amy Adams</a> made <a href="http://www.thedailybeast.com/articles/2014/12/12/exclusive-sony-hack-reveals-jennifer-lawrence-is-paid-less-than-her-male-co-stars.html">less money than their male costars</a> for the movie <a href="http://www.imdb.com/title/tt1800241/">American Hustle</a>. Specifically, Lawrence and Adams earned 7% of the profits while their male co-stars earned 9%: a 28% increase in pay.</p>

<p>That made me curious: is the discrepancy in pay between male-leads and female-leads justifiable? Do movies with male lead actors generate more box office revenue than movies with female leads? Are movies with male leads <em>better</em> than those with female leads?</p>

<p>Using movie data from <a href="http://www.omdbapi.com">OMDb API</a>, which is sourced from <a href="http://www.imdb.com">IMDb</a> and <a href="http://www.rottentomatoes.com">Rotten Tomatoes</a> data, I found that on average, blockbuster movies with male leads generate 22% more domestic box office revenue than those with female leads, and that this difference is statistically significant.</p>

<h2>Setting Up the Movie Data</h2>

<p><img src="/img/movie-gender/movie-gender-data.png" alt=""></p>

<p>I&rsquo;ve talked about processing the OMDb dataset in <a href="http://minimaxir.com/2016/04/trust-but-verify/">my previous post</a>. For this analysis, I&rsquo;ll be filtering on a specific subset of movies:</p>

<ul>
<li>Movies with <strong>at least $10 million in inflation-adjusted domestic box office revenue</strong>. My <a href="http://minimaxir.com/2016/01/movie-revenue-ratings/">first analysis</a> showed that there is a distinct cluster of movies above the $10M threshold specifically. These blockbusters are also what the public knows and best reflects the perception of the industry.</li>
<li>Movies which were <strong>released in 2000 or later</strong>. There was <a href="http://minimaxir.com/2016/04/trust-but-verify/">missing box office revenue data</a> I had found with earlier years so I would prefer to use more robust data to be safe. Additionally, this avoids the complicated issue of <a href="https://www.reddit.com/r/dataisbeautiful/comments/4bcb6x/john_goodman_is_not_the_greatest_supporting_actor/d17y82k">20th-century gender politics in cinema</a>, which I cannot easily address statistically.</li>
</ul>

<p>After applying the filters and cleaning the data further to eliminate miscoded movies, I have created a dataset of 2,020 movies. No movies were removed as outliers in box office revenue (such as Star Wars VII and Avatar) since several tests failed to identify them as statistical outliers.</p>

<p>I identified the lead actor of each movie, using the first credited actor on the IMDb cast overview (NB: this may lead to counterintuitive behavior in casts with unknown leads; the first credited actor for <a href="http://www.imdb.com/title/tt2488496/">Star Wars: The Force Awakens</a> on IMDb is Harrison Ford, who is not the lead and I corrected it to Daisy Ridley in the data). Then I determined their gender by referencing a few gender/first-name mappings (with thanks to <a href="https://twitter.com/matthew_daniels">Matt Daniels</a> and his great work on <a href="http://polygraph.cool/films/index.html">gender and film dialogue</a>).</p>

<p>In all, the dataset has 467 (23%) of movies with a female lead actor, and 1,553 (77%) movies with a male lead actor. Both counts are more than enough for this analysis.</p>

<p>You can view and download the final dataset <a href="https://docs.google.com/spreadsheets/d/1UMV-6yCjHBveyOcZwiilEm2DWRMjdzAbgdHutcjCn-E/edit?usp=sharing">in this Google Sheet</a>.</p>

<h2>Distribution of Box Office Revenue</h2>

<p>Let&rsquo;s start with simple histograms of the box office data. What are the distributions of the data for each gender?</p>

<p><img src="/img/movie-gender/movie-gender-1.png" alt=""></p>

<p>On average, blockbuster movies with male lead actors generate <strong>$79.8M in revenue</strong>. The distribution, even when log-scaled, is skewed right, with the median being much lower at $49.8M.</p>

<p><img src="/img/movie-gender/movie-gender-2.png" alt=""></p>

<p>On average, blockbuster movies with female lead actors generate <strong>$65.6M in revenue</strong>. The general shape of the distribution is the same as with male lead actors.</p>

<p>Double-checking the math:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">79.8M / 65.6M = 22% increase in average box office revenue for male-lead movies
</code></pre></div>
<p>So it is.</p>

<p>Let&rsquo;s overlay the two distributions after normalizing and smoothing with a <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">2D kernel destiny estimator</a>.</p>

<p><img src="/img/movie-gender/movie-gender-3.png" alt=""></p>

<p>Female movies have a clear mode near its average, but male movies have a flatter distribution, with significantly more movies making 9 figures.</p>

<p>But is the difference between the two averages statistically significant? We can run two statistical tests between the box-office revenues of male-led and female-lead movies:</p>

<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov–Smirnov test</a> for determining if two populations have the same distribution. The null hypothesis is that the two are drawn from the same distribution; the alternative hypothesis is that the distributions are different. We reject the null hypothesis at the 95% level in favor of the alternative if the p-value of the test is less than 0.05. Running the test, <strong>p &lt; 0.01</strong>, so we can say the distributions are statistically different. (the p-values are the same whether the box office revenues are log-transformed or not)</li>
<li>The <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Wilcoxon rank-sum test</a> for determining if the means (averages) of two populations are the same (null hypothesis) or different (alternative hypothesis). This test is used instead of a <em>t</em>-test if the populations are not Normally distributed, which is the case here. Running the test (one-sided, since only checking if a mean is greater), <strong>p &lt; 0.01</strong>, so we can say the two means are statistically different. (again, the p-values are the same whether the box office revenues are log-transformed or not)</li>
</ul>

<p>So we have statistical evidence that male-lead movies generate more money on average than female-led movies. But this claim is very serious, and as a result, we need even more proof.</p>

<h2>The Resampling</h2>

<p>Although 2,020 movies is a fair sample size by statistical standards, some may argue that the movies were chosen too arbitrarily and that there is not enough data to support the conclusions I make above. Enter <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrap resampling</a>, in which we resample the data randomly (with replacement) to generate pseudo-datasets, and then calculate aggregate statistics (e.g. the average) on that simulated dataset. Repeat a large number of times, and we can form confidence intervals for the <em>true average</em> of a given data set.</p>

<p>In this case, we resample the box office revenues, calculate the average box office revenue for both male-led and female-led movies, store the result away and resample the data again, and keep repeating until satisfied.</p>

<p>Here&rsquo;s an animation of the resampling of both averages as the number of trials increases. As you can see, the shape of both distributions stabilize very quickly:</p>

<p><img src="/img/movie-gender/movie_frames.gif" alt=""></p>

<p>And the final plot, at 10,000 repetitions:</p>

<p><img src="/img/movie-gender/movie-gender-10.png" alt=""></p>

<p>The dot on the bottom of each distribution represents the <em>actual</em> sample average value calculated during the analysis earlier, while the line range represents a 95% confidence interval for the true average revenue value for each gender. The distribution of male-led movie averages is more narrow than female-led movies because there are 3 times as many male-led movies in the dataset.</p>

<p>As you can see, <em>the line ranges never intersect</em>. Even in the most favorable scenario at the 95% confidence level, the average  domestic box office revenue for male-led movies will be greater. Specifically, of the 10,000 trials, only 2 trials had the case where female-led movies had equal or greater average revenue than the corresponding male-led movie revenue average from the same resampling; this implies <strong>p &lt; 0.01</strong> for the statistical test on whether the means are same or different.</p>

<p>Interestingly, there&rsquo;s a little overlap between in the distributions, which occurs when there are multiple instances of Star Wars VII in the resampled dataset and its high box office revenue pushes the <em>entire</em> female-lead average up very significantly.</p>

<h2>Gender and Quality</h2>

<p>It is also worth checking if male-led movies are <em>better in quality</em> than female-lead movies, as if that&rsquo;s the case, it might provide a more logical explanation why male-led movies make more money.</p>

<p>Let&rsquo;s check out the distribution of <a href="http://www.rottentomatoes.com">Rotten Tomatoes</a> Tomatometer scores of blockbuster movies.</p>

<p><img src="/img/movie-gender/movie-gender-4.png" alt=""></p>

<p><img src="/img/movie-gender/movie-gender-5.png" alt=""></p>

<p><img src="/img/movie-gender/movie-gender-6.png" alt=""></p>

<p>There&rsquo;s no obvious difference. Female-led movies are about 2% points lower on average, but the general distribution is the same (<a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">uniform</a>). Overlaying the two distributions shows as such.</p>

<p>The difference in averages is <em>not</em> statistically significant, as both the Kolmogorov–Smirnov test and the Wilcoxon rank-sum test fail to reject the null hypothesis at the 95% level (<strong>p = 0.37</strong> and <strong>p = 0.13</strong> respectively).</p>

<p>How about <a href="http://www.metacritic.com">Metacritic</a> scores?</p>

<p><img src="/img/movie-gender/movie-gender-7.png" alt=""></p>

<p><img src="/img/movie-gender/movie-gender-8.png" alt=""></p>

<p><img src="/img/movie-gender/movie-gender-9.png" alt=""></p>

<p>Again, distributions are the same (both take on the shape of a <a href="https://en.wikipedia.org/wiki/Normal_distribution">Normal distribution</a>, interestingly). And again, the Kolmogorov–Smirnov test and the Wilcoxon rank-sum test fail. (<strong>p = 0.45</strong> and <strong>p = 0.14</strong> respectively).</p>

<p>The quality of a movie is independent from the gender of the lead actor in determining the financial performance of a movie.</p>

<h2>Conclusion</h2>

<p>In 2014, <a href="http://www.mpaa.org/wp-content/uploads/2015/03/MPAA-Theatrical-Market-Statistics-2014.pdf">according to the MPAA</a>, the gender breakdown of moviegoers who see blockbuster movies is about 50/50, eliminating another potential explanation for the average movie revenue discrepancy.</p>

<p>Granted, there still can be more work done, such as controlling on movie Genre in addition to gender. Is the gender of the lead actor a <em>causal</em> factor in a movie&rsquo;s success? Not necessarily, and this analysis does not assert such. But there definitely is a revenue disparity that&rsquo;s worth investigating, and it&rsquo;s not just that &ldquo;male-led movies are better.&rdquo;</p>

<p>There may be more movies like Star Wars Episode VII where a movie with a female lead can hit almost a billion dollars domestically (e.g. Star Wars VIII). Things are looking upward, and it would not surprise me if the 22% revenue difference decreases and disappears in the next decade.</p>

<hr>

<p><em>You can view the code used to process the data and generate the data visualizations <a href="https://github.com/minimaxir/movie-gender/blob/master/movie_gender.ipynb">in this Jupyter notebook</a>, <a href="https://github.com/minimaxir/movie-gender">open-sourced on GitHub</a>, or you can <a href="https://github.com/minimaxir/movie-gender/raw/master/movie_gender_pdf.pdf">view as a PDF</a> which is better if you are on a mobile device.</em></p>

<p><em>You are free to use the charts from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Importance of Sanity-Checking Datasets Before Analysis]]></title>
    <link href="http://minimaxir.com/2016/04/trust-but-verify/"/>
    <updated>2016-04-06T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2016/04/trust-but-verify</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve done some cool things with movie data using a dataset from <a href="http://www.omdbapi.com">OMDb API</a>, which is sourced from <a href="http://www.imdb.com">IMDb</a> and <a href="http://www.rottentomatoes.com">Rotten Tomatoes</a> data. In my <a href="http://minimaxir.com/2016/01/movie-revenue-ratings/">previous article</a> on the dataset, I plotted the relationship between the domestic box office revenue of movies and their Rotten Tomatoes scores.</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-1.png" alt=""></p>

<p>I want to take another look at domestic Box Office Revenues with aggregate statistics such as means/medians on categorical variables such as MPAA rating and release month. For this type of analysis in particular, I&rsquo;ll also need to implement code in <a href="https://www.r-project.org">R</a> for inflation adjustment.</p>

<p>However, I ran into a few unexpectedly silly issues.</p>

<h2>Seeing Double</h2>

<p>There are many similarities between data validation and the Quality Assurance process of product development, which is why this particular area appeals to me personally as a Software QA Engineer. Whenever a cool dataset is released publicly, I play around with it to look for any obvious flaws and to get a good all-around benchmark on the robustness of the data (this is a separate procedure from the traditional &ldquo;data cleaning&rdquo; phase necessary to begin quantification on some poorly-structured datasets).</p>

<p>Do the extreme values in the data make sense? Is the data encoded in a sane format? Are there any obvious gaps or logical contradictions in summary representations of the data, especially when compared to other canonical sources? </p>

<p>These concerns are also some of the reasons I&rsquo;ve switched to the <a href="http://jupyter.org">Jupyter Notebook</a> as my primary data science IDE. After each block of code which transforms data, I can print the data frame inline to immediately see the results of the code execution, and refer back to them if anything odd happens in the future.</p>

<p>Let&rsquo;s say I have a data frame of Movies using the latest data dump (3/26/16) from OMDb. This data set contains 1,160,273 movies, including both IMDb and Rotten Tomatoes data. After cleaning the data (not shown), I can use the R package <code>dplyr</code> by Hadley Wickham to sort the data frame by Box Office Revenue descending, and print the <code>head</code> (top) of the data.</p>
<div class="highlight"><pre><code class="language-R" data-lang="R"><span class="kp">print</span><span class="p">(</span>df <span class="o">%&gt;%</span> select<span class="p">(</span>imdbID<span class="p">,</span> Title<span class="p">,</span> Year<span class="p">,</span> BoxOffice<span class="p">)</span> <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>BoxOffice<span class="p">))</span> <span class="o">%&gt;%</span> <span class="kp">head</span><span class="p">(</span><span class="m">25</span><span class="p">),</span> n <span class="o">=</span> <span class="m">25</span><span class="p">)</span>
</code></pre></div>
<p><img src="/img/trust-but-verify/data-2.png" alt=""></p>

<p>Those movies being the best <em>makes sense</em>. For <a href="http://www.rottentomatoes.com/m/star_wars_episode_vii_the_force_awakens/">Star Wars: The Force Awakens</a>, I can compare it to the Box Office reported on the corresponding Rotten Tomatoes page, which in turn matches the <a href="http://www.boxofficemojo.com/movies/?id=starwars7.htm">domestic Box Office Revenue</a> on <a href="http://www.boxofficemojo.com">Box Office Mojo</a>.</p>

<p>But wait, <a href="https://en.wikipedia.org/wiki/The_Dark_Knight_%28film%29">The Dark Knight</a> appears <em>twice</em>? How?!</p>

<p>There&rsquo;s no way I would have missed something this obvious during the sanity-check for my previous article. In order to make sure that I&rsquo;m not going insane, I double-checked the December 2015 data dump I used for that post, derived the top movies with the same methodology for the modern data dump, and the duplicate movies <em>were not present</em>. Weird.</p>

<p>There are 2 different IDs for
The Dark Knight, and for some other movies near the top (<a href="http://www.imdb.com/title/tt4817264/">Inside Out</a>, &ldquo;<a href="http://www.imdb.com/title/tt3138972/">The Gravity</a>&rdquo;). Fortunately, duplicate data like this is easy to debug. The second data entry for The Dark Knight has a greater IMDb ID (1774602) which means it was likely added to the site later. Let&rsquo;s look up the <a href="http://www.imdb.com/title/tt1774602/">corresponding IMDb page</a>:</p>

<p><img src="/img/trust-but-verify/dark-knight.png" alt=""></p>

<p>Huh. Apparently someone put a filler movie entry with the same name and release year as a blockbuster movie in hopes that people search for it by accident (and since it received 50 ratings and an average score of 8.6, this tactic was successful).</p>

<p>Using the Rotten Tomatoes <a href="http://developer.rottentomatoes.com/docs/read/json/v10/Movie_Alias">IMDb Lookup API</a>, we find that &ldquo;The Dark Knight&rdquo; page on Rotten Tomatoes&hellip;<a href="http://api.rottentomatoes.com/api/public/v1.0/movie_alias.json?type=imdb&amp;id=1774602">doesn&rsquo;t exist</a>.</p>

<p>We can run a safe deduplicate by removing entries with the same title (excluding the &ldquo;The&rdquo; if present) and release year.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">df_dup <span class="o">&lt;-</span> df <span class="o">%&gt;%</span> select<span class="p">(</span>Title<span class="p">,</span> Year<span class="p">)</span> <span class="o">%&gt;%</span> mutate<span class="p">(</span>Title <span class="o">=</span> <span class="kp">gsub</span><span class="p">(</span><span class="s">&quot;The &quot;</span><span class="p">,</span> <span class="s">&quot;&quot;</span><span class="p">,</span> Title<span class="p">))</span>
dup <span class="o">&lt;-</span> <span class="kp">duplicated</span><span class="p">(</span>df_dup<span class="p">)</span>   <span class="c1"># find entry indices which are duplicates</span>
<span class="kp">rm</span><span class="p">(</span>df_dup<span class="p">)</span>   <span class="c1"># remove temp dataframe</span>

df_dedup <span class="o">&lt;-</span> df <span class="o">%&gt;%</span> filter<span class="p">(</span><span class="o">!</span>dup<span class="p">)</span>   <span class="c1"># keep entries which are *not* dups</span>
<span class="kp">print</span><span class="p">(</span>df_dedup <span class="o">%&gt;%</span> select<span class="p">(</span>imdbID<span class="p">,</span> Title<span class="p">,</span> Year<span class="p">,</span> BoxOffice<span class="p">)</span> <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>BoxOffice<span class="p">))</span> <span class="o">%&gt;%</span> <span class="kp">head</span><span class="p">(</span><span class="m">25</span><span class="p">),</span> n <span class="o">=</span> <span class="m">25</span><span class="p">)</span>
</code></pre></div>
<p><img src="/img/trust-but-verify/data-1.png" alt=""></p>

<p>There we go! The de-duped dataset has 1,114,431 movies, impliying that there were 45,842 of these duplicate entries.</p>

<p>I&rsquo;m not sure <em>whose</em> fault it is that duplicate movies suddenly became present in the data dump: OMDb or Rotten Tomatoes. <em>But it doesn&rsquo;t matter</em>: the wrong entries still need to be addressed, and it&rsquo;s good to have a test case for the future too.</p>

<h2>Inflation Station</h2>

<p>A <a href="http://stackoverflow.com/a/26068058">Stack Overflow answer</a> from <a href="http://stackoverflow.com/users/1048757/brash-equilibrium">Ben Hanowell</a> has a good R implementation and rationale for implementing inflation adjustment using the <a href="https://research.stlouisfed.org/fred2/data/CPIAUCSL.txt">historical Consumer Price Index data</a> from the <a href="https://www.stlouisfed.org">Federal Reserve Bank of St. Louis</a>.</p>

<p>Take the index for each year (averaging each month for simplicity) and create an adjustment factor to convert historical dollar amounts into present-day dollar amounts. Much better than plugging hundreds of thousands of values into an online calculator. Here&rsquo;s the SO code made <code>dplyr</code>-friendly for this purpose, with the requisite sanity-checks.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">inflation <span class="o">&lt;-</span> read_csv<span class="p">(</span><span class="s">&quot;http://research.stlouisfed.org/fred2/data/CPIAUCSL.csv&quot;</span><span class="p">)</span> <span class="o">%&gt;%</span>
                    group_by<span class="p">(</span>Year <span class="o">=</span> <span class="kp">as.integer</span><span class="p">(</span><span class="kp">substr</span><span class="p">(</span>DATE<span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">4</span><span class="p">)))</span> <span class="o">%&gt;%</span>
                    summarize<span class="p">(</span>Avg_Value <span class="o">=</span> <span class="kp">mean</span><span class="p">(</span>VALUE<span class="p">))</span> <span class="o">%&gt;%</span>   <span class="c1"># average across all months</span>
                    mutate<span class="p">(</span>Adjust <span class="o">=</span> <span class="kp">tail</span><span class="p">(</span>Avg_Value<span class="p">,</span> <span class="m">1</span><span class="p">)</span> <span class="o">/</span> Avg_Value<span class="p">)</span>   <span class="c1"># normalize by most-recent year</span>

<span class="kp">print</span><span class="p">(</span>inflation <span class="o">%&gt;%</span> <span class="kp">head</span><span class="p">())</span>
<span class="kp">print</span><span class="p">(</span>inflation <span class="o">%&gt;%</span> <span class="kp">tail</span><span class="p">())</span>
</code></pre></div>
<p><img src="/img/trust-but-verify/inf.png" alt=""></p>

<p>For example, to get the inflation-adjusted Box Office Revenue for a movie released in 1949 in 2016 dollars, we multiply the reported revenue by 10. That sounds about right (and matches closely enough to the output of the <a href="http://data.bls.gov/cgi-bin/cpicalc.pl?cost1=1&amp;year1=1949&amp;year2=2016">Bureau of Labor Statistics inflation calculator</a>).</p>

<p>Now map each inflation adjustment factor to each movie by merging the two datasets (on the <code>Year</code> column), then multiply the Box Office revenue by the adjustment factor to get the inflation-adjusted revenue. Plus another sanity-check for good measure. </p>
<div class="highlight"><pre><code class="language-r" data-lang="r">df_dedup_join <span class="o">&lt;-</span> df_dedup <span class="o">%&gt;%</span> inner_join<span class="p">(</span>inflation<span class="p">)</span> <span class="o">%&gt;%</span> mutate<span class="p">(</span>AdjBoxOffice <span class="o">=</span> BoxOffice <span class="o">*</span> Adjust<span class="p">)</span>

<span class="kp">print</span><span class="p">(</span>df_dedup_join <span class="o">%&gt;%</span> select<span class="p">(</span>Title<span class="p">,</span> Year<span class="p">,</span> AdjBoxOffice<span class="p">)</span> <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>AdjBoxOffice<span class="p">))</span> <span class="o">%&gt;%</span> <span class="kp">head</span><span class="p">(</span><span class="m">25</span><span class="p">),</span> n<span class="o">=</span><span class="m">25</span><span class="p">)</span>
</code></pre></div>
<p><img src="/img/trust-but-verify/data-3.png" alt=""></p>

<p>Uh-oh.</p>

<p>I mean, <a href="https://en.wikipedia.org/wiki/The_Lorax_(TV_special)">The Lorax</a> probably earned $1.2 billion in VHS sales for Earth Day education <em>alone</em>, but the TV special was never released in theaters. There was a <a href="https://en.wikipedia.org/wiki/The_Lorax_(film)">CGI remake of The Lorax</a> a few years ago which was reasonably popular. Could it be that someone at Rotten Tomatoes or Box Office Mojo confused the two media?</p>

<p>That is exactly what happened. On Rotten Tomatoes, The <a href="http://www.rottentomatoes.com/m/the-lorax/">1972 Lorax</a> was encoded with similar box office revenue as the <a href="http://www.rottentomatoes.com/m/the_lorax/">2012 Lorax</a>; then the inflation factor sextupled it. For this type of data fidelity issue, it&rsquo;s considerably more obvious whose at fault.</p>

<p>Unfortunately, that&rsquo;s not the end of problems with the dataset. I compared my results with <a href="http://www.vox.com/2016/4/4/11351788/batman-v-superman-terrible-reviews#undefined">Vox&rsquo;s dataset</a> on worldwide historical box office revenues. In the Top 200 Movies by inflation-adjusted revenue, there are noted historical movie omissions such as <a href="http://www.rottentomatoes.com/m/jaws/">Jaws</a> and <a href="http://www.rottentomatoes.com/m/star_wars/">Star Wars: A New Hope</a>. It turns out Rotten Tomatoes does not have Box Office Revenue data for these movies at all. </p>

<p>That is a very serious problem which I&rsquo;ll have to think about if it blocks any analysis on aggregate box office data completely. In the end, sanity-checking third party data is important because you never know <em>how</em> the data will surprise you, until it&rsquo;s too late.</p>

<hr>

<p><em>You can view the Top 200 movies by domestic box office revenue for each of the 12/15 source dataset, the 3/16 dataset, the 3/16 deduped dataset, and the 3/16 deduced inflation-adjusted data <a href="https://github.com/minimaxir/movie-data-sanity-checking">in this GitHub repository</a>, along with the Jupyter notebook.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unlimited Data Storage Using Image Steganography and Cat GIFs]]></title>
    <link href="http://minimaxir.com/2016/03/gif-unlimited/"/>
    <updated>2016-03-29T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2016/03/gif-unlimited</id>
    <content type="html"><![CDATA[<p><span><style>
.suspicious-pregnant-pause {
  padding: 60px 0;
}
</style></span></p>

<p>In 2016, animated GIFs are absurdly popular. <a href="https://www.messenger.com">Facebook Messenger</a>, <a href="https://twitter.com">Twitter</a>, and <a href="https://slack.com">Slack</a> now natively support GIF sharing, and startups which help facilitate GIF sharing such as <a href="http://giphy.com">Giphy</a> and <a href="https://www.riffsy.com">Riffsy</a> have received <a href="https://www.crunchbase.com/organization/giphy"><em>tens of millions</em> of dollars</a> in venture capital. I even built my own tool to <a href="https://github.com/minimaxir/video-to-gif-osx">convert a video to a GIF</a> on OSX because <a href="http://minimaxir.com/2015/08/gif-to-video-osx/">I wanted to be cool</a> too.</p>

<p>On <a href="https://www.reddit.com">Reddit</a>, GIFs are also popular for reactions, often containing submission titles with the phrase MRW (&ldquo;my reaction when&rdquo;). Here is a wordcloud of the titles of all GIF submissions to Reddit in 2015:</p>

<p><img src="/img/gif-unlimited/reddit-gif-wordcloud-e.png" alt=""></p>

<p>Completely unrelated, <a href="https://en.wikipedia.org/wiki/Steganography">image steganography</a> is a technique that allows the embedding of messages into the pixels of images themselves, allowing for the transmission of secret messages. Image steganography was popular on the internet in the early 2000&rsquo;s, but died out before image-sharing between peers was commonplace and information encryption became a hot political issue.</p>

<h2>It&rsquo;s a Secret to Everybody</h2>

<p>In 2012, Zach Oakes created <a href="https://sekao.net/pixeljihad/">PixelJihad</a>, an <a href="https://github.com/oakes/PixelJihad">open-source</a>, client-side tool for image steganography using JavaScript and the HTML5 Canvas. This implementation embeds text messages by converting them into raw binary and modifying the least-significant bit (Aaron Miller wrote <a href="http://www.aaronmiller.in/thesis/">an informative paper</a> on LSB encoding) of the red, blue, and green binary pixel values of the specified image to match the message (e.g. a white pixel with red, blue, and green values of <code>[255, 255, 255]</code> can be represented in binary form as <code>[11111111, 11111111, 11111111]</code>; LSB encoding changes the right-most bit to 0 or 1 for each pixel channel). Changing the last bit of a color channel will not result in a detectable visual change by the human eye.</p>

<p>The image pixels to-be-modified during encoding are selected according to a semirandom schedule. The message can then be decoded by reading the LSB of the changed pixels from the schedule and reconstructing the binary message, then converting back into text.</p>

<p>Since we can set 3 bits per physical pixel, the maximum amount of data (in bytes) that can be stored is equivalent to the width of an image (in pixels) multiplied by the height times 3, divided by 8.</p>

<p>Take the Reddit GIF wordcloud I posted above. If you download the image and load it into PixelJihad, you can see I hid a secret message showing how to get the Reddit word data using <a href="https://cloud.google.com/bigquery/">BigQuery</a>. The visualization is 600px by 400px, therefore it can store up to:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">600 * 400 * 3 bits / 8 = 90000 Bytes / 1000 = 90 kB
</code></pre></div>
<p>The image itself is 257 kB, but a bonus 35% in data storage isn&rsquo;t bad.</p>

<p>PixelJihad only supports text messages with an artificial limit of 1,000 characters (~1 kB) for <em>sanity</em>, but I&rsquo;m not fond of being sane. There is <em>nothing</em> from a technical perspective stopping users from encoding larger text messages, or even raw file data as pure binary into images.</p>

<p>But embedding data into a <em>static</em> image is boring. What happens when you embed data into <em>each frame of a GIF</em>? Just a thought.</p>

<h2>Meow!</h2>

<p>Now, back to what you&rsquo;re reading this article for, who cares about all this stegosaurus crap. Here is a cute cat GIF <a href="https://www.reddit.com/r/gifs/comments/256bir/abduction_cat_re_flying_cat_re_so_big/">from Reddit</a>. Look at its cuteness.</p>

<p><img src="/img/gif-unlimited/aliencat.gif" alt=""></p>

<p>Aww.</p>

<p>The GIF file size is 2.44 MB (sorry, readers on mobile devices!). The dimensions of the GIF are 300px by 399px. But if you open the GIF in Preview on OSX, you can see that the GIF is comprised of 72 frames.</p>

<p><img src="/img/gif-unlimited/gif-finder-17.png" alt=""></p>

<p>Performing the same math:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">300 * 399 * 72 frames * 3 / 8 = 3231900 / 1000000 = 3.23 MB
</code></pre></div>
<p>The potential amount of encodable data is larger than the file size of the image itself! Screw secret messages, we&rsquo;re literally <em>creating</em> data storage out of nothing!</p>

<p>You know what this means? We can <em>embed the cat GIF into itself</em> steganographically by splitting it over all 72 frames, and get 780 kB left over to store whatever we want.</p>

<p>And then we can <em>do it again</em> to the newly-embedded image.</p>

<p>Infinitely.</p>

<p>780 kB free with each iteration.</p>

<p>Cat GIF recursion.</p>

<h2>Purrcursion</h2>

<p>Let&rsquo;s say we want to embed a 10 MB music file into a GIF. Since 10000 kB / 780 kB = 12.82, we re-embed the cat GIF 13 times, including 1/13th of the music file as sidecar data with each iteration. And now we have a cat GIF that contains some pretty sweet tunes!</p>

<p>And not only do we have a magical cat GIF, the data is completely portable; you can simply e-mail the cat GIF to your friend and they can decode the high-fidelity music file out of the GIF using an app that implements this schema. Or you can post the cat GIF w/ music to Reddit for <em>double</em> meaningless internet points.</p>

<p>The only limitation to the &ldquo;infinitely&rdquo; part is the increasing amount of CPU processing power for each decoding, but that&rsquo;s fixable since it&rsquo;s 2016 and smartphones have quad-core processors. Worth every ounce of battery life, in my opinion.</p>

<p>*<em>tl;dr *</em> I was bored and decided to create infinite data in a way that makes people feel fuzzy inside. No big deal. Of course, I can&rsquo;t show <em>you</em> a proof-of-concept of how this works. That would be silly. But if you&rsquo;re a venture capitalist who wants to invest more tens of millions of dollars into a clearly-sustainable GIF startup, feel free to email me at <a href="mailto:max@minimaxir.com">max@minimaxir.com</a>.</p>

<div class="suspicious-pregnant-pause">&hellip;</div>

<div class="suspicious-pregnant-pause">&hellip;</div>

<div class="suspicious-pregnant-pause">&hellip;</div>

<p>Yeah, infinite data via image steganography doesn&rsquo;t actually work. But the reasons it doesn&rsquo;t work are not obvious unless you have deep knowledge of how image compression reduces the file size of images, and conversely, how modifying images could cause them to become <em>larger</em> in file size  (I may or may not have spent several hours forking PixelJihad and adding modern features before realizing that I am not a data storage magician).</p>

<p>I will write a follow-up post soon detailing the steganographic gotchas. Until then, keep in mind that there is no such thing as a free cat GIF.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Facebook Reactions and the Problems With Quantifying Likes Differently]]></title>
    <link href="http://minimaxir.com/2016/02/facebook-reactions/"/>
    <updated>2016-02-29T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2016/02/facebook-reactions</id>
    <content type="html"><![CDATA[<p>Facebook added <a href="http://newsroom.fb.com/news/2016/02/reactions-now-available-globally/">Facebook Reactions</a>, allowing users to do more than just &ldquo;Like&rdquo; posts and statuses as they have done for the past decade. Likes were the universal symbol of approval on social media. Now, Facebook users can apply more granular responses, from positive emotions like <strong>Love</strong>, to negative emotions such as <strong>Angry</strong>. This was widely believed to be Facebook&rsquo;s compromise instead of adding a Dislike button.</p>

<p><img src="/img/facebook-reactions/facebook_react.png" alt=""></p>

<p>Of course, there&rsquo;s an ulterior motive. The use of reactions provides organic data on the sentiment of a status, which is helpful for numerous marketing and statistical applications. As <a href="http://www.buzzfeed.com/alexkantrowitz/facebook-reactions-launch-today">BuzzFeed notes</a>, Facebook ads may be able &ldquo;to write one product message for someone who mostly uses <strong>Sad</strong> and another who mostly uses <strong>Wow</strong> or <strong>Love.</strong>&rdquo;</p>

<p>However, this isn&rsquo;t the first time a big social network has tried implementing reactions alongside Likes/Dislikes. Four years ago, YouTube added <a href="http://googlesystem.blogspot.com/2011/06/youtube-reactions.html">Reaction buttons</a> to their comments section:</p>

<p><img src="/img/facebook-reactions/youtube-reactions.png" alt=""></p>

<p>&hellip;and removed them sometime after without fanfare, replacing it with the simple Like/Dislike bar.</p>

<p>Presumably, YouTube implemented the buttons for the similar reason as Facebook. What makes things different now, if anything?</p>

<h2>A Quantitative Approach to Feeling</h2>

<p>Even after YouTube&rsquo;s failure, another data-driven website implemented reaction buttons: BuzzFeed (who else?). At the end of each article (in most categories), registered users can select a quirky reaction to indicate how they felt about the article.</p>

<p><img src="/img/facebook-reactions/buzzfeedreactions.png" alt=""></p>

<p>The heart represents <strong>Love</strong> internally and is by-far the most-used reaction on BuzzFeed posts. When I started scraping BuzzFeed data in 2014 <a href="http://minimaxir.com/2015/01/linkbait/">to analyze clickbait</a>, I made sure to grab the reaction data of other reactions as well to see if there are any interesting trends or correlations between reactions. A cursory glance at the scraped reaction data revealed a problem that forced me to disregard it.</p>

<p>An important part of variable selection for analysis and modeling is avoiding <em>redundant</em> features, as that can cause issues such as <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a> and <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>. For Facebook, avoiding adding redundant Reactions was an <a href="https://medium.com/facebook-design/reactions-not-everything-in-life-is-likable-5c403de72a3f">explicit design goal</a> of the feature, but the positive emotions such as <strong>Like</strong> and <strong>Wow</strong> might be overly similar regardless (I believe it fair to compare the behavior of BuzzFeed users with the average Facebook user, given that they hit the same demographics). Do BuzzFeed readers use specific positive reactions differently? Did they use specific negative reactions?</p>

<p>I rechecked my 2014 data in light of Facebook Reactions. The scraped dataset contains reaction data from 9,883 BuzzFeed articles in the Celebrity, Animals, Books, Longform, and Business categories. From that, I made a <a href="http://vita.had.co.nz/papers/gpp.pdf">pairs plot</a> for the counts of all the <em>positive</em> reactions on the articles to illustrate all bivariate relationships:</p>

<ul>
<li>The lower half of the pairs plot is a scatterplot for the two reactions; the axes represent the number of votes for a given reaction on a BuzzFeed article (both axes are scaled logarithmically), color intensity indicates the number of articles at that X/Y combo, and the line is a linear trendline of least-squares.</li>
<li>The diagonal of the pairs plot represents the density distribution of reaction vote counts for that reaction. (also logarithmically scaled on the X axis)</li>
<li>The upper half of the pairs plot illustrates the Pearson correlation between the non-log quantities of the two reaction variables. The stars represent statistical significance of the correlation test; since the data set is large, all correlations are statistically significant (rejection of null hypothesis of no correlation) at p &lt; 0.001.</li>
</ul>

<p><img src="/img/facebook-reactions/buzzfeed-pos.png" alt=""></p>

<p>All of the bivariate correlations of positive reactions are <em>moderately or strongly positively correlated</em>, which is problematic for analysis (except one: apparently, there is little statistical relationship between things that are cute and things that make you go YAAASS). So why not just use the <strong>Love</strong> reaction, since articles tend to get about 100 Loves, while other reactions get around 10?</p>

<p>Does the same hold for negative reactions? Relatedly, we would also expect a negative correlation between the number of <strong>Love</strong> reactions and negative reactions, right?</p>

<p><img src="/img/facebook-reactions/buzzfeed-neg.png" alt=""></p>

<p>All negative reactions are positively correlated, as expected, but there is a weak <em>positive</em> correlation between <strong>Love</strong> and <strong>Hate</strong>, which is definitely not right. There isn&rsquo;t an ideal &ldquo;negative&rdquo; reaction, since all have similar distributions.</p>

<p>Why does Facebook have 6 different responses to gauge positivity or negativity when one reaction for each would be both more accurate and more intuitive for the user?</p>

<h2>Conceal, Don&rsquo;t Feel</h2>

<p>There are other qualitative issues with Facebook&rsquo;s current implementation of Reactions. Apparently, Likes and Reactions are treated <em>differently internally</em>. As a result, you get separate notifications for Likes and Reactions.</p>

<p><img src="/img/facebook-reactions/facebook_react2.png" alt=""></p>

<p>Why? No idea. There is enough Notification spam on Facebook, I don&rsquo;t need <em>double notifications</em> in my Notification feed for every status I make.</p>

<p>What&rsquo;s important to note is that a user cannot both Like and React to a status; only one or the other. As a result, the number of Likes on statuses overall will drop, and this is a <em>major</em> problem for businesses who are dependent on measuring the number of Likes for engagement.</p>

<p>I took a look at the Facebook Graph API endpoint for <a href="https://developers.facebook.com/docs/graph-api/reference/v2.5/post">Facebook Page Posts</a> (same endpoint I use for my <a href="https://github.com/minimaxir/facebook-page-post-scraper">Facebook Page Data Scraper</a>), and I can confirm that the API can only report the number of Likes on a status; not the number of Likes + Reactions, or number of Likes + number of each Reaction.</p>

<p><img src="/img/facebook-reactions/cnn_fb.jpg" alt=""></p>

<p>There is no way currently to automate the retrieval of Reactions data from Facebook posts, which is an unfortunate oversight (especially considering how Twitter <a href="https://blog.twitter.com/2015/hearts-for-developers">handled the transition</a> from Favorites to Likes easily).</p>

<p>The example <a href="https://www.facebook.com/cnn/posts/10154506885211509">CNN story</a> I used for that screenshot is anecdotally one of the very few examples I&rsquo;ve noticed where the number of Likes is <em>almost equal</em> to negative emotions, a relationship which should be weakly correlated and therefore this knowledge may be useful to isolate the story as unusual (and serve ads accordingly). At Facebook&rsquo;s immense scale, identifying a relatively small proportion of unusual stories might be enough to justify adding Reactions.</p>

<p>Or maybe this feature is just the harbinger of a new generation of emotionally-charged linkbait. Perhaps there is more to this Facebook Reactions data than what meets the eye, and I&rsquo;ll update my scripts and do further statistical analysis when able. But given what has happened with Reactions data before with YouTube, I am unconvinced and I still believe the functionality as a whole is a usability regression that won&rsquo;t last.</p>

<p>A Dislike button would have been better, just saying.</p>

<hr>

<p><em>You can view the code and data used to generate the BuzzFeed Reaction data visualizations <a href="https://github.com/minimaxir/facebook-reactions/blob/master/buzzfeed_reactions.ipynb">in this Jupyter notebook</a>, <a href="https://github.com/minimaxir/facebook-reactions">open-sourced on GitHub</a>, or you can <a href="https://github.com/minimaxir/facebook-reactions/raw/master/reactions_pdf.pdf">view as a PDF</a>, which is better if you are on a mobile device.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[You're Not Allowed to Criticize Startups, You Stupid Hater]]></title>
    <link href="http://minimaxir.com/2016/01/startup-haters/"/>
    <updated>2016-01-25T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2016/01/startup-haters</id>
    <content type="html"><![CDATA[<p>A couple weeks ago, <a href="https://itunes.apple.com/us/app/peach-a-space-for-friends/id1067891186?mt=8">Peach</a>, a messaging app, was released on iOS. Not only did Peach trend on Twitter on the day of release, but there was also an <em>unusual</em> amount of media coverage for app, with both <a href="http://techcrunch.com/2016/01/08/peach-is-a-slick-new-messaging-app-from-the-founder-of-vine/">TechCrunch</a> and <a href="http://www.buzzfeed.com/katienotopoulos/do-i-dare-to-post-on-peach">BuzzFeed</a> posting glowing reviews for it, noting that the app is &ldquo;slick&rdquo; and &ldquo;blowing up&rdquo; respectively (commenters in the BuzzFeed article thought it was paid advertising). Usually, reports for apps from both sites are more neutral, which made me curious.</p>

<p><img src="/img/startup-haters/peach-1.png" alt=""></p>

<p>There are <em>already</em> an absurd amount of messaging apps out there, and as a result, product differentiation is the primary value proposition. (e.g. security and <a href="https://itunes.apple.com/us/app/telegram-messenger/id686449807?mt=8">Telegram Messenger</a>) What is Peach&rsquo;s differentiation? &ldquo;Magic Words&rdquo;, apparently, which let you perform contextual actions with words! These Magic Words are implemented in a manner similar to a <a href="https://en.wikipedia.org/wiki/Command-line_interface">command-line interface</a> (CLI). <a href="http://nymag.com/following/2016/01/how-the-command-line-became-mainstream-again.html">NYMag</a> goes into extreme detail about the feature. Nerds love CLIs, so you can tell the product was made by smart people!</p>

<p><img src="/img/startup-haters/peach-2.png" alt=""></p>

<p>The <a href="https://news.ycombinator.com/item?id=10883698">Hacker News thread</a> about the NYMag article was interesting. One commenter (jobu) that the Magic Words are similar to those of <a href="https://slack.com">Slack</a>, which allow for &ldquo;/&rdquo; commands (again, messaging app differentiation). A HN user (pbreit) argued in response:</p>

<blockquote>
<p>You&rsquo;re missing the whole thing. For starters, Slack is for work and Peach is for personal. But I think before you dismiss something as &ldquo;isn&rsquo;t it just x plus y?&rdquo; you need to take a little bit more time and thinking to try and figure out what the product designers are trying to achieve and how.</p>
</blockquote>

<p>Wait, what? Are the users of Peach supposed to be mind readers? Are people stupid for not understanding the purpose of Peach?</p>

<p>Another user (thwarted) replied to that response with such:</p>

<blockquote>
<p>Maybe the product designers and product providers should be more explicit about how their product should be used and who their target demographic is, if they want a successful product, rather than making everyone &ldquo;figure out&rdquo; what they are trying to achieve and how.</p>
</blockquote>

<p>pbreit replied:</p>

<blockquote>
<p>&ldquo;Figuring it out&rdquo; might be the point. You do realize that you are referring to some of the very few people who have actually already built an amazingly successful product.</p>
</blockquote>

<p>I should have mentioned earlier that Peach was made by <a href="http://byte.co">Byte</a>, which is ran by <a href="https://twitter.com/dhof">Dom Hofmann</a>, a founder of Vine which was purchased by Twitter before launch (it also appears Peach is a pivot from the failed Byte app; something left out by the blog articles). Yes, if a founder has had a previously successful startup, the previous success implies a higher probability for future startup success than for a startup run by an unknown founder. However, it&rsquo;s still not a 100% guarantee, and startups should not be immune to criticism because of asymmetric information between the users and the startup itself. Peach is impeachable. (pun intended)</p>

<p>TechCrunch did a <a href="http://techcrunch.com/2016/01/11/hype-or-not-peach-hit-the-top-10-social-networking-app-list-fast/">follow-up article</a> three days after release stating (paraphrased) &ldquo;Peach is #9 in the App Store in the Social Networking category, despite the haters! Now they will be a success! Haters gonna hate!&rdquo; Social networking is a double-edged sword in terms of network effects: they can grow incredibly fast as friends-of-friends register, and they can <em>die</em> incredibly fast once they all realize the app is a fad. As of publishing, Peach is ranked #110 in the Social Networking category, and off-the-charts in Overall. And <a href="https://www.appannie.com/apps/ios/app/peach-a-space-for-friends/rank-history/">still dropping</a>, with no apparent recovery.</p>

<p><img src="/img/startup-haters/peach-3.png" alt=""></p>

<p>I seriously wish the tech media would admit they were wrong, but they never will. The startup world is in dire need of cautionary tales as valuations inflate to absurd levels.</p>

<p>But hey, isn&rsquo;t any startup ran by a previously-funded founder a sure thing? They know what it takes to create a company, even if it&rsquo;s in a different and highly competitive environment years later than their first success! One success is all a founder needs.</p>

<p>A startup&rsquo;s release is unpolished and buggy? It&rsquo;s just an MVP, and startups are expected to have issues out of the gate, so you have to forgive them! They don&rsquo;t need QA engineers.</p>

<p>A startup uses growth hacking to get early traction? It&rsquo;s hustle, and they should be applauded for their creativity! Whoever complains is just in the minority, anyways.</p>

<p>A startup gets a lot of points on Product Hunt? That&rsquo;s validation, especially since it was submitted by one of the friends/investors of the startup for all their network to see and upvote! A strong personal network is the only thing you need for business success.</p>

<p>A startup raises a Series A? That means venture capitalists did due diligence, and if the idea sucked, they would not have invested! Millions of dollars is a lot of money to those people.</p>

<p>A startup loses traction and buzz? It&rsquo;s not dead, it can come back as long as it has money in the bank! Your lack of faith demotivates entrepreneurs.</p>

<p>A startup <em>dies</em>? They can just try again! It was bad luck anyways and they likely did nothing wrong.</p>

<p>A startup is mercy-killed through an acquisition/acquihire? Just another step in their incredible journey! The founders still win, and now they have the &ldquo;exited&rdquo; label to leverage for future employment.</p>

<p>Sarcastic rhetorical questions aside, it is very annoying that it is considered socially unacceptable in Silicon Valley to criticize a startup. Even though the majority of startups fail, actually <em>saying so</em> is sacrilegious. In Peach&rsquo;s particular case, the fact that the app is from a previous exited founder should invite <em>more</em> scrutiny instead of giving it a free pass.</p>

<p><strong>EDIT</strong>: Fixed joke so that it is funny.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Video Games and Charity: Analyzing Awesome Games Done Quick 2016 Donations]]></title>
    <link href="http://minimaxir.com/2016/01/agdq-2016/"/>
    <updated>2016-01-11T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2016/01/agdq-2016</id>
    <content type="html"><![CDATA[<p><a href="https://gamesdonequick.com">Awesome Games Done Quick</a>, and its sister event Summer Games Done Quick, are a fundraising events that livestreams video game speedruns <a href="http://www.twitch.tv/gamesdonequick/profile">live on Twitch</a> for charity. Beginning in January 2011, before Twitch was launched out from Justin.tv, <a href="https://en.wikipedia.org/wiki/Awesome_Games_Done_Quick_and_Summer_Games_Done_Quick#List_of_marathons">AGDQ was very small</a> and only raised $52,519.83 for the <a href="http://preventcancer.org">Prevent Cancer Foundation</a>; now, in 2016, from January 3rd to January 10th, AGDQ <a href="https://gamesdonequick.com/tracker/index/agdq2016">successfully raised</a> about $1.2 <em>million</em> for the charity.</p>

<p>A <a href="https://en.wikipedia.org/wiki/Speedrun">speedrun</a>, as the name suggests, is the process of completing a video game as fast as possible, optionally with self-imposed challenges to make things more interesting. Speedruns can emphasize extreme player skill and/or clever glitch abuse. And unexpected mistakes which make the results hilarious.</p>

<p>One of the first runs of AGDQ 2016, <a href="https://www.youtube.com/watch?v=jLlian3g7Gg">Super Monkey Ball</a>, demonstrates all of these. (run starts at 5:57)</p>

<div class="responsive-video"><iframe src="http://www.youtube.com/embed/jLlian3g7Gg " style="width: 100%; height: 100%; border: none; padding-bottom: 20px" allowfullscreen></iframe></div>

<p>AGDQ 2016 also has fun with the concept of speedrunning. One of the best events of AGDQ 2016 was a blind speedrun of user-created <a href="https://www.youtube.com/watch?v=8qC584MWXO4">Super Mario Maker</a> levels from top designers, in which hilarity ensued. (run starts at 27:41)</p>

<div class="responsive-video"><iframe src="http://www.youtube.com/embed/8qC584MWXO4 " style="width: 100%; height: 100%; border: none; padding-bottom: 20px" allowfullscreen></iframe></div>

<p>It might be interesting to know <em>which</em> video games lead to the achievement of over $1M donated to charity and the nature of the donations in general.</p>

<h2>Gaming Data</h2>

<p>With a few quick scripts on Kimono to scrape data from the <a href="https://gamesdonequick.com/tracker/donations/agdq2016">AGDQ 2016 donation page</a> (+ a <em>lot</em> of postprocessing in R!), I obtained a dataset of all 30,528 donations, their donors, when they donated, during what speedrun they donated, and <em>why</em> they donated. (<a href="https://docs.google.com/spreadsheets/d/1yyfkS0jvRK1cWrQesYiBn1TMGC93lo1MqahcU3XeGIU/edit?usp=sharing">Google Sheets link</a> for all the data)</p>

<p>Here are the cumulative donations during AGDQ, color coded by day:</p>

<p><img src="/img/agdq-2016/agdq-1.png" alt=""></p>

<p>Cumulative donations were strong the entire run. On the second-to-last day, the donations rallied and increased exponentially, clearing $1M handily on the last day.</p>

<p>The donation amount minimum is $5, but the average is significantly higher at $39.62. What is the distribution of donations?</p>

<p>Here is a distribution of donations from $5 to $100 (for ease of visualization/interpretation), which account for 97% of all donations:</p>

<p><img src="/img/agdq-2016/agdq-2.png" alt=""></p>

<p>The median donation amount is $20. What&rsquo;s interesting is that donations occur at clear break points: not only are there many donations at multiple of $10, but there are many donations at $25 and $75 as well. The $50 and $75 points also potentially benefited for being the threshold for entry into a <a href="https://gamesdonequick.com/tracker/prizes/agdq2016">grand prize raffle</a>. I&rsquo;ll note off-chart that there is a spike in $1,000 donations, the threshold for the audience-clapping in celebration and the <a href="https://gamesdonequick.com/tracker/donation/212588">top single donation</a> was made by an AGDQ sponsor, <a href="https://www.theyetee.com/">The Yetee</a>, at $18,225. The <a href="https://gamesdonequick.com/tracker/donation/209613">top donation by a non-sponsor</a> is from Minecraft creator Notch at $8,000, which he <a href="https://gamesdonequick.com/tracker/donation/234071">did twice</a>.</p>

<p>Which games are the most popular and generated the most amount of money for the Prevent Cancer Foundation?</p>

<p><img src="/img/agdq-2016/agdq-4.png" alt=""></p>

<p>Unsurprisingly, Nintendo games are the most popular due to the nostalgia factor. In fairness, the top runs on this chart occur during the last two days of AGDQ 2016, which as mentioned previously may have been affected by a rally, so we cannot assert causality. The appearance of <a href="http://yachtclubgames.com/shovel-knight/">Shovel Knight</a> and <a href="https://en.wikipedia.org/wiki/Bloodborne">Bloodborne</a> as leading donation games, both relatively recently released, shows that speedrunning has more appeal than just retro games.</p>

<p>A popular technique in charity drives is donation incentives, which help bolster the number of donations total. The <a href="https://gamesdonequick.com/tracker/bids/agdq2016">AGDQ bid incentives</a> can include bonus game segments, or certain game decisions, such as what name to give to a main character.</p>

<p>Any donation can optionally be assigned as a donation toward an incentive. Which run received the most money toward incentives?</p>

<p><img src="/img/agdq-2016/agdq-5.png" alt=""></p>

<p>Super Metroid donations incentives accounted for nearly <em>&frac14;th</em> of all the money raised at AGDQ. Final Fantasy IV accounted for a large amount as well, however, in both cases, the rally effect may apply. </p>

<p>Speaking of the Super Metroid donation incentives, it should be noted that this particular incentive is one of the most culturally-important incentives in the show. Super Metroid has an optional objective to Save The Animals from planetary destruction, but this costs time, and time is important for a speedrun. Or the speedruner can Kill The Animals through inaction for efficiency, &ldquo;Saving the Frames.&rdquo;</p>

<p>What is the split of these incentive choices?</p>

<p><img src="/img/agdq-2016/agdq-6.png" alt=""></p>

<p>Yes, the animals were killed (specifically, they were <strong>REKT</strong>, in the words of a last-minute donator). Both bonus games and vanity naming were popular, but nothing compared to the Save the Animals / Kill the Animals bid war.</p>

<p>Lastly, people can leave comments with donations, and these comments are usually read on-stream when possible, as you&rsquo;ve likely noticed if you&rsquo;re watched the videos above.</p>

<p>Here&rsquo;s a fun, nonscientific word cloud of those comments:</p>

<p><img src="/img/agdq-2016/agdq-7.png" alt=""></p>

<p>Lots of positivity, aside from the whole &ldquo;Kill the Animals&rdquo; thing. After all, the event is all about preventing cancer.</p>

<p>While this post isn&rsquo;t an academic analysis, it&rsquo;s neat to see to see what kinds of things drive donation to charity. This model of livestreaming and charitable applications is very successful, and important given the renewed attention toward livestreaming with the rise of Twitch to mainstream attention, alongside the rise of personal streaming with apps like Periscope. Donation incentives are a <em>very</em> successful technique for facilitating donations.</p>

<p>It will be interesting to see if Twitch and events like AGDQ can leverage charitable livestreaming, or if another startup/organization beats them first. Bidding-Wars-for-Deciding-the-Fate-of-Fictional-Animals-as-a-Service has a nice ring to it.</p>

<hr>

<p><em>You can access a Jupyter notebook with the data processing and chart processing code <a href="https://github.com/minimaxir/agdq-2016">in this GitHub repository</a>. If you use the processed donation data or visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks! :)</em></p>

<p><em>Note for donation incentive statistics: the user can theoretically split their donation among multiple incentives; unfortunately I assumed at time of scrape that all the money could only go toward one bid. All donations I investigated from the source data were toward a single incentive except two donations from The Yetee which I fixed manually. If there are any data discrepancies, that is the likely cause.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Movie Review Aggregator Ratings Have No Relationship with Box Office Success]]></title>
    <link href="http://minimaxir.com/2016/01/movie-revenue-ratings/"/>
    <updated>2016-01-07T08:30:00-08:00</updated>
    <id>http://minimaxir.com/2016/01/movie-revenue-ratings</id>
    <content type="html"><![CDATA[<p><a href="http://www.rottentomatoes.com">Rotten Tomatoes</a> has become synonymous with movie quality in recent years. The Rotten Tomatoes Tomatometer aggregates all reviews written by movie critics for a given movie on the internet, determines whether each reviewer rates the movie as &ldquo;Fresh&rdquo; or &ldquo;Rotten&rdquo; and calculates an average. If the proportion of Fresh reviews for a given movie is greater than or equal to 60%, the movie itself is considered &ldquo;Fresh&rdquo; and receives a special icon.</p>

<p><img src="/img/movie-revenue-ratings/examples.png" alt=""></p>

<p>Top Movies like Christopher Nolan&rsquo;s <a href="http://www.rottentomatoes.com/m/the_dark_knight/">The Dark Knight</a> received a 94% Rotten Tomatoes rating, and generated $533.3 million in domestic box office revenue. But other movies, like Michael Bay&rsquo;s <a href="http://www.rottentomatoes.com/m/transformers_revenge_of_the_fallen/">Transformers: Revenge of the Fallen</a>, received a 19% Tomatometer rating, but still generated $402.1 million in domestic box office revenue.</p>

<p>How strong is the relationship between Tomatometer scores and box office success, anyways? Or are other, better metrics? Time to make some pretty charts.</p>

<p>I obtained a large amount of movie data from the <a href="http://www.omdbapi.com">OMDb API</a>, which provides easy access to movie metadata from IMDb and Rotten Tomatoes. This data contains Rotten Tomatoes Tomatometer scores, Rotten Tomatoes Audience Scores, IMDb User Rankings, and Metacritic Scores. If you want to know how I processed the data in R and plotted the charts using ggplot2, I have <a href="https://www.youtube.com/watch?v=F5Hjlkxw_2A">prepared a screencast</a> for your viewing pleasure.</p>

<div class="responsive-video"><iframe src="http://www.youtube.com/embed/F5Hjlkxw_2A " style="width: 100%; height: 100%; border: none; padding-bottom: 20px" allowfullscreen></iframe></div>

<p>For this analysis, we will be looking at the <a href="http://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/">log-transformation</a> of domestic box office revenue, since the values are skewed by mega-blockbusters like the ones mentioned previously. Revenues are not inflation-adjusted since the rating data is only present for recent years and due to the log-transformation already present, inflation correction would not impact this particular analysis much.</p>

<h2>Rotten Tomatoes Tomatometer</h2>

<p>After processing, I have a data subset of 4,863 movies with both Tomatometer and Box Office Gross values. Let&rsquo;s plot all those movies on a scatterplot of log(BoxOffice) vs. Meter with each point having a slight transparency; that way, clusters of points will be come apparent where the areas are darker on the chart.</p>

<p>We expect a positive linear relationship: movies with high Tomatometer scores to have high box office revenue, and inversely movies with low score to have low box office revenue.</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-1.png" alt=""></p>

<p>Wait, why does the trendline have a <em>negative</em> slope?</p>

<p>The <a href="https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">Pearson correlation</a> between the Tomatometer scores and log(BoxOffice) is <strong>-0.18</strong>, implying a weak <em>negative</em> linear relationship between the two variables. Not what I expected.</p>

<p>There do appear to be clusters in the data. There is a group of points between $10M and $100M revenue and 0% to 20% Tomatometer rating.  Another group is present between $1,000 and $1M revenue and 80% to 100% RT rating. Both of these areas are outside of a linear relationship: perhaps these clusters are skewing trends too?</p>

<p>Let&rsquo;s try another visualization of the data using <a href="https://en.wikipedia.org/wiki/Contour_line">contour maps</a>, which allow the data to become 3D, so-to-speak. Using a 2D <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">kernel density estimator</a>, we can identify and color areas on the plot according to the number of points present in that area; the greater the color saturation, the more points present in the given area.</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-2.png" alt=""></p>

<p>The two clusters mentioned previously are now much more apparent. It appears there are two distinct sets of movies: blockbusters which critics hate, and limited-appeal films which critics loves. Incidentally, there is no discernible difference between movies which are Fresh (&gt;60%) and Rotten.</p>

<h2>Metacritic</h2>

<p>The <a href="http://www.metacritic.com">Metacritic</a> score is also <a href="http://www.metacritic.com/about-metascores">derived from review data</a> by critics; however, instead of calculating a binary review sentiment and calculating a proportion from that sentiment, Metacritic gives a quantification from 0 to 100 to each critic review and averages them together.</p>

<p>Does that change the results for 4,479 movies?</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-7.png" alt=""></p>

<p>Correlation between Metacritic score and log(BoxOffice) is <strong>-0.13</strong>, which puts the analysis in a similar state as the Rotten Tomatoes data. However, the blockbuster cluster has shifted right, and the lesser-appeal cluster has shifted left.</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-8.png" alt=""></p>

<p>Clusters are much closer together.</p>

<p>Perhaps a review metric by non-critics will tell a different story.</p>

<h2>Rotten Tomatoes Audience Score</h2>

<p>The Audience Score is calculated in a similar way to the Rotten Tomatoes Tomatometer score: user to the site rate a movie from 0 to 5 stars in half-star increments (i.e. effectively a scale from 0-10) and the proportion of reviews with 3.5 star ratings or higher becomes the Audience Score.</p>

<p>This also presents a cognitive bias in ratings: the <a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/FourPointScale">Four Point Scale</a>, where having a discrete form of ranking may cause people to tend to rate toward the top of the scale and make the entire metric skewed or misleading.</p>

<p>How does the Audience Score compare for 5,163 movies? After all, the audience is the group of people who determine how much money a movie makes at the Box Office.</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-3.png" alt=""></p>

<p>Correlation between the Audience score and log(BoxOffice) is <strong>0.05</strong>, which is a positive linear correlation, but representative of barely any practical correlation.</p>

<p>Speaking of the Four Point Scale, notice how, like with Metacritic score, there are barely any movies between 0% and 20% Audience Score. Is there really a skew? Let&rsquo;s look at the contours:</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-4.png" alt=""></p>

<p>The locations of the clusters are much different than that of Tomatometer clusters. Both clusters are closer together, with the blockbuster cluster between 50% and 60% audience score and the lesser-appeal cluster between 70% and 80%. Hence, the low correlation.</p>

<h2>IMDb</h2>

<p><a href="http://www.imdb.com">IMDb</a> works <a href="http://www.imdb.com/help/show_leaf?votestopfaq">almost the same way</a> as the Metacritic for non-critics: ratings from IMDb users between 1-10 (note that 0 is missing!) are averaged to get a final score.</p>

<p>How do 5,167 movies fare?</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-5.png" alt=""></p>

<p><strong>What?!</strong></p>

<p>The point groupings are at the <em>same</em> positions of ratings, and the correlation between IMDb ratings and log(BoxOffice) is <strong>0.00</strong>. Yes, there&rsquo;s <em>zero</em> correlation!</p>

<p>Checking the contour map confirms it:</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-6.png" alt=""></p>

<p>That is <em>literally</em> a Four Point Scale between 5 and 8!</p>

<p>The Rotten Tomatoes metric is the only metric that actually <em>uses</em> the entire rating scale. None of the other potential metrics provide more insight into a potential reason for high box-office revenue. Perhaps the movie rating system itself is broken.</p>

<p>That&rsquo;s not to say that movies need high box-office revenues to be considered successful. However, working with movie profitability, and by extension movie budget, is opening another can-of-worms with respect to data integrity. (that said, on Reddit, /u/chartmkr recently <a href="https://www.reddit.com/r/dataisbeautiful/comments/3zpp3w/movie_budgets_and_box_office_success_19552015_oc/">posted a visualization</a> of Gross vs. Budget which is interesting).</p>

<p>It&rsquo;ll still be fun to point to a Rotten Tomatoes Tomatometer rating as a kneejerk reaction to whether a movie rocks/sucks. Although, the reasons for movie financial success at the box office definitely warrant further investigation.</p>

<p><strong>UPDATE 1/11/15</strong>: On a <a href="https://news.ycombinator.com/item?id=10872076">discussion on Hacker News</a>, it was suggested that the blockbuster movies and the indie movies cancel each other out, i.e. blockbusters have a positive correlation and indies have a negative correlation.</p>

<p>For the blockbuster cluster alone, the log-correlation is <strong>0.23</strong> (not weak but not great positive correlation). For the indie cluster alone, the log-correlation is <strong>-0.12</strong> (same as original analysis).</p>

<p>For future analysis, it may be worthwhile to split these two clusters. I stand by the original analysis for this post: very frequently I&rsquo;ve heard the question &ldquo;is this a good movie?&rdquo; and the response is &ldquo;what does the RT score say?&rdquo; Both Box Office revenues and RT scores are important measures of quality (depending on perspective), and users who want to see or purchase a movie may not necessarily care if it&rsquo;s indie or a blockbuster.</p>

<p>User cwyers <a href="https://news.ycombinator.com/item?id=10878019">suggested</a> that Simpson&rsquo;s Paradox may be in play since the number of theaters showing a movie is positively correlated to box office revenue, adding a potentially-confounding affect. I will see if I can obtain that data for future analysis.</p>

<hr>

<p><em>You can access the open-sourced Jupyter notebook and high-resolution charts from this article in <a href="https://github.com/minimaxir/movie-revenue-ratings">this GitHub repository</a>. If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!</em></p>

<p><em>Unfortunately, I cannot redistribute the data itself due to licensing concerns.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Let's Code an Analysis and Visualizations of Yelp Data using R and ggplot2]]></title>
    <link href="http://minimaxir.com/2015/12/lets-code-1/"/>
    <updated>2015-12-28T09:00:00-08:00</updated>
    <id>http://minimaxir.com/2015/12/lets-code-1</id>
    <content type="html"><![CDATA[<p>One of the reasons I have open-sourced the code for my complicated data visualizations is transparency for the creation process. 2015 was a <a href="http://qz.com/580859/the-most-misleading-charts-of-2015-fixed/">year of misleading and incorrect data visualizations</a>, and I don&rsquo;t want to help contribute to the misconception that data can be used for trickery. &ldquo;Big data&rdquo; in particular is a area where the steps to reproduce results are rarely released publicly in a step-by-step manner, often in an attempt to make the resulting analysis unimpeachable.</p>

<p>It&rsquo;s time to take things to the next level of transparency by recording <a href="https://en.wikipedia.org/wiki/Screencast">screencasts</a> of my data analysis and visualizations.</p>

<p>Last week, ggplot2 author Hadley Wickham released <a href="http://blog.rstudio.org/2015/12/21/ggplot2-2-0-0/">a surprise update</a> for my favorite R package, bumping the version to 2.0.0. Why not celebrate by playing around with ggplot2 and making some pretty charts?</p>

<h2>Let&rsquo;s Code!</h2>

<p>I have recorded a screencast of myself coding in R to play around with data from <a href="http://www.yelp.com/dataset_challenge">Yelp Dataset Challenge</a> and <a href="https://www.youtube.com/watch?v=Emt9bn0D5ZI">uploaded it to YouTube</a>. Additionally, the video can be played at an unusually high quality for screencasting: 1440p on supported browsers, at 60 frames per second.</p>

<div class="responsive-video"><iframe src="http://www.youtube.com/embed/Emt9bn0D5ZI " style="width: 100%; height: 100%; border: none; padding-bottom: 20px" allowfullscreen></iframe></div>

<p>This particular screencast is also my first significant attempt at working with audio/video editing and voice-over. Feel free to provide suggestions for future videos.</p>

<p>Since the screencast is 40 minutes long (inadvertently!), I&rsquo;ve written an abridged summary of the screencast, along with some clarification of points made.</p>

<h2>Yelp Data v2</h2>

<p>A year ago I made a <a href="http://minimaxir.com/2014/09/one-star-five-stars/">blog post analyzing the same Yelp data</a>. Now that the data set contains 1.6 million reviews (as opposed to just 1.1 million back then), it might be interesting to look at it again to see if anything has changed. The data is formatted as by-line JSON: I wrote a pair of Python scripts to convert it to CSV for easy import into R.</p>

<p>The screencast centralizes on three R packages: readr, dplyr, and ggplot2. (all authored by Hadley Wickham)</p>

<p>Loading the dataset into R is easy and fast with <code>read_csv</code>.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_reviews <span class="o">&lt;-</span> read_csv<span class="p">(</span><span class="s">&quot;yelp_reviews.csv&quot;</span><span class="p">)</span></code></pre></figure>

<p>Since dplyr was loaded beforehand, read_csv loads the data into a tbl_df instead of a normal data.frame. When you call a normal data.frame by itself, <em>all data is printed to console</em>, which is a problem when you have 1.6M rows (yes, that happened during a test recording). Calling a tbl_df results in a very descriptive overview of the data:</p>

<p><img src="/img/lets-code-1/overview.png" alt=""></p>

<p>Most columns are self-explanatory. <code>review_length</code> is approximate number of words in the review, <code>pos_words</code> is the number of positive words in the review, <code>neg_words</code> is what you expect, <code>net_sentiment</code> is pos_words - neg_words.</p>

<p>A quick way to analyze the distribution of numerical data is to perform a summary on the data frame, which returns a by-column <a href="https://en.wikipedia.org/wiki/Five-number_summary">five-number summary</a> + mean:</p>

<p><img src="/img/lets-code-1/summary.png" alt=""></p>

<p>Ratings are biased toward 4 and 5 star reviews. There is a lot of skew for review length.</p>

<p>dplyr makes it easy to add columns in-line with the <code>mutate</code> command. Let&rsquo;s normalize the pos_words column:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_reviews <span class="o">&lt;-</span> df_reviews <span class="o">%&gt;%</span> mutate<span class="p">(</span>pos_norm <span class="o">=</span> pos_words <span class="o">/</span> review_length<span class="p">)</span></code></pre></figure>

<p>And we could do similar steps for the neg_words column too. Or use mutate to transform the data of an existing column.</p>

<p>Onto ggplot2. If you want a quick histogram of univariate data, qplot does just that. Let&rsquo;s visualize the distribution of stars.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">qplot<span class="p">(</span>data<span class="o">=</span>df_reviews<span class="p">,</span> stars<span class="p">)</span></code></pre></figure>

<p><img src="/img/lets-code-1/lc1-qplot-1.png" alt=""></p>

<p>Definitely a skew toward 4 and 5 star reviews.</p>

<p>We can do that for other variables too, like review length.</p>

<p><img src="/img/lets-code-1/lc1-qplot-2.png" alt=""></p>

<p>What about bivariate data? If you give two variables to qplot, it will create a scatter plot. Perhaps there is a relationship between the number of stars and the number of positive words?</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">qplot<span class="p">(</span>data<span class="o">=</span>df_reviews<span class="p">,</span> stars<span class="p">,</span> pos_words<span class="p">)</span></code></pre></figure>

<p>&hellip;and then we run into a problem. In this case, ggplot2 has to plot 1.6M points to screen, which can take awhile, especially if you are simultaneously using your GPU for video recording. Eventually, we get this:</p>

<p><img src="/img/lets-code-1/lc1-qplot-3.png" alt=""></p>

<p>At first glance, there appears to be a positive correlation between star rating and number of positive words, but that&rsquo;s misleading: since we don&rsquo;t have alpha transparency on the points, the density is ambiguous. (fixing it requires working outside of a qplot).</p>

<h2>Serious Business Data</h2>

<p>We load the Yelp Businesses data into R through the same way as the reviews data. Here&rsquo;s an overview of the data:</p>

<p><img src="/img/lets-code-1/businesses.png" alt=""></p>

<p>Both data frames have a <code>business_id</code> column. We can merge them with a <code>left_join</code>, a la SQL. If both data frames have a column with the same name, it will merge on that column by default.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_reviews <span class="o">&lt;-</span> df_reviews <span class="o">%&gt;%</span> left_join<span class="p">(</span>df_businesses<span class="p">)</span></code></pre></figure>

<p>Then the R console helpfully points out that both dataframes also have a &ldquo;stars&rdquo; column. Uh-oh.</p>

<p>We reset the df_reviews data frame from scratch and merge again, explicitly stating the &ldquo;by&rdquo; column for merging. Now we know <em>where</em> reviews were made, and that might provide helpful information.</p>

<h2>Aggregation Station</h2>

<p>It might be interesting to know the average star rating by city. dplyr allows for <code>group_by</code> and <code>summarize</code> operations in a similar manner as SQL.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_cities <span class="o">&lt;-</span> df_reviews <span class="o">%&gt;%</span> group_by<span class="p">(</span>city<span class="p">)</span> <span class="o">%&gt;%</span> summarize<span class="p">(</span>avg_stars <span class="o">=</span> <span class="kp">mean</span><span class="p">(</span>stars.x<span class="p">))</span></code></pre></figure>

<p><img src="/img/lets-code-1/cities.png" alt=""></p>

<p>&hellip;that&rsquo;s not good. The original Yelp Dataset Challenge page mentioned that the dataset is only from specific cities, not &ldquo;1023 E Frye Rd.&rdquo;</p>

<p><img src="/img/lets-code-1/Dataset_Map.png" alt=""></p>

<p>Hmrph.</p>

<p>From the map, it appears there is no overlap between any of the cities with geographic states, so let&rsquo;s use <code>state</code> instead. Additionally, we can add a count of reviews from that state, and sort by that count descending.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_states <span class="o">&lt;-</span> df_reviews <span class="o">%&gt;%</span> group_by<span class="p">(</span>state<span class="p">)</span> <span class="o">%&gt;%</span> summarize<span class="p">(</span>avg_stars <span class="o">=</span> <span class="kp">mean</span><span class="p">(</span>stars.x<span class="p">),</span> count<span class="o">=</span>n<span class="p">())</span> <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>count<span class="p">))</span></code></pre></figure>

<p><img src="/img/lets-code-1/states.png" alt=""></p>

<p>Looks good enough, but that&rsquo;s tempting fate.</p>

<h2>ggplot All the Things</h2>

<p>We can plot state vs. avg_stars with ggplot2. Setting it up is easy:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">ggplot<span class="p">(</span>data<span class="o">=</span>df_states<span class="p">,</span> aes<span class="p">(</span>state<span class="p">,</span> avg_stars<span class="p">))</span></code></pre></figure>

<p><img src="/img/lets-code-1/lc1-ggplot-1.png" alt=""></p>

<p>The blank plot is actually new to 2.0.0: running the code without any layers would normally throw an error. The axis values appear valid. Let&rsquo;s add columns via <code>geom_bar</code>:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">ggplot<span class="p">(</span>data<span class="o">=</span>df_states<span class="p">,</span> aes<span class="p">(</span>state<span class="p">,</span> avg_stars<span class="p">))</span> <span class="o">+</span> geom_bar<span class="p">()</span></code></pre></figure>

<p>&hellip;and this results in an error. geom_bar by itself does histograms on raw values, as shown in the qplots. The correct fix is to add a <code>stat=&quot;identity&quot;</code> parameter to geom_bar, which tells it to scale the bars by the given value of the aesthetic.</p>

<p><img src="/img/lets-code-1/lc1-ggplot-2.png" alt=""></p>

<p>Better. But the x-axis is cluttered and the States would look better on the y-axis. Time for a <code>coord_flip</code>.</p>

<p><img src="/img/lets-code-1/lc1-ggplot-3.png" alt=""></p>

<p>Better. Now time to fix the order. You may notice that the order of the states is alphabetical going from the bottom of the axis to the top, and R will always set this order for any character vector. We want the sort the labels by their average star rating, descending. To do that we change the internal factor labels of state volume to the specified order.</p>

<p>In the recording, this took awhile due to several brain farts (which happen often when dealing with factor ordering). First, we need to remove a few states with few reviews using a filter The easiest way to do this is to sort the original data frame by avg_stars descending, then set the factor order by using the new state order <em>in reverse</em>. (Ok, ok, it might be easier to just sort ascending and not reverse, but it makes the overview harder to visualize)</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_states <span class="o">&lt;-</span> df_states <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>avg_stars<span class="p">))</span> <span class="o">%&gt;%</span> filter<span class="p">(</span>count <span class="o">&gt;</span> <span class="m">2000</span><span class="p">)</span> <span class="o">%&gt;%</span> mutate<span class="p">(</span>state <span class="o">=</span> <span class="kp">factor</span><span class="p">(</span>state<span class="p">,</span> levels<span class="o">=</span><span class="kp">rev</span><span class="p">(</span>state<span class="p">)))</span></code></pre></figure>

<p><img src="/img/lets-code-1/states-2.png" alt=""></p>

<p>Rerunning the plot code afterward yields:</p>

<p><img src="/img/lets-code-1/lc1-ggplot-4.png" alt=""></p>

<p>Good! Why not add labels for each point? This can be done with geom_text, along with adding <code>hjust=1</code> to offset the label, changing the size, and setting the text to white. We can round the avg_star values to 2 decimal places as well.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">ggplot<span class="p">(</span>data<span class="o">=</span>df_states<span class="p">,</span> aes<span class="p">(</span>state<span class="p">,</span> avg_stars<span class="p">))</span> <span class="o">+</span> geom_bar<span class="p">(</span>stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span> coord_flip<span class="p">()</span> <span class="o">+</span> geom_text<span class="p">(</span>aes<span class="p">(</span>label<span class="o">=</span><span class="kp">round</span><span class="p">(</span>avg_stars<span class="p">,</span> <span class="m">2</span><span class="p">)),</span> hjust<span class="o">=</span><span class="m">1</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;white&quot;</span><span class="p">)</span></code></pre></figure>

<p><img src="/img/lets-code-1/lc1-ggplot-5.png" alt=""></p>

<p>The &ldquo;3.7&rdquo; label requires using the <code>sprintf</code> function instead of <code>round</code> to print &ldquo;3.70&rdquo;, which is not fun. Otherwise, these labels are nice so far. Why not add a theme and axis labels?</p>

<p>I go to my <a href="http://minimaxir.com/2015/02/ggplot-tutorial/">previous ggplot2 tutorial</a> and copy-paste the FiveThirtyEight-inspired theme from there because I am efficient. (The theme required loading the RColorBrewer package, though). The axis labels are added through the <code>labs</code> function. (note that since the axes are flipped, the labels must be flipped too!)</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">ggplot<span class="p">(</span>data<span class="o">=</span>df_states<span class="p">,</span> aes<span class="p">(</span>state<span class="p">,</span> avg_stars<span class="p">))</span> <span class="o">+</span> geom_bar<span class="p">(</span>stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span> coord_flip<span class="p">()</span> <span class="o">+</span> geom_text<span class="p">(</span>aes<span class="p">(</span>label<span class="o">=</span><span class="kp">round</span><span class="p">(</span>avg_stars<span class="p">,</span> <span class="m">2</span><span class="p">)),</span> hjust<span class="o">=</span><span class="m">2</span><span class="p">,</span> size<span class="o">=</span><span class="m">2</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;white&quot;</span><span class="p">)</span> <span class="o">+</span> fte_theme<span class="p">()</span> <span class="o">+</span> labs<span class="p">(</span>y<span class="o">=</span><span class="s">&quot;Average Star Rating by State&quot;</span><span class="p">,</span> x<span class="o">=</span><span class="s">&quot;State&quot;</span><span class="p">,</span> title<span class="o">=</span><span class="s">&quot;Average Yelp Review Star Ratings by State&quot;</span><span class="p">)</span></code></pre></figure>

<p><img src="/img/lets-code-1/lc1-ggplot-6.png" alt=""></p>

<p>Why not add 95% confidence intervals for each average? (Note that the normality assumptions for the confidence interval may not be entirely valid). We can calculate the standard error of the mean and rebuild the dataframe and reorder factors again.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_states <span class="o">&lt;-</span> df_reviews <span class="o">%&gt;%</span> group_by<span class="p">(</span>state<span class="p">)</span> <span class="o">%&gt;%</span> summarize<span class="p">(</span>avg_stars <span class="o">=</span> <span class="kp">mean</span><span class="p">(</span>stars.x<span class="p">),</span> count<span class="o">=</span>n<span class="p">(),</span> se_mean<span class="o">=</span>sd<span class="p">(</span>stars.x<span class="p">)</span><span class="o">/</span><span class="kp">sqrt</span><span class="p">(</span>count<span class="p">))</span> <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>avg_stars<span class="p">))</span> <span class="o">%&gt;%</span> filter<span class="p">(</span>count <span class="o">&gt;</span> <span class="m">2000</span><span class="p">)</span> <span class="o">%&gt;%</span> mutate<span class="p">(</span>state <span class="o">=</span> <span class="kp">factor</span><span class="p">(</span>state<span class="p">,</span> levels<span class="o">=</span><span class="kp">rev</span><span class="p">(</span>state<span class="p">)))</span></code></pre></figure>

<p>Time to add a <code>geom_errorbar</code> (not a <code>geom_crossbar</code>!)</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">ggplot<span class="p">(</span>data<span class="o">=</span>df_states<span class="p">,</span> aes<span class="p">(</span>state<span class="p">,</span> avg_stars<span class="p">))</span> <span class="o">+</span> geom_bar<span class="p">(</span>stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span> coord_flip<span class="p">()</span> <span class="o">+</span> geom_text<span class="p">(</span>aes<span class="p">(</span>label<span class="o">=</span><span class="kp">round</span><span class="p">(</span>avg_stars<span class="p">,</span> <span class="m">2</span><span class="p">)),</span> hjust<span class="o">=</span><span class="m">2</span><span class="p">,</span> size<span class="o">=</span><span class="m">2</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;white&quot;</span><span class="p">)</span> <span class="o">+</span> fte_theme<span class="p">()</span> <span class="o">+</span> labs<span class="p">(</span>y<span class="o">=</span><span class="s">&quot;Average Star Rating by State&quot;</span><span class="p">,</span> x<span class="o">=</span><span class="s">&quot;State&quot;</span><span class="p">,</span> title<span class="o">=</span><span class="s">&quot;Average Yelp Review Star Ratings by State&quot;</span><span class="p">)</span> <span class="o">+</span> geom_errorbar<span class="p">(</span>aes<span class="p">(</span>ymin<span class="o">=</span>avg_stars <span class="o">-</span> <span class="m">1.96</span> <span class="o">*</span> se_mean<span class="p">,</span> ymax<span class="o">=</span>avg_stars <span class="o">+</span> <span class="m">1.96</span> <span class="o">*</span> se_mean<span class="p">))</span></code></pre></figure>

<p><img src="/img/lets-code-1/lc1-ggplot-7.png" alt=""></p>

<p>Averages are very stable for all cities due to the large sample size.</p>

<p>At this point I realized the recording is too long and I end it there. For a normal blog post, I&rsquo;d add more theming, adjust colors so they don&rsquo;t clash, and add annotations, such as a line representing the true review average from the population. And ideally, performing statistical tests to determine if any averages are different from the population average.</p>

<p>Hopefully this gives some insight into the mechanical process of creating simple data visualizations with R and ggplot2 (the &ldquo;abridged summary&rdquo; ended up being as long as a typical blog post!). As my screencast shows, programming is a recurring process of saying &ldquo;this is easy to do!&rdquo; then failing miserably for stupid reasons. Even after the 40 minute screencast, there&rsquo;s still much, much more polish needed for the data visualization. My blog posts take a very long time to produce for those reasons; the clear, clean code from the finished product is not indicative of the unexpected errors that occur when writing it.</p>

<p>I did this recording &ldquo;blind&rdquo; to test whether or not it&rsquo;s feasible for me to <em>stream</em> the coding of data visualization on services like <a href="http://www.twitch.tv">Twitch</a>. It&rsquo;s definitely possible, but has more logistical challenges. (namely, that <a href="https://obsproject.com">OBS</a> is fussy outside of Windows and I still need to figure out how to configure it optimally). I admit the code in this screencast may not be the highest-quality code (in retrospect I should have put the code in an editor instead of directly in the console, and reuse dataframe/ggplot objects), but the transparent process for coding data visualizations is important. If there is enough interest, I may revisit Yelp data again, or even more advanced datasets.</p>

<hr>

<p><em>You can access the R code used for the data visualizations and the Python scripts used to process the raw Yelp dataset <a href="https://github.com/minimaxir/lets-code-1">in this GitHub repository</a>. However, the raw data itself cannot be redistributed.</em></p>

<p><em>For those wondering what I used for recording the screencast:</em></p>

<p>Computer: <em>Late 2013 13&quot; Retina MacBook Pro running OS X 10.11.2</em></p>

<p>Recording Software: <em>Screenflow 4.5</em></p>

<p>Microphone: <em>Shure MV5 Digital Condenser</em></p>

<p>Music: <em>Various artists from the &ldquo;No Attribution Required&rdquo; section of the YouTube Audio Library</em></p>
]]></content>
  </entry>
  
</feed>
