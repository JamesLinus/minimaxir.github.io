<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[minimaxir | Max Woolf's Blog]]></title>
  <link href="http://minimaxir.com/rss.xml" rel="self"/>
  <link href="http://minimaxir.com/"/>
  <updated>2015-12-20T19:19:02-08:00</updated>
  <id>http://minimaxir.com/</id>
  <author>
    <name><![CDATA[Max Woolf]]></name>
    <email><![CDATA[max@minimaxir.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Mapping Where Arrests Frequently Occur in San Francisco Using Crime Data]]></title>
    <link href="http://minimaxir.com/2015/12/sf-arrest-maps/"/>
    <updated>2015-12-07T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2015/12/sf-arrest-maps</id>
    <content type="html"><![CDATA[<p>In my previous post, <a href="http://minimaxir.com/2015/12/sf-arrests/">Analyzing San Francisco Crime Data to Determine When Arrests Frequently Occur</a>, I found out that there are trends where SF Police arrests occur more frequently than others. By processing the <a href="https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry">SFPD Incidents dataset</a> from the <a href="https://data.sfgov.org">SF OpenData portal</a>, I found that arrests typically occur on Wednesdays at 4-5 PM, and that the type of crime is relevant to the frequency of the crime. (e.g. DUIs happen late Friday/Saturday night).</p>

<p>However, I could not understand <em>why</em> Wednesday/4-5PM is a peak time for arrests. In addition to analyzing <em>when</em> arrests occur, I also looked at <em>where</em> arrests occur. For example, perhaps more crime happens as people are leaving work; in that case, we would expect to see crimes downtown.</p>

<h2>Making a Map of SF Arrests</h2>

<p>Continuing from the previous analysis, I have a data frame of all police arrests that have occurred in San Francisco from 2003 - 2015 (587,499 arrests total).</p>

<p><img src="http://minimaxir.com/img/sf-arrest-map/arrests.png" alt="" /></p>

<p>What is the most efficient way to make a map for the data? There are too many data points for rendering each point in a tool like <a href="http://www.tableau.com">Tableau</a> or <a href="https://www.google.com/maps">Google Maps</a>. I can use <code>ggplot2</code> again as I did <a href="http://minimaxir.com/2015/11/nyc-ggplot2-howto/">to make a map of New York City</a> manually, but as noted in that article, the abstract nature of the map may hide information.</p>

<p>Enter <code>ggmap</code>. ggmap, an <a href="https://github.com/dkahle/ggmap">R package by David Kahle</a>, is a tool that allows the user to retrieve a map image from a number of map data providers, and integrates seamlessly with ggplot2 for simple visualization creation. Kahle and Hadley Wickham (the creator of ggplot2) <a href="https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf">coauthored a paper</a> describing practical applications of ggmap.</p>

<p>I will include most of the map generation code in-line. <em>For more detailed code and output, a <a href="https://github.com/minimaxir/sf-arrests-when-where/blob/master/crime_data_sf.ipynb">Jupyter notebook</a> containing the code and visualizations used in this article is available open-source on GitHub.</em></p>

<p>By default, you can ask ggmap just for a location using <code>get_map()</code>, and it will give you an approximate map around that location. You can configure the zoom level on that point as well. Optionally, if you need precise bounds for the map, you can set the bounding box manually, and the <a href="http://boundingbox.klokantech.com">Bounding Box Tool</a> works extremely well for this purpose, with the CSV coordinate export already being in the correct format.</p>

<p>ggmap allows maps from sources such as <a href="https://www.google.com/maps">Google Maps</a> and <a href="http://www.openstreetmap.org/#map=5/51.500/-0.100">OpenStreetMap</a>, and the maps can be themed, such as a color map of a black-and-white map. A black-and-white minimalistic map would be best for readability. A <a href="https://www.reddit.com/r/dataisbeautiful/comments/3ule41/mapping_restaurants_in_san_francisco_by_health/">Reddit submission by /u/all_genes_considered</a> used <a href="http://maps.stamen.com/#terrain/12/37.7706/-122.3782">Stamen maps</a> as a source with the toner-lite theme, and that worked well.</p>

<p>Since we&rsquo;ve identified the map parameters, now we can request an appropriate map:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">bbox <span class="o">=</span> <span class="kt">c</span><span class="p">(</span><span class="m">-122.516441</span><span class="p">,</span><span class="m">37.702072</span><span class="p">,</span><span class="m">-122.37276</span><span class="p">,</span><span class="m">37.811818</span><span class="p">)</span>

map <span class="o">&lt;-</span> get_map<span class="p">(</span>location <span class="o">=</span> bbox<span class="p">,</span> <span class="kn">source</span> <span class="o">=</span> <span class="s">&quot;stamen&quot;</span><span class="p">,</span> maptype <span class="o">=</span> <span class="s">&quot;toner-lite&quot;</span><span class="p">)</span></code></pre></div>


<p>Plotting the map by itself results in something like this:</p>

<p><img src="http://minimaxir.com/img/sf-arrest-map/sf-arrest-where-0.png" alt="" /></p>

<p>On the right track (aside from two Guerrero Streets), but obviously it&rsquo;ll need some aesthetic adjustments.</p>

<p>Let&rsquo;s plot all 587,499 arrests on top of the map for fun and see what happens.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggmap<span class="p">(</span>map<span class="p">)</span> <span class="o">+</span>
            geom_point<span class="p">(</span>data <span class="o">=</span> df_arrest<span class="p">,</span> aes<span class="p">(</span>x<span class="o">=</span>X<span class="p">,</span> y<span class="o">=</span>Y<span class="p">),</span> color <span class="o">=</span> <span class="s">&quot;#27AE60&quot;</span><span class="p">,</span> size <span class="o">=</span> <span class="m">0.5</span><span class="p">,</span> alpha <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span> <span class="o">+</span>
            fte_theme<span class="p">()</span> <span class="o">+</span>
            theme<span class="p">(</span><span class="kc">...</span><span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Locations of Police Arrests Made in San Francisco from 2003 – 2015&quot;</span><span class="p">)</span></code></pre></div>


<ul>
<li><code>ggmap()</code> sets up the base map.</li>
<li><code>geom_point()</code> plots points. The data for plotting is specified at this point. &ldquo;color&rdquo; and &ldquo;size&rdquo; parameters do just that. An alpha of 0.01 causes each point to be 99% transparent; therefore, addresses with a lot of points will be more opaque.</li>
<li><code>fte_theme()</code> is my theme based on the FiveThirtyEight style.</li>
<li><code>theme()</code> is needed for a few additional theme tweaks to remove the axes/margins</li>
<li><code>labs()</code> is for labeling the plot (<em>always</em> label!)</li>
</ul>


<p>Rendering the plot results in:</p>

<p><a href="http://i.imgur.com/Xu8wXzc.png" target=_blank><img src="http://minimaxir.com/img/sf-arrest-map/sf-arrest-where-1.png" alt="" /></a></p>

<p><em>NOTE: All maps in this article are embedded at a lower size to ensure that the article doesn&rsquo;t take days to load. To load a high-resolution version of any map, click on it and it will open in a new tab.</em></p>

<p><em>Additionally, since ggmap forces plots to a fixed ratio, this results in the &ldquo;random white space&rdquo; problem mentioned in the NYC article, for which I still have not found a solution, but have minimized the impact.</em></p>

<p>It&rsquo;s clear to see where arrests in the city occur. A large concentration in the Tenderloin and the Mission, along with clusters in Bayview and Fisherman&rsquo;s Wharf. However, point-stacking is not helpful when comparing high-density areas, so this visualization can be optimized.</p>

<p>How about faceting by type of crime again? We can render a map of San Francisco for each type of crime, and then we can see if the clusters for a given type of crime are different from others.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggmap<span class="p">(</span>map<span class="p">)</span> <span class="o">+</span>
            geom_point<span class="p">(</span>data <span class="o">=</span> df_arrest <span class="o">%&gt;%</span> filter<span class="p">(</span>Category <span class="o">%in%</span> df_top_crimes<span class="o">$</span>Category<span class="p">[</span><span class="m">2</span><span class="o">:</span><span class="m">19</span><span class="p">]),</span> aes<span class="p">(</span>x<span class="o">=</span>X<span class="p">,</span> y<span class="o">=</span>Y<span class="p">,</span> color<span class="o">=</span>Category<span class="p">),</span> size<span class="o">=</span><span class="m">0.75</span><span class="p">,</span> alpha<span class="o">=</span><span class="m">0.05</span><span class="p">)</span> <span class="o">+</span>
            fte_theme<span class="p">()</span> <span class="o">+</span>
            theme<span class="p">(</span><span class="kc">...</span><span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Locations of Police Arrests Made in San Francisco from 2003 – 2015, by Type of Crime&quot;</span><span class="p">)</span> <span class="o">+</span>
            facet_wrap<span class="p">(</span><span class="o">~</span> Category<span class="p">,</span> nrow <span class="o">=</span> <span class="m">3</span><span class="p">)</span></code></pre></div>


<p>Again, only one line of new code for the facet, although the source data needs to be filtered as it was in the previous post.</p>

<p>Running the code yields:</p>

<p><a href="http://i.imgur.com/6NgzV3k.jpg" target=_blank><img src="http://minimaxir.com/img/sf-arrest-map/sf-arrest-where-2.png" alt="" /></a></p>

<p>This is certainly more interesting (and pretty). Some crimes, such as Assaults, Drugs/Narcotics, and Warrants, occur all over the city. Other crimes, such as Disorderly Conduct and Robbery, primarily have clusters in the Tenderloin and in the Mission close to the 16th Street BART stop. (Prostitution notably has a cluster in the Mission and a cluster <em>above</em> the Tenderloin.)</p>

<p>Again, we can&rsquo;t compare high-density points, so now we should probably normalize the data by facet. One way to do this is to weight each point by the reciprocal of the number of points in the facet (e.g. if there are 5,000 Fraud arrests, assign a weight of 1/5000 to each Fraud arrest), and aggregate the sums of the weights in a geographical area.</p>

<p>We can reuse the normalization code from the previous post, and the hex overlay code from my NYC taxi plot post as well:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">sum_thresh <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">,</span> threshold <span class="o">=</span> <span class="m">10</span><span class="o">^</span><span class="m">-3</span><span class="p">)</span> <span class="p">{</span>
    <span class="kr">if</span> <span class="p">(</span><span class="kp">sum</span><span class="p">(</span>x<span class="p">)</span> <span class="o">&lt;</span> threshold<span class="p">)</span> <span class="p">{</span><span class="kr">return</span> <span class="p">(</span><span class="kc">NA</span><span class="p">)}</span>
    <span class="kr">else</span> <span class="p">{</span><span class="kr">return</span> <span class="p">(</span><span class="kp">sum</span><span class="p">(</span>x<span class="p">))}</span>
<span class="p">}</span>

plot <span class="o">&lt;-</span> ggmap<span class="p">(</span>map<span class="p">)</span> <span class="o">+</span>
            stat_summary_hex<span class="p">(</span>data <span class="o">=</span> df_arrest <span class="o">%&gt;%</span> filter<span class="p">(</span>Category <span class="o">%in%</span> df_top_crimes<span class="o">$</span>Category<span class="p">[</span><span class="m">2</span><span class="o">:</span><span class="m">19</span><span class="p">])</span> <span class="o">%&gt;%</span> group_by<span class="p">(</span>Category<span class="p">)</span> <span class="o">%&gt;%</span> mutate<span class="p">(</span>w<span class="o">=</span><span class="m">1</span><span class="o">/</span>n<span class="p">()),</span> aes<span class="p">(</span>x<span class="o">=</span>X<span class="p">,</span> y<span class="o">=</span>Y<span class="p">,</span> z<span class="o">=</span>w<span class="p">),</span> fun<span class="o">=</span>sum_thresh<span class="p">,</span> alpha <span class="o">=</span> <span class="m">0.8</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;#CCCCCC&quot;</span><span class="p">)</span> <span class="o">+</span>
            fte_theme<span class="p">()</span> <span class="o">+</span>
            theme<span class="p">(</span><span class="kc">...</span><span class="p">)</span> <span class="o">+</span>
            scale_fill_gradient<span class="p">(</span>low <span class="o">=</span> <span class="s">&quot;#DDDDDD&quot;</span><span class="p">,</span> high <span class="o">=</span> <span class="s">&quot;#2980B9&quot;</span><span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Locations of Police Arrests Made in San Francisco from 2003 – 2015, Normalized by Type of Crime&quot;</span><span class="p">)</span> <span class="o">+</span>
            facet_wrap<span class="p">(</span><span class="o">~</span> Category<span class="p">,</span> nrow <span class="o">=</span> <span class="m">3</span><span class="p">)</span></code></pre></div>


<ul>
<li><code>sum_thresh()</code> is a helper function that aggregates the sums of weights, but will not plot the corresponding hex if there is not enough data at that location.</li>
<li><code>scale_fill_gradient()</code> sets the gradient for the chart. If there are few arrests, the hex will be gray; if there are many arrests, it will be deep blue.</li>
</ul>


<p>Putting it all together:</p>

<p><a href="http://i.imgur.com/LXKPseq.jpg" target=_blank><img src="http://minimaxir.com/img/sf-arrest-map/sf-arrest-where-3.png" alt="" /></a></p>

<p>This confirms the interpretations mentioned above.</p>

<p>Since the code base is already created, it is very simple to facet on any variable. So why not create a faceted map for <em>every conceivable variable</em>?</p>

<p>How about checking arrest locations by Police Districts?</p>

<p><a href="http://i.imgur.com/i82wsIZ.png" target=_blank><img src="http://minimaxir.com/img/sf-arrest-map/sf-arrest-where-4.png" alt="" /></a></p>

<p>The map shows that the hex plotting works correctly, at the least. Notably, the Central, Northern, and Southern Police Districts end up making a large proportion of their arrests nearby the Tenderloin/Market Street instead of anywhere else in their area of perview.</p>

<p>Is the location of arrests seasonal? Does it vary by the month the arrest occured?</p>

<p><a href="http://i.imgur.com/u2eQMZf.png" target=_blank><img src="http://minimaxir.com/img/sf-arrest-map/sf-arrest-where-5.png" alt="" /></a></p>

<p>Nope. Still Tenderloin and Mission.</p>

<p>Maybe the locations of arrests have changed over time, as legal polices changed. Let&rsquo;s facet by year.</p>

<p><a href="http://i.imgur.com/x4SRnkU.png" target=_blank><img src="http://minimaxir.com/img/sf-arrest-map/sf-arrest-where-6.png" alt="" /></a></p>

<p>Here things are <em>slightly</em> different across each facet; Tenderloin had a much higher concentration of arrests peaking in 2009-2010, and the concentration of yearly arrests in the Tenderloin has decreased relative to everywhere else in the city.</p>

<p>Does the location of arrests vary by the time of day? As noted earlier, there could be more arrests downtown during working hours.</p>

<p><a href="http://i.imgur.com/cDKo8Lt.png" target=_blank><img src="http://minimaxir.com/img/sf-arrest-map/sf-arrest-where-7.png" alt="" /></a></p>

<p>Higher relative concentration in Tenderlion/Mission during work hours, lesser during the night.</p>

<p>Last try. Perhaps the day of week leads to different locations, especially as people tend to go out to bars all across the city.</p>

<p><a href="http://i.imgur.com/tNBRilL.png" target=_blank><img src="http://minimaxir.com/img/sf-arrest-map/sf-arrest-where-8.png" alt="" /></a></p>

<p><em>Zero</em> difference. Eh.</p>

<p>We did learn that there is certainly a lot of arrests in the Tenderloin and 16th Street/Mission BART stop, though. However, that doesn&rsquo;t necessarily mean there is more <em>crime</em> in those areas (correlation does not imply causation), but it is something worth noting when traveling around the San Francisco.</p>

<h2>Bonus: Do Social Security payments lead to an increase in arrests?</h2>

<p>In response to my previous article, <a href="https://www.reddit.com/r/sanfrancisco/comments/3vfgg2/analyzing_san_francisco_crime_data_to_determine/cxn29wd">Redditor /u/NowProveIt hypothesizes</a> that the spike in Wednesday arrests could be attributed to <a href="https://www.ssa.gov/kc/rp_paybenefits.htm">Social Security disability</a> (RDSI) payments. The <a href="https://www.socialsecurity.gov/pubs/EN-05-10031-2015.pdf">Social Security Benefit Payments schedule</a> is typically every second, third, and fourth Wednesday of a month.</p>

<p>Normally, you would expect that the arrest behavior for any Wednesday in a given month to be independent from each other. Therefore, if the arrest behavior for the <em>first</em> Wednesday is different than that for the secord/third/fourth Wednesday (presumably, the First Wednesday has fewer arrests overall), then we might have a lead.</p>

<p>Through more <code>dplyr</code> shenanigans, I am able to filter the dataset of arrests to Wednesday arrests only, and classify each Wednesday as the first, second, third, or fourth of the month. (there are occasionally fifth Wednesdays but no one cares about those).</p>

<p><img src="http://minimaxir.com/img/sf-arrest-map/ordinal.png" alt="" /></p>

<p>We can plot a single line chart for each ordinal of the number of arrests over the day. We are looking to see if the First Wednesday has different behavior than the other Wednesdays.</p>

<p><img src="http://minimaxir.com/img/sf-arrest-map/ssi-crime-1.png" alt="" /></p>

<p>&hellip;and it doesn&rsquo;t.</p>

<p>Looking at locations data doesn&rsquo;t help either.</p>

<p><a href="http://i.imgur.com/wTDKjOm.png" target=_blank><img src="http://minimaxir.com/img/sf-arrest-map/ssi-crime-2.png" alt="" /></a></p>

<p>Oh well, it was worth a shot.</p>

<p>As always, all the code and raw images are available <a href="https://github.com/minimaxir/sf-arrests-when-where">in the GitHub repository</a>. Not many more questions were answered by looking at the location data of San Francisco crimes. But that&rsquo;s OK. There&rsquo;s certainly other cool things to do with this data. Kaggle, for instance, is creating <a href="https://www.kaggle.com/c/sf-crime/scripts">a repository of scripts</a> which play around with the Crime Incident dataset.</p>

<p>But for now, at least I made a few pretty charts out of it.</p>

<hr />

<p><em>If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analyzing San Francisco Crime Data to Determine When Arrests Frequently Occur]]></title>
    <link href="http://minimaxir.com/2015/12/sf-arrests/"/>
    <updated>2015-12-04T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2015/12/sf-arrests</id>
    <content type="html"><![CDATA[<p>The <a href="https://data.sfgov.org">SF OpenData portal</a> is a good source for detailed statistics about San Francisco. One of the most popular datasets on the portal is the <a href="https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry">SFPD Incidents dataset</a>, which contains a tabular list of 1,842,050 reports (at time of writing) from 2003 to present.</p>

<p><img src="http://minimaxir.com/img/sf-arrests/incident-data.png" alt="" /></p>

<p>The data can be exported into a 377.9 MB CSV; not large enough to be considered &ldquo;big data,&rdquo; but still too heavy for programs like Excel to process efficiently. Let&rsquo;s take a look at the data using <a href="https://www.r-project.org">R</a> and see if there&rsquo;s anything interesting.</p>

<h2>Processing the Data</h2>

<p>For this article, I&rsquo;m going to do something different and illustrate the data processing step-by-step, both as a teaching tool, and to show that I am not using vague methodology to generate a narratively-convenient conclusion. <em>For more detailed code and output, a <a href="https://github.com/minimaxir/sf-arrests-when-where/blob/master/crime_data_sf.ipynb">Jupyter notebook</a> containing the code and visualizations used in this article is available open-source on GitHub.</em></p>

<p>Loading a 1.9 million row file into R can take awhile, even on modern computers with a SSD. Enter <code>readr</code>, <a href="https://github.com/hadley/readr">another R package</a> by <code>ggplot2</code> author Hadley Wickham, which grants access to a <code>read_csv()</code> function that has nearly 10x the speed of the base <code>read.csv()</code> R function, with more sensible defaults too.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">path <span class="o">&lt;-</span> <span class="s">&quot;~/Downloads/SFPD_Incidents_-_from_1_January_2003.csv&quot;</span>

df <span class="o">&lt;-</span> read_csv<span class="p">(</span>path<span class="p">)</span></code></pre></div>


<p>In memory, the data set is 180.9 MB, and removing a few useless columns (e.g. IncidentNum) further reduces the size to 126.9 MB. Since there are many redundancies in the row data (e.g. only 10 distinct PdDistrict values), R can perform memory optimizations.</p>

<p>You may have noticed in the first article image that the text data in some of the columns is in ALL CAPS, which would look ugly if the text was used in a data visualization. We can create a helper function to convert a column of text values into proper case through the use of <a href="http://stackoverflow.com/questions/15776732/how-to-convert-a-vector-of-strings-to-title-case">regular expression shenanigans</a>.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">proper_case <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">)</span> <span class="p">{</span>
    <span class="kr">return</span> <span class="p">(</span><span class="kp">gsub</span><span class="p">(</span><span class="s">&quot;\\b([A-Z])([A-Z]+)&quot;</span><span class="p">,</span> <span class="s">&quot;\\U\\1\\L\\2&quot;</span> <span class="p">,</span> x<span class="p">,</span> perl<span class="o">=</span><span class="kc">TRUE</span><span class="p">))</span>
<span class="p">}</span></code></pre></div>


<p>Now we can do more through processing using <code>dplyr</code>, <a href="https://github.com/hadley/dplyr"><em>another</em> Hadley Wickham R package</a>. dplyr is a utility that makes R easier to use: it provides a new syntax that allows data manipulation with intuitive function names, the functions can be chained using the <code>%&gt;%</code> operator for efficiency, and all data processing is <em>significantly</em> faster due to a C++ code base. (Fun fact: before the release of dplyr, I intended to quit using R for data analysis in favor of Python. Base R syntax is <em>that</em> difficult to use.)</p>

<p>In dplyr, <code>mutate</code> allows the creation and transformation of columns. We will transform the text columns by running the columns through the <code>proper_case</code> function earlier:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">df <span class="o">&lt;-</span> df <span class="o">%&gt;%</span> mutate<span class="p">(</span>Category <span class="o">=</span> proper_case<span class="p">(</span>Category<span class="p">),</span>
                 Descript <span class="o">=</span> proper_case<span class="p">(</span>Descript<span class="p">),</span>
                 PdDistrict <span class="o">=</span> proper_case<span class="p">(</span>PdDistrict<span class="p">),</span>
                 Resolution <span class="o">=</span> proper_case<span class="p">(</span>Resolution<span class="p">))</span></code></pre></div>


<p>After all that, the data looks like:</p>

<p><img src="http://minimaxir.com/img/sf-arrests/processed-table.png" alt="" /></p>

<p>Much better!</p>

<p>However, many of the records have a &ldquo;None&rdquo; value for Resolution. This implies that the police appeared at the incident but did no action, which isn&rsquo;t that helpful for analysis. How about we look at incidents which resulted in an arrest?</p>

<p>dplyr&rsquo;s  <code>filter</code> command does that, and we can use <code>grepl()</code> to do a text search for each Resolution value for the presence of &ldquo;Arrest&rdquo;.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">df_arrest <span class="o">&lt;-</span> df <span class="o">%&gt;%</span> filter<span class="p">(</span><span class="kp">grepl</span><span class="p">(</span><span class="s">&quot;Arrest&quot;</span><span class="p">,</span> Resolution<span class="p">))</span></code></pre></div>


<p>That&rsquo;s it! There are 587,499 arrests total in the dataset.</p>

<h2>Arrests Over Time</h2>

<p>One of the most simple data visualizations is a line chart, and it&rsquo;s a good starting point to use for analyzing arrests. Has the number of daily arrests been changing over time? dplyr and ggplot2 make this very easy to visualize in R.</p>

<p>First, the Date column must be formatted as a Date internally in R instead of text. Then we <code>group_by</code> the Date, and then use <code>summarize</code> to perform an aggregate on each group; in this case, count how many entries for the group. (<code>n()</code> is a convenient shortcut). We can also ensure that the dates are in ascending order.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">df_arrest_daily <span class="o">&lt;-</span> df_arrest <span class="o">%&gt;%</span>
                    mutate<span class="p">(</span>Date <span class="o">=</span> <span class="kp">as.Date</span><span class="p">(</span>Date<span class="p">,</span> <span class="s">&quot;%m/%d/%Y&quot;</span><span class="p">))</span> <span class="o">%&gt;%</span>
                    group_by<span class="p">(</span>Date<span class="p">)</span> <span class="o">%&gt;%</span>
                    summarize<span class="p">(</span>count <span class="o">=</span> n<span class="p">())</span> <span class="o">%&gt;%</span>
                    arrange<span class="p">(</span>Date<span class="p">)</span></code></pre></div>


<p><img src="http://minimaxir.com/img/sf-arrests/date-table.png" alt="" /></p>

<p>Nifty! However, keep in mind that there are thousands of days in this dataset.</p>

<p>Now we can make a pretty line chart in ggplot2. Here&rsquo;s the code, and I will explain what everything does afterward:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df_arrest_daily<span class="p">,</span> aes<span class="p">(</span>x <span class="o">=</span> Date<span class="p">,</span> y <span class="o">=</span> count<span class="p">))</span> <span class="o">+</span>
    geom_line<span class="p">(</span>color <span class="o">=</span> <span class="s">&quot;#F2CA27&quot;</span><span class="p">,</span> size <span class="o">=</span> <span class="m">0.1</span><span class="p">)</span> <span class="o">+</span>
    geom_smooth<span class="p">(</span>color <span class="o">=</span> <span class="s">&quot;#1A1A1A&quot;</span><span class="p">)</span> <span class="o">+</span>
    fte_theme<span class="p">()</span> <span class="o">+</span>
    scale_x_date<span class="p">(</span>breaks <span class="o">=</span> date_breaks<span class="p">(</span><span class="s">&quot;2 years&quot;</span><span class="p">),</span> labels <span class="o">=</span> date_format<span class="p">(</span><span class="s">&quot;%Y&quot;</span><span class="p">))</span> <span class="o">+</span>
    labs<span class="p">(</span>x <span class="o">=</span> <span class="s">&quot;Date of Arrest&quot;</span><span class="p">,</span> y <span class="o">=</span> <span class="s">&quot;# of Police Arrests&quot;</span><span class="p">,</span> title <span class="o">=</span> <span class="s">&quot;Daily Police Arrests in San Francisco from 2003 – 2015&quot;</span><span class="p">)</span></code></pre></div>


<ul>
<li><code>ggplot()</code> sets up the base chart and axes.</li>
<li><code>geom_line()</code> creates the line for the line chart. &ldquo;color&rdquo; and &ldquo;size&rdquo; parameters do just that.</li>
<li><code>geom_smooth()</code> adds a smoothing spline on top of the chart to serve as a trendline, which is helpful since there are a lot of points.</li>
<li><code>fte_theme()</code> is my theme based on the FiveThirtyEight style.</li>
<li><code>scale_x_date()</code> explicitly sets the x-axis to scale with date values. However, there are a few extremely useful formatting parameters with this function: &ldquo;breaks&rdquo; lets you set the chart breaks in plain English, and &ldquo;labels&rdquo; lets you format the dates at this breaks; in this case, there are breaks every 2 years, and only the year will be displayed for minimalism.</li>
<li><code>labs()</code> is a quick shortcut for labeling your axes and plot (<em>always</em> label!)</li>
</ul>


<p>Running the code and saving the output results in this image:</p>

<p><img src="http://minimaxir.com/img/sf-arrests/sf-arrest-when-1.png" alt="" /></p>

<p>The line chart has high variation due to the number of points (in retrospect, a 30-day moving average of arrests would work better visually). As the trendline indicates, the trend is actually <em>multimodal</em>, with daily arrest peaks in 2009 and 2014. Definitely interesting. The number of arrests appears to be on a downward trend since then.</p>

<p>The next step is to look into possible answers for the day-by-day variation.</p>

<h2>When Do Arrests Happen?</h2>

<p>One of my go-to data visualizations is a heat map of times of week; in this case, we can find which day-of-week and time-of-day when the most Arrests occur in San Francisco, and compare that with other time slots at a glance.</p>

<p>This requires the Hour and Day-of-Week to be present in separate columns: we have a DOY column already, but we need to parse the Hour component out of the HH:MM values in the Time column.</p>

<p>This requires another helper function which uses <code>strsplit()</code> to split a single time value to Hour and Minute components, take the first value (Hour), and convert that value to a numeric value (instead of text) For example, &ldquo;09:40&rdquo; input returns 9.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">get_hour <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">)</span> <span class="p">{</span>
    <span class="kr">return</span> <span class="p">(</span><span class="kp">as.numeric</span><span class="p">(</span><span class="kp">strsplit</span><span class="p">(</span>x<span class="p">,</span><span class="s">&quot;:&quot;</span><span class="p">)[[</span><span class="m">1</span><span class="p">]][</span><span class="m">1</span><span class="p">]))</span>
<span class="p">}</span></code></pre></div>


<p>Unfortunately, this will not work for an entire column. Using <code>sapply()</code> applies a specified function to each element in a column, which accomplishes the same goal.</p>

<p>The goal is to count how many Arrests occur for a given day-of-week and hour combination. In dplyr, we <code>group_by</code> both &ldquo;DayOfWeek&rdquo; and &ldquo;Hour&rdquo;, and then use <code>summarize</code> again.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">df_arrest_time <span class="o">&lt;-</span> df_arrest <span class="o">%&gt;%</span>
                    mutate<span class="p">(</span>Hour <span class="o">=</span> <span class="kp">sapply</span><span class="p">(</span>Time<span class="p">,</span> get_hour<span class="p">))</span> <span class="o">%&gt;%</span>
                    group_by<span class="p">(</span>DayOfWeek<span class="p">,</span> Hour<span class="p">)</span> <span class="o">%&gt;%</span>
                    summarize<span class="p">(</span>count <span class="o">=</span> n<span class="p">())</span></code></pre></div>


<p><img src="http://minimaxir.com/img/sf-arrests/dow-table.png" alt="" /></p>

<p>A few more tweaks are done (off camera) to convert the Hours to representations like &ldquo;12 PM&rdquo; and get everything in the correct order.</p>

<p>Now, it&rsquo;s time to make the heatmap using <code>ggplot2</code>. Here&rsquo;s the code, and I will explain what the new functions do:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df_arrest_time<span class="p">,</span> aes<span class="p">(</span>x <span class="o">=</span> Hour<span class="p">,</span> y <span class="o">=</span> DayOfWeek<span class="p">,</span> fill <span class="o">=</span> count<span class="p">))</span> <span class="o">+</span>
    geom_tile<span class="p">()</span> <span class="o">+</span>
    fte_theme<span class="p">()</span> <span class="o">+</span>
    theme<span class="p">(</span><span class="kc">...</span><span class="p">)</span> <span class="o">+</span>
    labs<span class="p">(</span>x <span class="o">=</span> <span class="s">&quot;Hour of Arrest (Local Time)&quot;</span><span class="p">,</span> y <span class="o">=</span> <span class="s">&quot;Day of Week of Arrest&quot;</span><span class="p">,</span> title <span class="o">=</span> <span class="s">&quot;# of Police Arrests in San Francisco from 2003 – 2015, by Time of Arrest&quot;</span><span class="p">)</span> <span class="o">+</span>
    scale_fill_gradient<span class="p">(</span>low <span class="o">=</span> <span class="s">&quot;white&quot;</span><span class="p">,</span> high <span class="o">=</span> <span class="s">&quot;#27AE60&quot;</span><span class="p">,</span> labels <span class="o">=</span> comma<span class="p">)</span></code></pre></div>


<ul>
<li><code>geom_title()</code> creates tiles. (instead of lines)</li>
<li><code>theme()</code> is needed for a few additional theme tweaks to get the gradient bar to render (tweaks not shown)</li>
<li><code>scale_fill_gradient()</code> tells the tiles to fill on a gradient, from white as the lowest value to a green as the highest value. The &ldquo;labels = comma&rdquo; parameter is a hidden helpful tip to allow any values in the legend to show with commas.</li>
</ul>


<p>Putting it all together:</p>

<p><img src="http://minimaxir.com/img/sf-arrests/sf-arrest-when-2.png" alt="" /></p>

<p>The heatmap is an intuitive result. Arrests don&rsquo;t happen in the early morning, and arrests tend to be elevated Friday and Saturday night, when everyone is out on the town.</p>

<p>However, the peak arrest time is apparently on Wednesdays at 4-5 PM. Wednesdays and the 4-5 PM timeslot in general have elevated arrest frequency, too. Why is that the case?</p>

<p>This requires further analysis.</p>

<h2>Facets of Arrest</h2>

<p>Perhaps the odd results can be explained by another lurking variable. Logically, certain types of crime, such as DUIs, should happen primarily at night. ggplot2 has tool known as faceting that makes such analysis easy by rendering a chart for each instance of another value in another variable. In this case, with only <em>one</em> line of ggplot2 code, we can plot a heatmap for <em>each</em> of the top types of arrests, and see if there is any significant variation in the heatmap.</p>

<p>After quickly using dplyr to aggregate and sort the top categories of arrest, by number of occurrences:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">df_top_crimes <span class="o">&lt;-</span> df_arrest <span class="o">%&gt;%</span>
                    group_by<span class="p">(</span>Category<span class="p">)</span> <span class="o">%&gt;%</span>
                    summarize<span class="p">(</span>count <span class="o">=</span> n<span class="p">())</span> <span class="o">%&gt;%</span>
                    arrange<span class="p">(</span>desc<span class="p">(</span>count<span class="p">))</span></code></pre></div>


<p><img src="http://minimaxir.com/img/sf-arrests/top-crimes.png" alt="" /></p>

<p>&ldquo;Other Offenses&rdquo; is a catch-all, so we will ignore that. Filter on the top 18 types of crime excluding Other Offenses and aggregate as usual.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">df_arrest_time_crime <span class="o">&lt;-</span> df_arrest <span class="o">%&gt;%</span>
                    filter<span class="p">(</span>Category <span class="o">%in%</span> df_top_crimes<span class="o">$</span>Category<span class="p">[</span><span class="m">2</span><span class="o">:</span><span class="m">19</span><span class="p">])</span> <span class="o">%&gt;%</span>
                    mutate<span class="p">(</span>Hour <span class="o">=</span> <span class="kp">sapply</span><span class="p">(</span>Time<span class="p">,</span> get_hour<span class="p">))</span> <span class="o">%&gt;%</span>
                    group_by<span class="p">(</span>Category<span class="p">,</span> DayOfWeek<span class="p">,</span> Hour<span class="p">)</span> <span class="o">%&gt;%</span>
                    summarize<span class="p">(</span>count <span class="o">=</span> n<span class="p">())</span></code></pre></div>


<p>Time for the heat map! The <code>ggplot</code> code is nearly identical to the previous heatmap code, except we add <code>facet_wrap()</code>.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df_arrest_time_crime<span class="p">,</span> aes<span class="p">(</span>x <span class="o">=</span> Hour<span class="p">,</span> y <span class="o">=</span> DayOfWeek<span class="p">,</span> fill <span class="o">=</span> count<span class="p">))</span> <span class="o">+</span>
    geom_tile<span class="p">()</span> <span class="o">+</span>
    fte_theme<span class="p">()</span> <span class="o">+</span>
    theme<span class="p">(</span><span class="kc">...</span><span class="p">)</span> <span class="o">+</span>
    labs<span class="p">(</span>x <span class="o">=</span> <span class="s">&quot;Hour of Arrest (Local Time)&quot;</span><span class="p">,</span> y <span class="o">=</span> <span class="s">&quot;Day of Week of Arrest&quot;</span><span class="p">,</span> title <span class="o">=</span> <span class="s">&quot;# of Police Arrests in San Francisco from 2003 – 2015, by Category and Time of Arrest&quot;</span><span class="p">)</span> <span class="o">+</span>
    scale_fill_gradient<span class="p">(</span>low <span class="o">=</span> <span class="s">&quot;white&quot;</span><span class="p">,</span> high <span class="o">=</span> <span class="s">&quot;#2980B9&quot;</span><span class="p">)</span> <span class="o">+</span>
    facet_wrap<span class="p">(</span><span class="o">~</span> Category<span class="p">,</span> nrow <span class="o">=</span> <span class="m">6</span><span class="p">)</span></code></pre></div>


<p><img src="http://minimaxir.com/img/sf-arrests/sf-arrest-when-3.png" alt="" /></p>

<p>Easy visualization to make, but it&rsquo;s not fully correct. There can only be one scale for the whole visualization, which is why the categories with lots of arrests appear colored and others do not (however, it shows that Drugs/Narcotics arrests are a large contributor to the Wednesday emphasis of the data). We need to normalize the counts by facet. dplyr has a nice trick for normalization: group by the normalization variable (Category), then mutate to add a column based on the aggregate for each unique value.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">df_arrest_time_crime <span class="o">&lt;-</span> df_arrest_time_crime <span class="o">%&gt;%</span>
                            group_by<span class="p">(</span>Category<span class="p">)</span> <span class="o">%&gt;%</span>
                            mutate<span class="p">(</span>norm <span class="o">=</span> count<span class="o">/</span><span class="kp">sum</span><span class="p">(</span>count<span class="p">))</span></code></pre></div>


<p><img src="http://minimaxir.com/img/sf-arrests/crime-norm.png" alt="" /></p>

<p>Setting the &ldquo;fill&rdquo; to &ldquo;norm&rdquo; and rerunning the heatmap code yields:</p>

<p><img src="http://minimaxir.com/img/sf-arrests/sf-arrest-when-4.png" alt="" /></p>

<p>Now things get interesting.</p>

<p>Prostitution has the most notably unique behavior, which high concentrations of arrests at night on weekdays. Drunkenness and DUIs have high concentrations at night on weekends. And Disorderly Conduct has a high concentration of arrests at 5 AM on weekdays? That&rsquo;s not intuitive.</p>

<p>Notably, some offenses have relatively random times of arrests, such as Stolen Property and Vehicle Theft.</p>

<p>However, this doesn&rsquo;t help explain why arrests tend to happen Wednesdays/4-5PM. Maybe faceting by another variable will provide more information.</p>

<p>Perhaps Police district? Maybe some PDs in San Francisco are more zealous than others. Since we created a code workflow earlier, we can apply it to any other variable very easily; in this case, it&rsquo;s mostly just replacing instances of &ldquo;Category&rdquo; with &ldquo;PdDistrict.&rdquo;</p>

<p>Doing thus yields this heatmap.</p>

<p><img src="http://minimaxir.com/img/sf-arrests/sf-arrest-when-5.png" alt="" /></p>

<p>Which isn&rsquo;t helpful. The charts are mostly identical to each other, and to the original heatmap. (Central Station (<a href="http://www.sf-police.org/Modules/ShowDocument.aspx?documentID=27554">coverage map</a>), however, has activity correlated to Drunkenness arrests.)</p>

<p>Perhaps the frequency of arrests is correlated to the time of year? How about faceting by month?</p>

<p><img src="http://minimaxir.com/img/sf-arrests/sf-arrest-when-6.png" alt="" /></p>

<p>Nope. Zero difference.</p>

<p>Last try. As shown in the line chart, the # of Arrests has oscillated over the years. Perhaps there&rsquo;s a specific year that&rsquo;s skewing the results. Let&rsquo;s facet by Year.</p>

<p><img src="http://minimaxir.com/img/sf-arrests/sf-arrest-when-7.png" alt="" /></p>

<p>Nope<sup>2</sup>. 2010-2012 have elevated Wednesday activity, but not by much.</p>

<p>This is frustrating. As of this posting, I don&rsquo;t have an obvious answer for the elevated arrests Wednesdays at 4-5PM. That being said, there definitely is still more to learn from looking at SF Crime data, although that&rsquo;s enough analysis for the time being.</p>

<p><a href="http://minimaxir.com/2015/12/sf-arrest-maps/">My next article</a> discusses how to plot arrests on a map using the <code>ggmap</code> R library, which hopefully will provide more answers. The <a href="https://github.com/minimaxir/sf-arrests-when-where">GitHub repository</a> contains a Jupyter notebook with code and visualizations for both for this article, and for the upcoming ggmap visualizations (if you want a sneak peek) which will show <em>where</em> arrests in San Francisco frequently occur.</p>

<hr />

<p><em>If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Visualize New York City Using Taxi Location Data and ggplot2]]></title>
    <link href="http://minimaxir.com/2015/11/nyc-ggplot2-howto/"/>
    <updated>2015-11-16T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2015/11/nyc-ggplot2-howto</id>
    <content type="html"><![CDATA[<p>A few months ago, I had <a href="http://minimaxir.com/2015/08/nyc-map/">posted a visualization</a> of NYC Yellow Taxis using <a href="http://ggplot2.org">ggplot2</a>, an extremely-popular R package by Hadley Wickham for data visualization. At the time, the code used for the chart was very messy since I was eager to create something cool after seeing the <a href="https://news.ycombinator.com/item?id=10003118">referenced Hacker News thread</a>. Due to popular demand, I&rsquo;ve cleaned up the code and have <a href="https://github.com/minimaxir/nyc-taxi-notebook">released it open source</a>, with a few improvements.</p>

<p>Here are some tips and tutorials on how to make such visualizations.</p>

<h2>Getting the Data</h2>

<p><em>As usual, a <a href="https://github.com/minimaxir/nyc-taxi-notebook/blob/master/nyc_taxi_map.ipynb">Jupyter notebook</a> containing the code and visualizations used in this article is available open-source on GitHub.</em></p>

<p>A quick summary of the previous post: I obtained the data from <a href="https://cloud.google.com/bigquery/">BigQuery</a>, which <a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml">was uploaded</a> from the official NYC Taxi &amp; Limousine Commission datasets, plotted each taxi point as a tiny white dot on a fully-black map, and colorized the dots depending on the number of taxis at that location.</p>

<p>In September, the <a href="https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips">BigQuery dataset</a> was updated to include all data from January 2009 to June 2015: over 1.1 <em>billion</em> Yellow Taxi rides recorded. Here&rsquo;s an updated query, which additionally calculates the total non-tip revenue for a given location, since that might be useful later, and implements a <a href="https://www.reddit.com/r/bigquery/comments/3fo9ao/nyc_taxi_trips_now_officially_shared_by_the_nyc/ctqfr8h">sanity check filter</a> noted by Felipe Hoffa.</p>

<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">ROUND</span><span class="p">(</span><span class="n">pickup_latitude</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">AS</span> <span class="n">lat</span><span class="p">,</span>
<span class="n">ROUND</span><span class="p">(</span><span class="n">pickup_longitude</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">AS</span> <span class="n">long</span><span class="p">,</span>
<span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">AS</span> <span class="n">num_pickups</span><span class="p">,</span>
<span class="k">SUM</span><span class="p">(</span><span class="n">fare_amount</span><span class="p">)</span> <span class="k">AS</span> <span class="n">total_revenue</span>
<span class="k">FROM</span> <span class="p">[</span><span class="n">nyc</span><span class="o">-</span><span class="n">tlc</span><span class="p">:</span><span class="n">yellow</span><span class="p">.</span><span class="n">trips</span><span class="p">]</span>
<span class="k">WHERE</span> <span class="n">fare_amount</span><span class="o">/</span><span class="n">trip_distance</span> <span class="k">BETWEEN</span> <span class="mi">2</span> <span class="k">AND</span> <span class="mi">10</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">lat</span><span class="p">,</span> <span class="n">long</span></code></pre></div>


<p>The resulting dataset is 4 million rows and 116MB in size! This is well over the limit for downloading from the web BigQuery client, so you must use a local client (in the attached notebook, R), and it will still take about 10-15 minutes to download (as a result, I recommend caching the results locally). Relatedly, rendering 4 million points on a single plot on screen may be computationally intensive: I strongly recommend rendering the visualization to disk by instantiating a <code>png</code> device or by using <code>ggsave</code>.</p>

<p>Here&rsquo;s a few results from that query.</p>

<p><img src="http://minimaxir.com/img/nyc-ggplot2-howto/test-data.png" alt="" /></p>

<p>As you can see, the second latitude/longitude combo is blatantly <em>wrong</em>. This isn&rsquo;t the first fidelity issue with the dataset, but we will address those in due time.</p>

<h2>Plotting the Taxis</h2>

<p>Let&rsquo;s do a basic ggplot2 plot to test things out. All we need to do is plot a small point for every lat/long combination, and then save the resulting plot.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df<span class="p">,</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>size<span class="o">=</span><span class="m">0.06</span><span class="p">)</span> <span class="o">+</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-1.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">600</span><span class="p">,</span> h<span class="o">=</span><span class="m">600</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></div>


<p><img src="http://minimaxir.com/img/nyc-ggplot2-howto/nyc-taxi-1.png" alt="" /></p>

<p>&hellip;stupid data fidelity issues.</p>

<p>This issue is fixed by constraining the plot to a <a href="https://en.wikipedia.org/wiki/Minimum_bounding_box">bounding box</a> of latitude and longitude coordinates corresponding to NYC. Flickr has <a href="https://www.flickr.com/places/info/2459115">a good starting point</a> for a NYC bounding box; I took that and edited the limits more precisely using the <a href="http://boundingbox.klokantech.com">Bounding Box Tool</a>.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">min_lat <span class="o">&lt;-</span> <span class="m">40.5774</span>
max_lat <span class="o">&lt;-</span> <span class="m">40.9176</span>
min_long <span class="o">&lt;-</span> <span class="m">-74.15</span>
max_long <span class="o">&lt;-</span> <span class="m">-73.7004</span></code></pre></div>


<p>You could also enforce the bounding box during the BigQuery. Now let&rsquo;s implement the bounding box in the plot:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df<span class="p">,</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>size<span class="o">=</span><span class="m">0.06</span><span class="p">)</span> <span class="o">+</span>
            scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_long<span class="p">,</span> max_long<span class="p">))</span> <span class="o">+</span>
            scale_y_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_lat<span class="p">,</span> max_lat<span class="p">))</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-2.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">600</span><span class="p">,</span> h<span class="o">=</span><span class="m">600</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></div>


<p><img src="http://minimaxir.com/img/nyc-ggplot2-howto/nyc-taxi-2.png" alt="" /></p>

<p>Much, much better! Now that the visualization generally looks like what we want it to be, we can start theming.</p>

<p>Let&rsquo;s start small and do just a few tweaks:</p>

<ul>
<li>Filter the data slightly to reduce some erroneous points.</li>
<li>The theme must be primarily a black background, with most of the ggplot2 theme attributes stripped out and the margins nullified. (implemented as <code>theme_map_dark()</code>; code is in the notebook)</li>
<li>Set the resolution of the rendering device to 300 DPI; this reduces some of the aliasing in the resulting image.</li>
</ul>


<div class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df <span class="o">%&gt;%</span> filter<span class="p">(</span>num_pickups <span class="o">&gt;</span> <span class="m">10</span><span class="p">),</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>color<span class="o">=</span><span class="s">&quot;white&quot;</span><span class="p">,</span> size<span class="o">=</span><span class="m">0.06</span><span class="p">)</span> <span class="o">+</span>
            scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_long<span class="p">,</span> max_long<span class="p">))</span> <span class="o">+</span>
            scale_y_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_lat<span class="p">,</span> max_lat<span class="p">))</span> <span class="o">+</span>
            theme_map_dark<span class="p">()</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-3.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">600</span><span class="p">,</span> h<span class="o">=</span><span class="m">600</span><span class="p">,</span> res<span class="o">=</span><span class="m">300</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></div>


<p><img src="http://minimaxir.com/img/nyc-ggplot2-howto/nyc-taxi-3.png" alt="" /></p>

<p>Right on track! Now time to make things more professional.</p>

<p>This requires the implementation of a few more aesthetics:</p>

<ul>
<li>Add a gradient color based on intensity of the number of pickups: since the number of pickups will logically be near streets, the coloring will be more intense near streets. Exact color doesn&rsquo;t matter; I used the purple Wisteria from <a href="https://flatuicolors.com">Flat UI Colors</a> to represent maximum intensity. Additionally, the scale should be logarithmic to make the colors stand out. (Another approach is to scale the transparency of the points instead, which is the approach <a href="http://www.brianrlance.com/blog/2015/8/7/nyc-visualized-via-taxi-pickup-locations">Brian Lance had done</a> and that works well too)</li>
<li>Annotate the theme with a proper title (and remove the scale legend; since the exact values on specific points will not be helpful)</li>
<li>Force the plot to obey the dimension ratio with <code>coord_equal()</code>, otherwise the map will stretch and distort to fill the entirety of the plotting area. (you can see a vertical stretch effect with the previous image)</li>
</ul>


<p>Putting it all together:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df <span class="o">%&gt;%</span> filter<span class="p">(</span>num_pickups <span class="o">&gt;</span> <span class="m">10</span><span class="p">),</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">,</span> color<span class="o">=</span>num_pickups<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>size<span class="o">=</span><span class="m">0.06</span><span class="p">)</span> <span class="o">+</span>
            scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_long<span class="p">,</span> max_long<span class="p">))</span> <span class="o">+</span>
            scale_y_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_lat<span class="p">,</span> max_lat<span class="p">))</span> <span class="o">+</span>
            theme_map_dark<span class="p">()</span> <span class="o">+</span>
            scale_color_gradient<span class="p">(</span>low<span class="o">=</span><span class="s">&quot;#CCCCCC&quot;</span><span class="p">,</span> high<span class="o">=</span><span class="s">&quot;#8E44AD&quot;</span><span class="p">,</span> trans<span class="o">=</span><span class="s">&quot;log&quot;</span><span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Map of NYC, Plotted Using Locations Of All Yellow Taxi Pickups&quot;</span><span class="p">)</span> <span class="o">+</span>
            theme<span class="p">(</span>legend.position<span class="o">=</span><span class="s">&quot;none&quot;</span><span class="p">)</span> <span class="o">+</span>
            coord_equal<span class="p">()</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-4.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">600</span><span class="p">,</span> h<span class="o">=</span><span class="m">600</span><span class="p">,</span> res<span class="o">=</span><span class="m">300</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></div>


<p>results in this image, which is what we want! However there is a slight problem, and I will wrap the image in a red border to demonstrate.</p>

<p><span><style>
.border img {
  border: 3px solid red;
}
</style></span></p>

<p><span class="border"><img src="http://minimaxir.com/img/nyc-ggplot2-howto/nyc-taxi-4.png" alt="" /></span></p>

<p>Due to <code>coord_equal()</code> enforcing the chart dimensions, the rendering device has a gap of white space at the top due to interaction with the <code>grid</code> graphics package that ggplot2 is based upon; normally not a problem for default charts, but a waste of space for visualizations with non-white backgrounds.</p>

<p>I attempted to fix this issue by forcing <code>grid</code> to render a black rectangle then plot on top of it. Unfornately, that was not successful. The quickest workaround is to set the image dimensions through trial-and-error such that the issue is minimized.</p>

<p>All things considered, that&rsquo;s minor but should still be noted. The streets of Manhattan are visible! And there&rsquo;s still more that can be done.</p>

<h2>Hexing the Revenue</h2>

<p>Hex map overlays are a popular technique for aggregating two-dimensional data on a 3rd dimension. ggplot2 has a relatively new <a href="http://docs.ggplot2.org/current/stat_summary_hex.html">stat_summary_hex</a> function which does just that.</p>

<p>Why not aggregate total revenue for NYC Yellow Taxi Pickups to determine where taxis generate the most money? Since we conveniently have the code to generate a map of NYC already, we can plot the hex bins on top of that map, after a few more tweaks:</p>

<ul>
<li>Set the 3rd dimension, <code>z</code>, to <code>total_revenue</code></li>
<li>Set the aggregation to the <code>sum</code> function, so it sums up all the revenues within a bin.</li>
<li>Scale the total hex revenues with a gradient.</li>
<li>Tweak all the aesthetics: color of the base points, the color of the hexes, the transparency of the hexes, and the name of the chart.</li>
<li>Set the chart dimensions to avoid the <code>coord_equal()</code> issue mention above.</li>
</ul>


<div class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df <span class="o">%&gt;%</span> filter<span class="p">(</span>num_pickups <span class="o">&gt;</span> <span class="m">20</span><span class="p">),</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">,</span> z<span class="o">=</span>total_revenue<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>size<span class="o">=</span><span class="m">0.06</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;#999999&quot;</span><span class="p">)</span> <span class="o">+</span>
            stat_summary_hex<span class="p">(</span>fun <span class="o">=</span> <span class="kp">sum</span><span class="p">,</span> bins<span class="o">=</span><span class="m">100</span><span class="p">,</span> alpha<span class="o">=</span><span class="m">0.7</span><span class="p">)</span> <span class="o">+</span>
            scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_long<span class="p">,</span> max_long<span class="p">))</span> <span class="o">+</span>
            scale_y_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_lat<span class="p">,</span> max_lat<span class="p">))</span> <span class="o">+</span>
            theme_map_dark<span class="p">()</span> <span class="o">+</span>
            scale_fill_gradient<span class="p">(</span>low<span class="o">=</span><span class="s">&quot;#CCCCCC&quot;</span><span class="p">,</span> high<span class="o">=</span><span class="s">&quot;#27AE60&quot;</span><span class="p">,</span> labels<span class="o">=</span>dollar<span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Total Revenue for NYC Yellow Taxis by Pickup Location, from Jan 2009 ― June 2015&quot;</span><span class="p">)</span> <span class="o">+</span>
            coord_equal<span class="p">()</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-5.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">950</span><span class="p">,</span> h<span class="o">=</span><span class="m">860</span><span class="p">,</span> res<span class="o">=</span><span class="m">300</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></div>


<p><img src="http://minimaxir.com/img/nyc-ggplot2-howto/nyc-taxi-5.png" alt="" /></p>

<p>That wasn&rsquo;t too bad. The gradient shows that <a href="https://www.google.com/maps/place/penn+station+nyc/@40.750568,-73.993519,15z">Penn Station</a> in Manhattan, along with the two airports, are the largest revenue generators.</p>

<p>I <a href="https://www.reddit.com/r/dataisbeautiful/comments/3sjmc0/total_revenue_for_nyc_yellow_taxis_by_pickup/">posted the hex-overlayed map</a> on Reddit to /r/dataisbeautiful as a part of my data visualization beta-testing. Although the chart received just under 200 upvotes, the comments in the Reddit thread were <em>unanimously negative</em>. Reddit user /u/DanHeidel <a href="https://www.reddit.com/r/dataisbeautiful/comments/3sjmc0/total_revenue_for_nyc_yellow_taxis_by_pickup/cwxuy1e">posted a long rant</a> on the problems with the aesthetics of the chart. And for the most part, I agree with his assessment.</p>

<p>Let&rsquo;s try again, and address the claims made in the Reddit comments.</p>

<ul>
<li>Only show hex bins where there is enough valid data, which should remove the mysterious hexes over the water. This can be implemented through a helper aggregate function which does not render the hex if the total revenue of the hex is under some threshold value. (I set it to $100,000)</li>
<li>Scale the total hex revenue logarithmically, and change the color to a Red hue (Alizarin) to make the step values more visible.</li>
<li>Zoom the chart dimensions closer to Manhattan.</li>
<li>Make a few more aesthetic tweaks.</li>
</ul>


<p>Here&rsquo;s take two:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">total_rev <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">,</span> threshold <span class="o">=</span> <span class="m">10</span><span class="o">^</span><span class="m">5</span><span class="p">)</span> <span class="p">{</span>
    <span class="kr">if</span> <span class="p">(</span><span class="kp">sum</span><span class="p">(</span>x<span class="p">)</span> <span class="o">&lt;</span> threshold<span class="p">)</span> <span class="p">{</span><span class="kr">return</span> <span class="p">(</span><span class="kc">NA</span><span class="p">)}</span>
    <span class="kr">else</span> <span class="p">{</span><span class="kr">return</span> <span class="p">(</span><span class="kp">sum</span><span class="p">(</span>x<span class="p">))}</span>
<span class="p">}</span>

plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df <span class="o">%&gt;%</span> filter<span class="p">(</span>num_pickups <span class="o">&gt;</span> <span class="m">10</span><span class="p">),</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">,</span> z<span class="o">=</span>total_revenue<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>size<span class="o">=</span><span class="m">0.06</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;#999999&quot;</span><span class="p">)</span> <span class="o">+</span>
            stat_summary_hex<span class="p">(</span>fun <span class="o">=</span> total_rev<span class="p">,</span> bins<span class="o">=</span><span class="m">100</span><span class="p">,</span> alpha<span class="o">=</span><span class="m">0.5</span><span class="p">)</span> <span class="o">+</span>
            scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">-74.0224</span><span class="p">,</span> <span class="m">-73.8521</span><span class="p">))</span> <span class="o">+</span>
            scale_y_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">40.6959</span><span class="p">,</span> <span class="m">40.8348</span><span class="p">))</span> <span class="o">+</span>
            theme_map_dark<span class="p">()</span> <span class="o">+</span>
            scale_fill_gradient<span class="p">(</span>low<span class="o">=</span><span class="s">&quot;#FFFFFF&quot;</span><span class="p">,</span> high<span class="o">=</span><span class="s">&quot;#E74C3C&quot;</span><span class="p">,</span> labels<span class="o">=</span>dollar<span class="p">,</span> trans<span class="o">=</span><span class="s">&quot;log&quot;</span><span class="p">,</span> breaks<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">10</span><span class="o">^</span><span class="p">(</span><span class="m">6</span><span class="o">:</span><span class="m">8</span><span class="p">)))</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Total Revenue for NYC Yellow Taxis by Pickup Location, from Jan 2009 ― June 2015&quot;</span><span class="p">)</span> <span class="o">+</span>
            coord_equal<span class="p">()</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-6.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">900</span><span class="p">,</span> h<span class="o">=</span><span class="m">900</span><span class="p">,</span> res<span class="o">=</span><span class="m">300</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></div>


<p><img src="http://minimaxir.com/img/nyc-ggplot2-howto/nyc-taxi-6.png" alt="" /></p>

<p>A good step forward. Revenue all through Manhattan is mostly the same except for Penn Station. Meanwhile, the hexes in LaGuardia Airport are noticeably more saturated than Penn Station.</p>

<p>Hopefully, this tutorial gave you a good look into a few interesting tricks that can be accomplished with ggplot2, even though the code can be somewhat messy. If you want more orthodox methods of plotting geographic data in ggplot2, you should look into the <a href="https://cran.r-project.org/web/packages/ggmap/index.html">ggmap</a> R package, which I used to plot <a href="http://minimaxir.com/2014/04/san-francisco/">Facebook Checkin data in San Francisco</a>, and look into the <a href="https://cran.r-project.org/web/packages/maps/index.html">maps</a> R package plus shape files, which I used to plot <a href="http://minimaxir.com/2015/01/tree-time/">Instagram photo location data</a>. Unfortunately, the code may not necessarily be less messy.</p>

<hr />

<p><em>If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Quantifying and Visualizing the Reddit Hivemind]]></title>
    <link href="http://minimaxir.com/2015/10/reddit-topwords/"/>
    <updated>2015-10-09T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2015/10/reddit-topwords</id>
    <content type="html"><![CDATA[<p>In my <a href="http://minimaxir.com/2015/10/reddit-bigquery/">last post on Reddit data</a> (I strongly suggest you read that first if you haven&rsquo;t already), I noted that analyzing the words used in Reddit submissions may be useful in quantifying the relationship of those keywords in the success of a Reddit submission. Indeed, if we can find out which topics Reddit users tend to upvote, we can identify what keywords are most attractive to the Reddit &ldquo;hivemind.&rdquo;</p>

<p>First, I gathered some preliminary statistics about all submissions to the top 500 subreddits on Reddit, again using the <a href="https://cloud.google.com/bigquery/">BigQuery</a> data dump compiled by Jason Baumgartner and Felipe Hoffa, in order to establish a good base for the analysis:</p>

<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">subreddit</span><span class="p">,</span>
  <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">AS</span> <span class="n">num_submissions</span><span class="p">,</span>
  <span class="n">ROUND</span><span class="p">(</span><span class="k">AVG</span><span class="p">(</span><span class="n">score</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">avg_score</span><span class="p">,</span>
  <span class="n">NTH</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">(</span><span class="n">score</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span> <span class="k">AS</span> <span class="n">lower_95</span><span class="p">,</span>
  <span class="n">NTH</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">(</span><span class="n">score</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span> <span class="k">AS</span> <span class="n">median</span><span class="p">,</span>
  <span class="n">NTH</span><span class="p">(</span><span class="mi">975</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">(</span><span class="n">score</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span> <span class="k">AS</span> <span class="n">upper_95</span>
<span class="k">FROM</span> <span class="p">[</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_posts</span><span class="p">.</span><span class="n">full_corpus_201509</span><span class="p">]</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">subreddit</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">num_submissions</span> <span class="k">DESC</span>
<span class="k">LIMIT</span> <span class="mi">500</span></code></pre></div>


<p>Which results in <a href="https://docs.google.com/spreadsheets/d/1lyEc7-5vkREKBV8fUreyOszICgyA0CqX2YHaK2-4ndo/edit?usp=sharing">this output</a>. The &ldquo;lower_95&rdquo; and and the &ldquo;upper_95&rdquo; columns represent the 2.5% and the 97.5% percentile respectively, which could be used to form a 95% confidence interval around the median. For example, <a href="http://reddit.com/r/funny">/r/funny</a> has a 2.5% percentile of 0 points, a median of 1 point, and a 97.5% percentile of <em>875 points</em>. However, from those values, it is clear the data is heavily skewed right, which makes running statistical tests more difficult.</p>

<p>Now we can query the top keywords for each subreddit, whose presence in a submission is related to the highest average score within that subreddit. This requires a very intricate and optimized BigQuery. Specifically, we want the query to:</p>

<ul>
<li>Get the Top 500 subreddits, by number of submissions (an abridged verson of query above).</li>
<li>Get all submissions from these subreddits.</li>
<li>Extract the keywords from all of these submissions, using a <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expression</a> to remove most punctuation (unfortunately, the regular expression will remove punctuation <em>within</em> words as well, resulting in some odd &ldquo;words&rdquo; in the final results) and flattening the resulting tokens into separate rows for aggregation.</li>
<li>Aggregate the keywords by both subreddit and the word itself, and obtain the # of distinct submissions the word is present in, the average score among all submissions, etc.</li>
<li>Keep only words which occur in atleast 1,000 distinct submissions, which is important for getting a good average.</li>
<li>For each subreddit, rank each remaining word by their average score, descending.</li>
<li>Keep only the Top 20 words for each subreddit. 500 subreddits x 20 words = 10,000 rows maximum, which the limit BigQuery allows for web download, so that works out well.</li>
</ul>


<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="p">(</span>
<span class="k">SELECT</span> <span class="n">subreddit</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">num_words</span><span class="p">,</span> <span class="n">avg_score</span><span class="p">,</span> <span class="n">lower_95</span><span class="p">,</span> <span class="n">median</span><span class="p">,</span> <span class="n">upper_95</span><span class="p">,</span> <span class="n">ROW_NUMBER</span><span class="p">()</span> <span class="n">OVER</span> <span class="p">(</span><span class="n">PARTITION</span> <span class="k">BY</span> <span class="n">subreddit</span> <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">avg_score</span> <span class="k">DESC</span><span class="p">)</span> <span class="n">score_rank</span>
<span class="k">FROM</span> <span class="p">(</span>
<span class="k">SELECT</span> <span class="n">subreddit</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="p">(</span><span class="n">id</span><span class="p">))</span> <span class="k">AS</span> <span class="n">num_words</span><span class="p">,</span> <span class="n">ROUND</span><span class="p">(</span><span class="k">AVG</span><span class="p">(</span><span class="n">score</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span> <span class="k">AS</span> <span class="n">avg_score</span><span class="p">,</span> <span class="n">NTH</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">(</span><span class="n">score</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span> <span class="k">AS</span> <span class="n">lower_95</span><span class="p">,</span> <span class="n">NTH</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">(</span><span class="n">score</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span> <span class="k">AS</span> <span class="n">median</span><span class="p">,</span> <span class="n">NTH</span><span class="p">(</span><span class="mi">975</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">(</span><span class="n">score</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span> <span class="k">AS</span> <span class="n">upper_95</span>
<span class="k">FROM</span><span class="p">(</span><span class="n">FLATTEN</span><span class="p">((</span>
  <span class="k">SELECT</span> <span class="n">SPLIT</span><span class="p">(</span><span class="k">LOWER</span><span class="p">(</span><span class="n">REGEXP_REPLACE</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">r</span><span class="s1">&#39;[^\w\&#39;</span><span class="p">]</span><span class="s1">&#39;, &#39;</span> <span class="s1">&#39;)), &#39;</span> <span class="err">&#39;</span><span class="p">)</span> <span class="n">word</span><span class="p">,</span> <span class="n">subreddit</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">id</span>
  <span class="k">FROM</span> <span class="p">[</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_posts</span><span class="p">.</span><span class="n">full_corpus_201509</span><span class="p">]</span>
  <span class="k">WHERE</span> <span class="n">subreddit</span> <span class="k">IN</span> <span class="p">(</span><span class="k">SELECT</span> <span class="n">subreddit</span> <span class="k">FROM</span> <span class="p">(</span><span class="k">SELECT</span> <span class="n">subreddit</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">AS</span> <span class="k">c</span> <span class="k">FROM</span> <span class="p">[</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_posts</span><span class="p">.</span><span class="n">full_corpus_201509</span><span class="p">]</span> <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">subreddit</span> <span class="k">ORDER</span> <span class="k">BY</span> <span class="k">c</span> <span class="k">DESC</span> <span class="k">LIMIT</span> <span class="mi">500</span><span class="p">))</span>
  <span class="p">),</span> <span class="n">word</span><span class="p">))</span>
<span class="k">GROUP</span> <span class="k">EACH</span> <span class="k">BY</span> <span class="n">subreddit</span><span class="p">,</span> <span class="n">word</span>
<span class="k">HAVING</span> <span class="n">num_words</span> <span class="o">&gt;=</span> <span class="mi">1000</span>
<span class="p">))</span>
<span class="k">WHERE</span> <span class="n">score_rank</span> <span class="o">&lt;=</span> <span class="mi">20</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">subreddit</span><span class="p">,</span> <span class="n">avg_score</span> <span class="k">DESC</span></code></pre></div>


<p>Phew! That query results in <a href="https://docs.google.com/spreadsheets/d/1MRfLR_TBO8zveKaifqVXTN0da7tf0FQOr5a2LVkZgkg/edit?usp=sharing">this output</a>, which we can use to plot a bar chart for each subreddit.</p>

<h2>Plotting the Words</h2>

<p><em>The R code used to generate the charts is available in <a href="https://github.com/minimaxir/reddit-subreddit-keywords/blob/master/reddit_subreddit_words.ipynb">this Jupyter notebook</a>, open-sourced on GitHub. Additionally, all 500 charts with the keyword ranks for all 500 subreddits are available in <a href="https://github.com/minimaxir/reddit-subreddit-keywords/tree/master/subreddit-mean">the parent repository</a>.</em></p>

<p>After a few R tricks, I managed to chart the Top 10 keywords for each of the Top 15 subreddits:</p>

<p><a href="https://i.imgur.com/dWdCnMI.png"> <img src="http://minimaxir.com/img/reddit-topwords/subreddit-means-half.png" alt="" /> </a></p>

<p><em>(Click on image for full-resolution)</em></p>

<p>As noted in the subtitle, each word appears in atleast 1,000 submissions by subreddit (which absorbs any messy outliers), and vertical line represents the true average upvotes per subreddit. One might argue that the median would be a better statistic instead of the median, due to the high skewness of the data. Thanks to the power of BigQuery, I was able to calculate the <a href="http://i.imgur.com/0PBolIq.png">top medians for each of the subreddits</a> with a slightly-tweaked query, but the chart is not as helpful. There are still a few useful implications of the median, though, which I&rsquo;ll show later. (No, I don&rsquo;t need to normalize the data in the chart since I am not making an apples-to-apples comparison between the values of the words among subreddits, and no, I don&rsquo;t need to remove stop words since this is visualizing an average, and not a count.)</p>

<p>When the visualization was <a href="https://www.reddit.com/r/dataisbeautiful/comments/3nz3zz/average_number_of_upvotes_for_reddit_submissions/">posted to Reddit</a> in the <a href="https://www.reddit.com/r/dataisbeautiful/">/r/dataisbeautiful</a> subreddit, it received over 3,500 upvotes. As many commenters on that submission correctly note, there are a few especially interesting observations for the keywords in those 15 subreddits, and when looking at the remaining subreddits, many trends in their keywords are made apparent.</p>

<h2>Politicalreddit</h2>

<p>The most notable trend with the Top 15 subreddits is in the <a href="https://www.reddit.com/r/politics/">/r/politics</a> subreddit, which tells users to &ldquo;Vote based on quality, not opinion,&rdquo; but has a high affiliation for submissions specifically involving Bernie Sanders and Elizabeth Warren.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/mean-010-politics.png" alt="" /></p>

<p><a href="https://www.reddit.com/r/technology/">/r/technology</a>, intended to be about &ldquo;a broad spectrum of conversation as to the innovations, aspirations, applications and machinations,&rdquo; has a high affiliation for political issues such as the net neutrality controversy involving the FCC, ISPs, and Comcast, and little affiliation for actual <em>technology</em>.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/mean-018-technology.png" alt="" /></p>

<p>Outside the Top 15 Subreddits, the political affiliations of subreddits such as <a href="http://reddit.com/r/Libertarian">/r/Libertarian</a> are less surprising and more funny as a result.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/mean-103-Libertarian.png" alt="" /></p>

<h2>Storyreddit</h2>

<p>One of the rising trends in the online publication landscape is that telling a story is more effective at generating attention. (thank BuzzFeed for that)</p>

<p>The biggest offender is <a href="https://www.reddit.com/r/aww/">/r/aww</a>, a subreddit about animals, has a high affinity for words <em>that aren&rsquo;t animals</em>, with only two animal words appearing in the Top 20.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/mean-011-aww.png" alt="" /></p>

<p>The presence of story titles is also apparent in <a href="https://www.reddit.com/r/Fitness/">/r/Fitness</a>, which in fairness, the atmosphere of self-improvement lends itself more to stories.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/mean-054-Fitness.png" alt="" /></p>

<h2>Metareddit</h2>

<p>The practice of upvoting submissions just because they mention a particular topic is colloquially known as &ldquo;circlejerking.&rdquo; <a href="https://www.reddit.com/r/circlejerk/">/r/circlejerk</a> fits that well, with titles designed to satirize clickbaity issues in order to receive upvotes ironically.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/mean-025-circlejerk.png" alt="" /></p>

<p><a href="https://www.reddit.com/r/Braveryjerk/">/r/Braveryjerk</a>, however, takes this practice to its logical conclusion.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/mean-486-Braveryjerk.png" alt="" /></p>

<h2>Esotericreddit</h2>

<p>Here are a few other infamous subreddits that people have requested.</p>

<p><a href="https://www.reddit.com/r/conspiracy/">/r/conspiracy</a>, which apparently have frequent instances of &ldquo;TIL&rdquo; (Today, I Learned) in titles, are simultaneously very <em>suspicious</em> of the actions in the  <a href="https://www.reddit.com/r/TIL/">/r/TIL</a> subreddit.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/mean-064-conspiracy.png" alt="" /></p>

<p>The keywords in <a href="https://www.reddit.com/r/teenagers/">/r/teenagers</a> captures the essence of modern social media.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/mean-072-teenagers.png" alt="" /></p>

<p>Some subreddits are just plain weird.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/mean-175-me_irl.png" alt="" /></p>

<p>That&rsquo;s <a href="https://www.reddit.com/r/me_irl/">/r/me_irl</a> for you.</p>

<h2>Looking at the Medians</h2>

<p>As shown in the linked median image earlier, the medians for most keywords are 1 or 2, which does not provide much visual information whether a keyword is more important than another.</p>

<p>There are notable exceptions, however, such as with <a href="https://www.reddit.com/r/Android/">/r/Android</a>.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/median-059-Android.png" alt="" /></p>

<p>That subreddit has official threads for events; it would make sense for users to upvote that whenever it appears as it&rsquo;s <em>important</em> that it&rsquo;s visible, but not necessarily as an agreement of the topic. That&rsquo;s why I believe looking at the medians is a different approach than looking at the means.</p>

<p>Same thing with <a href="https://www.reddit.com/r/relationships/">/r/relationships</a>, where an &ldquo;update&rdquo; is important.</p>

<p><img src="http://minimaxir.com/img/reddit-topwords/median-057-relationships.png" alt="" /></p>

<p>All in all, this is still just a first step for analyzing the importance of keywords in Reddit submission, and the impact of Reddit&rsquo;s hivemind. The next step would be NLP techniques such as POS tagging and TDF-IF, but those require very significant and very expensive computing power. At the least, the good results with these simple analyses validates the idea for further research.</p>

<p><em>Again the R code used to generate the charts is available in <a href="https://github.com/minimaxir/reddit-subreddit-keywords/blob/master/reddit_subreddit_words.ipynb">this Jupyter notebook</a>, open-sourced on GitHub. Additionally, all 500 charts with the keyword ranks for all 500 subreddits are available in <a href="https://github.com/minimaxir/reddit-subreddit-keywords/tree/master/subreddit-mean">the parent repository</a>.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Analyze Every Reddit Submission and Comment, in Seconds, for Free]]></title>
    <link href="http://minimaxir.com/2015/10/reddit-bigquery/"/>
    <updated>2015-10-02T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2015/10/reddit-bigquery</id>
    <content type="html"><![CDATA[<p>While working on my <a href="http://minimaxir.com/2014/12/reddit-statistics/">statistical analysis of 142 million Reddit submissions</a> last year, I had a surprising amount of trouble settings things up. It took a few hours to download the 40+ gigabytes of compressed data, and another few hours to parse the data and store in a local database. Even then, on my old-but-still-pretty-fast desktop PC, simple queries on the entire dataset took minutes to run, with complex queries occasionally taking upwards to an hour.</p>

<p>Over the past year, I&rsquo;ve had a lot of success using <a href="https://cloud.google.com/bigquery/">Google&rsquo;s BigQuery</a> tool for quick big data analysis without having to manage the data, such as the <a href="http://minimaxir.com/2015/08/nyc-map/">recent NYC Taxi data dump</a>. Recently, Jason Michael Baumgartner of <a href="https://pushshift.io">Pushshift.io</a> (a.k.a <a href="https://www.reddit.com/user/Stuck_In_the_Matrix">/u/Stuck_In_The_Matrix</a> on Reddit), who also provided me the original Reddit data, released <a href="https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/">new Reddit datasets</a> containing all submissions and all comments until August 2015. Google BigQuery Developer Advocate Felipe Hoffa <a href="https://www.reddit.com/r/bigquery/comments/3mv82i/dataset_reddits_full_post_history_shared_on/">uploaded the dataset</a> to a public table in BigQuery for anyone to perform analysis on the data.</p>

<p><img src="http://minimaxir.com/img/reddit-bigquery/bigquery-tool.png" alt="" /></p>

<p>With Reddit data in BigQuery, quantifying all the hundreds of millions of Reddit submissions and comments is trivial.</p>

<h2>Hello Reddit!</h2>

<p><em>Although the data is retrieved and processed in seconds, making the data visualizations in this post takes slightly longer. You can view the R code needed to reproduce the visualizations in <a href="https://github.com/minimaxir/reddit-bigquery/blob/master/reddit_bigquery.ipynb">this Jupyter notebook</a> open-sourced on GitHub.</em></p>

<p>BigQuery allows 1 terabyte (1000 GB) of <a href="https://cloud.google.com/bigquery/pricing">free data processing</a> per month; which is much more than it sounds like, and you&rsquo;ll see why.</p>

<p>BigQuery syntax works similar to typical SQL. If you can perform basic SQL aggregations such as <code>COUNT</code> and <code>AVG</code> on a tiny database, you can perform the same aggregations on a 100+ GB dataset.</p>

<p>We can write a simple query to just calculate how many Reddit submissions are made each day, to both check the robustness of the data, and to show how much Reddit has grown. (note that the <code>created</code> field is in seconds UTC; you will need to convert it to a timestamp, then convert to a <code>DATE</code> which converts the timestamp to a YYYY-MM-DD string, which is the format you want since it sorts lexigraphically)</p>

<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="nb">DATE</span><span class="p">(</span><span class="n">SEC_TO_TIMESTAMP</span><span class="p">(</span><span class="n">created</span><span class="p">))</span> <span class="n">date_submission</span><span class="p">,</span>
<span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">as</span> <span class="n">num_submissions</span>
<span class="k">FROM</span> <span class="p">[</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_posts</span><span class="p">.</span><span class="n">full_corpus_201509</span><span class="p">]</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">date_submission</span>
<span class="k">ORDER</span> <span class="k">by</span> <span class="n">date_submission</span></code></pre></div>


<p>Which results in a simple timeseries, as shown above. BigQuery allows you to download the data as a CSV, and it can easily be visualized in any statistical program such as Excel.</p>

<p>Of course, I prefer R and ggplot2.</p>

<p><img src="http://minimaxir.com/img/reddit-bigquery/reddit-bigquery-1.png" alt="" /></p>

<p>And that query only took 4.5 seconds to complete, with 1.46 GB data processed! BigQuery only counts the columns used in the query against the 1 TB quota; since the query only used one small column, the query is cheap. (if the query hits the raw submission title/comment text data, then the queries will be significantly larger data-wise)</p>

<h2>When is the best time to post to Reddit?</h2>

<p>One of the reasons people might look at Reddit data is for determining the best way to viral. One of the most important factors in making something go viral is timing; since Reddit has a ranking system based on both community approval and time-since-submission, along with the fact that there is more activity at certain times of the day, the time when a submission is made is especially important.</p>

<p>So here&rsquo;s another aggregation query that aggregates on the day of week a submission is made, and the hour when the submission is made; this creates a 7x24 matrix of timeslot possibilities. Both values are set to Eastern Standard Time, as U.S.-target websites tend to follow that timezone. Lastly, instead of checking the average amount of points for each submission at each timeslot (which would be skewed by the very high proportion of submissions with no upvotes), we&rsquo;ll look at how many submissions go viral (>3000) at each timeslot with a conditional <code>SUM(IF())</code> statement (which is equivalent to Excel&rsquo;s <code>COUNTIF</code> conditional)</p>

<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span>
  <span class="n">DAYOFWEEK</span><span class="p">(</span><span class="n">SEC_TO_TIMESTAMP</span><span class="p">(</span><span class="n">created</span> <span class="o">-</span> <span class="mi">60</span><span class="o">*</span><span class="mi">60</span><span class="o">*</span><span class="mi">5</span><span class="p">))</span> <span class="k">as</span> <span class="n">sub_dayofweek</span><span class="p">,</span>
  <span class="n">HOUR</span><span class="p">(</span><span class="n">SEC_TO_TIMESTAMP</span><span class="p">(</span><span class="n">created</span> <span class="o">-</span> <span class="mi">60</span><span class="o">*</span><span class="mi">60</span><span class="o">*</span><span class="mi">5</span><span class="p">))</span> <span class="k">as</span> <span class="n">sub_hour</span><span class="p">,</span>
  <span class="k">SUM</span><span class="p">(</span><span class="n">IF</span><span class="p">(</span><span class="n">score</span> <span class="o">&gt;=</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="k">as</span> <span class="n">num_gte_3000</span><span class="p">,</span>
<span class="k">FROM</span> <span class="p">[</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_posts</span><span class="p">.</span><span class="n">full_corpus_201509</span><span class="p">]</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">sub_dayofweek</span><span class="p">,</span> <span class="n">sub_hour</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">sub_dayofweek</span><span class="p">,</span> <span class="n">sub_hour</span></code></pre></div>


<p>2.5 seconds, 2.39 GB processed, and a few R tricks results in this:</p>

<p><img src="http://minimaxir.com/img/reddit-bigquery/reddit-bigquery-2.png" alt="" /></p>

<p>The day-of-week mostly does not matter, but hour matters significantly: about 3x as many submissions go viral when they are posted in the morning EST than if they are posted late in the day. (and rarely anything comparatively goes viral posted late at night, which is intuitive enough)</p>

<h2>Creating Wordclouds of Subreddit Comments</h2>

<p>Wordclouds are always a fun representation of data, although not necessarily the most quantifiable. BigQuery can help derive word counts on large quantities of data, although the query is much more complex.</p>

<p>Due to the amount of data, we&rsquo;ll only look at the latest Reddit comment data (August 2015), and we&rsquo;ll look at the <a href="https://www.reddit.com/r/news">/r/news</a> subreddit to see if there are any linguistic trends. In a subquery, the comment text (from non-bots!) is stripped of punctuation, set to lower case, and split into individual words; each word is counted and aggregated.</p>

<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">word</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">as</span> <span class="n">num_words</span><span class="p">,</span> <span class="k">AVG</span><span class="p">(</span><span class="n">score</span><span class="p">)</span> <span class="k">as</span> <span class="n">avg_score</span>
<span class="k">FROM</span><span class="p">(</span><span class="n">FLATTEN</span><span class="p">((</span>
  <span class="k">SELECT</span> <span class="n">SPLIT</span><span class="p">(</span><span class="k">LOWER</span><span class="p">(</span><span class="n">REGEXP_REPLACE</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">r</span><span class="s1">&#39;[\.\&quot;,*:()\[\]/|\n]&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)),</span> <span class="s1">&#39; &#39;</span><span class="p">)</span> <span class="n">word</span><span class="p">,</span> <span class="n">score</span>
  <span class="k">FROM</span> <span class="p">[</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_comments</span><span class="p">.</span><span class="mi">2015</span><span class="n">_08</span><span class="p">]</span>
  <span class="k">WHERE</span> <span class="n">author</span> <span class="k">NOT</span> <span class="k">IN</span> <span class="p">(</span><span class="k">SELECT</span> <span class="n">author</span> <span class="k">FROM</span> <span class="p">[</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_comments</span><span class="p">.</span><span class="n">bots_201505</span><span class="p">])</span>
  <span class="k">AND</span> <span class="n">subreddit</span><span class="o">=</span><span class="ss">&quot;news&quot;</span>
  <span class="p">),</span> <span class="n">word</span><span class="p">))</span>
<span class="k">GROUP</span> <span class="k">EACH</span> <span class="k">BY</span> <span class="n">word</span>
<span class="k">HAVING</span> <span class="n">num_words</span> <span class="o">&gt;=</span> <span class="mi">10000</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">num_words</span> <span class="k">DESC</span></code></pre></div>


<p>5.0 seconds and 11.3 GB processed, along with removing a few stopwords via R results in this.</p>

<p><img src="http://minimaxir.com/img/reddit-bigquery/reddit-bigquery-3.png" alt="" /></p>

<p>News is about <strong>people</strong>. And <strong>more</strong>. Makes sense. (An <code>avg_score</code> column for each word is included to allow for emulation of quantifiable impact of a given word, as used in my <a href="http://minimaxir.com/2015/01/linkbait/">BuzzFeed analysis</a>, although that is less useful for comments than submissions.)</p>

<h2>Monthly Active Users</h2>

<p>Although Reddit does provide a count of Subscribers for a given subreddit, most of those users are passive. One of the most important metrics of any startup is Monthly Active Users (MAUs), which we can get a reasonable approximation using the comment data. And not only that, we can aggregate the unique number of commenters by a given subreddit and by a given month, to see how subreddit activity changes over time relative to other subreddits.</p>

<p>How this works in BigQuery is a little more complicated, and requires the use of window functions:</p>

<ul>
<li>Aggregate by subreddit, month, and count of unique comment authors on <em>all</em> comments (this will result in a query with a lot of data processed!)</li>
<li>For each month, rank the subreddits by number of unique authors.</li>
<li>Take the top 20 subreddits by rank for each given month.</li>
</ul>


<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">subreddit</span><span class="p">,</span> <span class="nb">date</span><span class="p">,</span> <span class="n">unique_authors</span> <span class="k">FROM</span>
<span class="p">(</span><span class="k">SELECT</span> <span class="n">subreddit</span><span class="p">,</span> <span class="nb">date</span><span class="p">,</span> <span class="n">unique_authors</span><span class="p">,</span> <span class="n">ROW_NUMBER</span><span class="p">()</span> <span class="n">OVER</span> <span class="p">(</span><span class="n">PARTITION</span> <span class="k">BY</span> <span class="nb">date</span> <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">unique_authors</span> <span class="k">DESC</span><span class="p">)</span> <span class="n">rank</span> <span class="k">FROM</span>
<span class="p">(</span><span class="k">SELECT</span> <span class="n">subreddit</span><span class="p">,</span> <span class="k">LEFT</span><span class="p">(</span><span class="nb">DATE</span><span class="p">(</span><span class="n">SEC_TO_TIMESTAMP</span><span class="p">(</span><span class="n">created_utc</span><span class="p">)),</span> <span class="mi">7</span><span class="p">)</span> <span class="k">as</span> <span class="nb">date</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="k">UNIQUE</span><span class="p">(</span><span class="n">author</span><span class="p">))</span> <span class="k">as</span> <span class="n">unique_authors</span> <span class="k">FROM</span> <span class="n">TABLE_QUERY</span><span class="p">([</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_comments</span><span class="p">],</span> <span class="ss">&quot;table_id CONTAINS &#39;20&#39; AND LENGTH(table_id)&lt;8&quot;</span><span class="p">)</span>
<span class="k">GROUP</span> <span class="k">EACH</span> <span class="k">BY</span> <span class="n">subreddit</span><span class="p">,</span> <span class="nb">date</span>
<span class="p">))</span>
<span class="k">WHERE</span> <span class="n">rank</span> <span class="o">&lt;=</span> <span class="mi">20</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="nb">date</span> <span class="k">ASC</span><span class="p">,</span> <span class="n">unique_authors</span> <span class="k">DESC</span></code></pre></div>


<p>11.9 seconds and 53.3 GB (!) later, we can create a Top 20 Subreddits visualization for each month, and combine each image into a GIF with my trusty <a href="https://github.com/minimaxir/frames-to-gif-osx">Convert Frames to GIF</a> tool.</p>

<p><img src="http://minimaxir.com/img/reddit-bigquery/subreddit-ranks.gif" alt="" /></p>

<p>You can view the individual frames <a href="https://github.com/minimaxir/reddit-bigquery/tree/master/subreddit-ranks">in the project GitHub repository</a>. There are many trends revealed, such how some subreddits die over time (<a href="https://www.reddit.com/r/fffffffuuuuuuuuuuuu">r/fffffffuuuuuuuuuuuu</a>, <a href="https://www.reddit.com/r/technology">/r/technology</a>), how some rise over time (<a href="https://www.reddit.com/r/pcmasterrace">/r/pcmasterrace</a>), and how some subreddits have relative monthly spikes (<a href="https://www.reddit.com/r/thebutton">/r/thebutton</a>). Note that the colors have no visual meaning but are used to help easily differentiate between subreddits.</p>

<p>These sample queries are only a small sample of what can be done with the Reddit data and BigQuery. Although some data scientists may argue that using is BigQuery is pointless since ~200GB of data <a href="http://yourdatafitsinram.com">can fit in RAM</a>, the quick, dirty, and <em>cheap</em> option is much more pragmatic for the majority of potential data analysis on this Reddit dataset.</p>

<p><em>Again, you can view the R code needed to reproduce the visualizations in <a href="https://github.com/minimaxir/reddit-bigquery/blob/master/reddit_bigquery.ipynb">this Jupyter notebook</a>.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coding, Visualizing, and Animating Bootstrap Resampling]]></title>
    <link href="http://minimaxir.com/2015/09/bootstrap-resample/"/>
    <updated>2015-09-22T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2015/09/bootstrap-resample</id>
    <content type="html"><![CDATA[<p>A few days ago on Hacker News, I saw a <a href="https://news.ycombinator.com/item?id=10244950">nice submission</a> titled &ldquo;<a href="https://speakerdeck.com/jakevdp/statistics-for-hackers">Statistics for Hackers</a>,&rdquo; which was a slide deck written and presented by <a href="https://twitter.com/jakevdp">Jake Vanderplas</a>. The talk is very explicitly a tutorial for a few good statistical methods for those without a statistical background, using simple tools &ldquo;hackers&rdquo; are familiar with and are occasionally very necessary (it&rsquo;s not hacking in the sense of &ldquo;growth hacking,&rdquo; thankfully).</p>

<p>One important statistical tool discussed in the presentation is <a href="https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29">bootstrap resampling</a>, a good example for a programming-oriented presentation as it can be easily done with code. Bootstrapping is when a data set is randomized many, many times (usually thousands of times) to simulate similar data sets, and use those simulated datasets to obtain a more accurate range for a particular statistic about the data set, such as the statistical average.</p>

<p><img src="http://minimaxir.com/img/bootstrap-resample/talk0.png" alt="" /></p>

<p>I have reverse-engineered the data and code from the talk with R and ggplot2 in order to create more detailed implementations of bootstrapping, and also to add a few visual improvements.</p>

<h1>Preparing the Data</h1>

<p><em>If you want to view the code with more detailed output and extensive code for the ggplot2 charts, read this <a href="https://github.com/minimaxir/bootstrap-resample-notebook/blob/master/bootstrap_resample_notebook.ipynb">Jupyter notebook</a>, hosted and open-sourced on GitHub.</em></p>

<p>The first thing to do is recreate the test data.</p>

<p><img src="http://minimaxir.com/img/bootstrap-resample/talk1.png" alt="" /></p>

<p>Data is manually transcribed from the linked talk (to the best of my ability), and converted to a <code>tbl_df</code> for ease of use.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">x <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="m">8.1</span><span class="p">,</span> <span class="m">8.4</span><span class="p">,</span> <span class="m">8.8</span><span class="p">,</span> <span class="m">8.7</span><span class="p">,</span> <span class="m">9</span><span class="p">,</span> <span class="m">9.1</span><span class="p">,</span> <span class="m">9.2</span><span class="p">,</span> <span class="m">9.3</span><span class="p">,</span> <span class="m">9.4</span><span class="p">,</span> <span class="m">9.6</span><span class="p">,</span> <span class="m">9.9</span><span class="p">,</span> <span class="m">10</span><span class="p">,</span> <span class="m">10</span><span class="p">,</span> <span class="m">10.5</span><span class="p">,</span> <span class="m">10.6</span><span class="p">,</span> <span class="m">10.6</span><span class="p">,</span> <span class="m">11.2</span><span class="p">,</span> <span class="m">11.8</span><span class="p">,</span> <span class="m">12.6</span><span class="p">)</span>
y <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="m">21</span><span class="p">,</span> <span class="m">19</span><span class="p">,</span> <span class="m">18</span><span class="p">,</span> <span class="m">16</span><span class="p">,</span> <span class="m">15</span><span class="p">,</span> <span class="m">17</span><span class="p">,</span> <span class="m">17</span><span class="p">,</span> <span class="m">17</span><span class="p">,</span> <span class="m">19</span><span class="p">,</span> <span class="m">14</span><span class="p">,</span> <span class="m">14</span><span class="p">,</span> <span class="m">15</span><span class="p">,</span> <span class="m">11</span><span class="p">,</span> <span class="m">12</span><span class="p">,</span> <span class="m">12</span><span class="p">,</span> <span class="m">13</span><span class="p">,</span> <span class="m">10</span><span class="p">,</span> <span class="m">8</span><span class="p">,</span> <span class="m">9</span><span class="p">)</span>

df <span class="o">&lt;-</span> tbl_df<span class="p">(</span><span class="kt">data.frame</span><span class="p">(</span>x<span class="p">,</span>y<span class="p">))</span></code></pre></div>


<p>Let&rsquo;s create a quick plot in ggplot2 to compare it to the source image:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">qplot<span class="p">(</span>x<span class="p">,</span> y<span class="p">,</span> df<span class="p">)</span> <span class="o">+</span> geom_smooth<span class="p">(</span>method<span class="o">=</span><span class="s">&quot;lm&quot;</span><span class="p">)</span></code></pre></div>


<p><img src="http://minimaxir.com/img/bootstrap-resample/qplot.png" alt="" /></p>

<p>Pretty good copy, and it comes with a linear regression line for one line of code!</p>

<p>Performing the linear regression within R itself (via <code>lm()</code>) results in a model containing an X-intercept of 42.3 and a slope of -2.81. (<em>y = 42.3 - 2.81x</em>)</p>

<ul>
<li><strong>42.3</strong>, the coefficient for the (Intercept), represents the value of the stack if Wind Speed is 0.</li>
<li>For every 1 unit increase in Wind Speed, the estimated Height <strong>decreases by 2.81</strong>, the coefficient for the <code>x</code> variable.</li>
</ul>


<p>Both coefficients are extremely statistically significant. Regression as a whole has a R<sup>2</sup> of 82.71%; indicating that the change in Wind Speed explains 82.71% of the variation in Height. That&rsquo;s enough to assert causality in this mock data.</p>

<p>Now we can know the true value of the coefficients for the data, we can perform bootstrapping. We can create a function to perform a linear regression on the data frame, having been resampled <em>with replacement</em> (important!), and return the model coefficients:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">lm_res <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>df<span class="p">)</span> <span class="p">{</span>
    model <span class="o">&lt;-</span> lm<span class="p">(</span>y <span class="o">~</span> x<span class="p">,</span> df <span class="o">%&gt;%</span> sample_frac<span class="p">(</span>replace<span class="o">=</span><span class="bp">T</span><span class="p">))</span>
    <span class="kr">return</span> <span class="p">(</span>model<span class="o">$</span>coefficients<span class="p">)</span>
<span class="p">}</span></code></pre></div>


<p>R&rsquo;s <code>replicate</code> function runs a function a specified amount of times and stores the results in the same data frame; perfect for bootstrapping.</p>

<p>There&rsquo;s no limit to the number of repetitions you can perform using this method; however, you can hit diminishing returns as the distribution converges. Generally, 2,000 trials minimum are recommended, but let&rsquo;s do 10,000 trials just to be sure.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">df_boot <span class="o">&lt;-</span> <span class="kp">replicate</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span> lm_res<span class="p">(</span>df<span class="p">))</span>
df_boot <span class="o">&lt;-</span> tbl_df<span class="p">(</span><span class="kt">data.frame</span><span class="p">(</span><span class="kp">t</span><span class="p">(</span>df_boot<span class="p">)))</span></code></pre></div>


<p>This completed in about 13 seconds on my computer, and results in a dataset of 10,000 pairs of X-intercept and wind speed coefficients!</p>

<p>Now we can build 95% confidence intervals for each coefficient. This is done by finding the 2.5% quartile, and the 97.5% quartile; since there are a very large amount of bootstrapped samples, the confidence interval will be very accurate.</p>

<p>This can be done easily using the <code>gather()</code> function from <code>tidyr</code> to convert the data from wide to long, and <code>dplyr</code>&rsquo;s aggregation and summarization functions.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">df_boot_agg <span class="o">&lt;-</span> df_boot <span class="o">%&gt;%</span>
                gather<span class="p">(</span>var<span class="p">,</span> value<span class="p">)</span> <span class="o">%&gt;%</span>
                group_by<span class="p">(</span>var<span class="p">)</span> <span class="o">%&gt;%</span>
                summarize<span class="p">(</span>
                    average <span class="o">=</span> <span class="kp">mean</span><span class="p">(</span>value<span class="p">),</span>
                    low_ci <span class="o">=</span> quantile<span class="p">(</span>value<span class="p">,</span> <span class="m">0.025</span><span class="p">),</span>
                    high_ci <span class="o">=</span> quantile<span class="p">(</span>value<span class="p">,</span> <span class="m">0.975</span><span class="p">)</span>
                        <span class="p">)</span></code></pre></div>


<p>From the output, the true value of the X intercept has a 95% chance of being between 36.9 and 49.3, and the true value of the Wind Speed coefficient has a 95% chance of being between -3.53 and -2.28.</p>

<h1>Plotting the Bootstrap of One Variable</h1>

<p>The Wind Speed coefficient is much more interesting, statistically. We have enough information to derive a ggplot2 histogram of the resampled data, and a graphical representation of the confidence interval. We can also include the true value of the coefficient in the visualization (as the dotted vertical line) to see if it make sense.</p>

<p><img src="http://minimaxir.com/img/bootstrap-resample/uni_one.png" alt="" /></p>

<p>Plot is close to a <a href="https://en.wikipedia.org/wiki/Normal_distribution">Normal distribution</a> / bell curve, which is a nice property of bootstrapping and makes the results easier to analyze. The true coefficient value is in the interval, but not quite at the center.</p>

<p>We can write a function to generate a plot for different data size subsets (e.g. 100 samples, 200 samples, etc. up to 10,000 samples). This shows the impact of bootstrapping and how the confidence estimate intervals converge as the number of trials increases. Each of these plots serve as a frame in a GIF.</p>

<p>Each of these frames are stitched together into a high- quality, low file size by my new <a href="https://github.com/minimaxir/frames-to-gif-osx">Convert Frames to GIF OS X tool</a>!</p>

<p><img src="http://minimaxir.com/img/bootstrap-resample/uni_frames.gif" alt="" /></p>

<p>A 99 frame, 640px width GIF, at only half a megabyte! (R has an <a href="https://cran.r-project.org/web/packages/animation/index.html">animation package</a>, but with much worse results, which was the motivation for writing the OS X tool)</p>

<p>From the GIF, it&rsquo;s clear to see that the confidence intervals converge to the final values at about 2000 trials, with the shape of the distribution stabilizing at about 7000 trials.</p>

<h1>Plotting the Bootstrap of Two Variables</h1>

<p>The X-intercept and the Wind Speed coefficient have a linked, negatively correlated relationship, so it makes sense to plot them simultaneously, as in the original talk.</p>

<p><img src="http://minimaxir.com/img/bootstrap-resample/talk2.png" alt="" /></p>

<p>ggplot2&rsquo;s <code>density2d</code> plots a <a href="https://en.wikipedia.org/wiki/Contour_line">contour map</a>, which will help illustrate the area and magnitude where the data converges on a 2D plane. We can also include an <strong>orange</strong> bounding box using the specified values for the 2D confidence intervals for each variable, and a <strong>red</strong> dot representing the true value of the coefficients.</p>

<p><img src="http://minimaxir.com/img/bootstrap-resample/bi_one.png" alt="" /></p>

<p>The pair of coefficients can only occur in a certain range of values, much smaller than the bounding box. The true value is close to the most frequent occurrence, too.</p>

<p>Then we can make another GIF using the same method:</p>

<p><img src="http://minimaxir.com/img/bootstrap-resample/bi_frames.gif" alt="" /></p>

<p>Again, the visualization converges relatively quickly at about 2,000 trials. (although, in retrospect, the orange bounding box may not have been necessary.)</p>

<p>While the sample dataset isn&rsquo;t the most ideal for bootstrapping due to its extremely small size, bootstrapping at the least adds some clarity to the issues that results when simply performing statistics on a given dataset.</p>

<p><em>Again, if you want to view the code with more detailed output and extensive code for the ggplot2 charts, read this <a href="https://github.com/minimaxir/bootstrap-resample-notebook/blob/master/bootstrap_resample_notebook.ipynb">Jupyter notebook</a>.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Plagiarizing and Paraphrasing Code from an Online Class for Content Marketing]]></title>
    <link href="http://minimaxir.com/2015/09/code-of-plagiarism/"/>
    <updated>2015-09-03T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2015/09/code-of-plagiarism</id>
    <content type="html"><![CDATA[<p><span><style>
.post-content img {
  border: 2px solid #ccc;
}
</style></span></p>

<p><em>UPDATE 9/13/15: The author replied to this article in the comments and clarified in the source article the nature of the changes, and also gave more explicit credit to the original code authors.</em></p>

<hr />

<p>Over the summer, I took a couple of online classes on edX:
<a href="https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/info">CS100.1x Introduction to Big Data with Apache Spark</a> by Anthony D. Joseph, and the followup course <a href="https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/info">CS190.1x Scalable Machine Learning</a> by Ameet Talwalkar. Both courses focused on <a href="http://spark.apache.org">Apache Spark</a>, an up-and-coming technology in the data science world which promises incredible analytical performance. I strongly recommend taking both courses if you have an interest in data science. (however, both courses require strong knowledge of Python)</p>

<p>A <a href="https://news.ycombinator.com/item?id=10154491">Hacker News submission</a> linked to an article titled &ldquo;<a href="https://www.codementor.io/spark/tutorial/building-a-recommender-with-apache-spark-python-example-app-part1">Building a Movie Recommendation Service with Apache Spark and Flask - Part 1</a>&rdquo; on Codementor, which was relevant to my interests. However, while reading the article, I felt a sense of deja vu.</p>

<p>As it turns out, the concept, and the majority of the code for the recommendation service is taken directly from <a href="http://nbviewer.ipython.org/github/spark-mooc/mooc-setup/blob/master/lab4_machine_learning_student.ipynb">CS100.1x Assignment #4</a>, without giving any explicit attribution to any of the professors.</p>

<p>It is unusual to see someone not only plagiarize code, but to do it very blatantly and <em>very poorly</em>.</p>

<h1>Manipulating Movies</h1>

<p>The June 27th 2015 edX assignment uses Spark&rsquo;s MLib for parallelized machine learning on the <a href="http://grouplens.org/datasets/movielens/">MovieLens dataset</a> of millions of ratings of movies by users.</p>

<p>The July 7th 2015 Codementor article, in fairness, has nonplagiarized code to parse this data into RDDs (resilient distributed datasets), which is Spark&rsquo;s data structure that enables high scalability and parallelization. That&rsquo;s because the edX assignment has the data with a slightly different schema (comma-delimited vs. colon-delimited data).</p>

<p>The collaborative filtering algorithm using <a href="http://bugra.github.io/work/notes/2014-04-19/alternating-least-squares-method-for-collaborative-filtering/">Alternating Least Squares</a> (ALS) is what caused me to raise an eyebrow. As with any good statistical methodology, the data should be split into independent training, test, and validation datasets, where the training set is used to construct the model, the validation set is used to select the best model when model parameters vary, and the test dataset is used for the actual statistical analysis. The edX assignment explains this well:</p>

<p><img src="http://minimaxir.com/img/code-of-plagiarism/edx_1.png" alt="" /></p>

<p>The <code>[6, 2, 2]</code> parameter says that 60% of the data is split for training, 20% of validation, 20% for testing. The <code>seed=0L</code> ensures that the results are the same each time; important for reproducible results in the case for the assignment, but not good for general statistical analysis.</p>

<p>So what does the Codementor article say?</p>

<p><img src="http://minimaxir.com/img/code-of-plagiarism/code_1.png" alt="" /></p>

<p>Not much reasoning behind the split, but an <em>interesting</em> choice of split percentages and seed there. But what&rsquo;s curious is the variable names; the edX assignment has variables names as <a href="https://en.wikipedia.org/wiki/CamelCase">CamelCase</a>, while the Codementor article has variable names with underscore separators.</p>

<p>One of the recommended ways to select an predictive model is to optimize parameters of the predictive function. The edX assignment changes the <code>rank</code> (number of latent factors) of the <a href="http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html">Alternating Least Squares</a> model while keeping the other parameters constant:</p>

<p><img src="http://minimaxir.com/img/code-of-plagiarism/edx_2.png" alt="" /></p>

<p>The Codementor article has a similar approach.</p>

<p><img src="http://minimaxir.com/img/code-of-plagiarism/code_2.png" alt="" /></p>

<p>&hellip;<em>too</em> similar.</p>

<p>To find the &ldquo;best&rdquo; model, the edX assignment runs the model and computes the <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">Root-Mean-Square Error</a>, a popular metric for assessing model quality, between the predictive results of the model and the actual values for each <code>rank</code>, and selects the <code>rank</code> which results in the model with the lowest RMSE.</p>

<p><img src="http://minimaxir.com/img/code-of-plagiarism/edx_3.png" alt="" /></p>

<p>The Codementor approach is similar; except since the <code>computeError</code> function was defined earlier in the edX assignment, the Codementor article has to reimplement it inline.</p>

<p><img src="http://minimaxir.com/img/code-of-plagiarism/code_3.png" alt="" /></p>

<p>We can rule out &ldquo;coincidence&rdquo; at this point. What&rsquo;s concerning is that these workarounds make the code far less legible and more difficult to parse. (the results in both are the same of course: Rank 4 model had an RMSE of 0.8927, Rank 8 model had an RMSE of 0.8901, and Rank 12 model had an RMSE of 0.8902, making the Rank 8 model the winner)</p>

<p>The Codementor article repeats the predictive assessment with the full, 21x larger dataset, and getting a RMSE of 0.8218 with the Rank 8 model, which in fairness is new material. (although the conclusion, &ldquo;we can see how we got a more accurate recommender when using a much larger dataset,&rdquo; is not necessarily <em>caused</em> by using the larger dataset due to the nature of <em>random</em> splits; that&rsquo;s what <a href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29">cross-validation</a> is for).</p>

<h2>Code Paraphrasing</h2>

<p>The edX assignment has a fun section where the user can test the predictive model they created by giving their own ratings for movies, in the format of <code>(myUserID, movie ID, your rating)</code>. For example, here&rsquo;s what I put:</p>

<p><img src="http://minimaxir.com/img/code-of-plagiarism/edx_4.png" alt="" /></p>

<p>I may have felt like being lazy that day.</p>

<p>The Codementor article has an interesting approach.</p>

<p><img src="http://minimaxir.com/img/code-of-plagiarism/code_4.png" alt="" /></p>

<p>i.e. the same approach. But instead of &ldquo;myUser&rdquo;, it&rsquo;s &ldquo;new_user&rdquo;, and the &ldquo;0&rdquo; id is hard-coded in each rating. And more underscore variables? This use of &ldquo;code paraphrasing&rdquo; is a new concept to me.</p>

<p>The edX assignment ended by showing your top movies based on your recommendations and your model! (the code for the <code>ratingsWithNamesRDD</code> object is mine)</p>

<p><img src="http://minimaxir.com/img/code-of-plagiarism/edx_5.png" alt="" /></p>

<p>Of course, the recommended movies are limited to those which only have more than 75 reviews, in order to prevent movies which have only a few ratings from skewing the results.</p>

<p>Codementor&rsquo;s approach?</p>

<p><img src="http://minimaxir.com/img/code-of-plagiarism/code_5.png" alt="" /></p>

<p>The same thing&hellip;except, for some reason, the <code>filter</code> is limited to movies with <em>at least 25</em> reviews instead of more than 75, the top 25 movies are output instead of the top 20 (the first parameter in <code>takeOrdered</code>), and the initial title string was changed to the third-person voice? (notably, the &ldquo;more than 25&rdquo; text is incorrect since it&rsquo;s an &ldquo;at least&rdquo; statement)</p>

<h1>The Point of Plagiarizing Code</h1>

<p>The <a href="https://www.codementor.io/spark/tutorial/building-a-web-service-with-apache-spark-flask-example-app-part2">follow-up article</a> to the Codementor article incorporates the ALS recommender algorithm with <a href="http://flask.pocoo.org">Flask</a>, a popular Python framework for creating microservices. This is genuinely interesting new code not covered by the original edX assignment. So what was the point of the weak plagiarism in Part 1? A piece of content marketing for a personal portfolio?</p>

<p>If Part 1 was entirely &ldquo;<em>Hey, I took the CS100.1x Introduction to Big Data with Apache Spark by Anthony D. Joseph on edX which included a good Collaborative Filtering algorithm, here&rsquo;s a link to the assignment if you want to know more, and now I will use the code I wrote for that assignment to create a Flask app</em>,&rdquo; I would have been perfectly OK with that. (after the course was completed to avoid cheating, of course)</p>

<p>Coding is an industry that&rsquo;s generally more lax about reuse. I would not expect someone to cite every instance they used code from a <a href="http://stackoverflow.com">Stack Overflow</a> question. Forking code repositories on <a href="http://github.com">GitHub</a> is actively encouraged, as it often leads to new insights.</p>

<p>But the <em>willful</em> and unnecessary paraphrasing of code, replacing all the CamelCase variables with underscored variables, and code which is <em>worse</em> than the original, is hard to overlook. &ldquo;I recommend you for example this edX course&rdquo; does not give sufficient credit to the teachers and staff of CS100.1x.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Making a Video-to-GIF Right-Click Menu Item in OS X]]></title>
    <link href="http://minimaxir.com/2015/08/gif-to-video-osx/"/>
    <updated>2015-08-13T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2015/08/gif-to-video-osx</id>
    <content type="html"><![CDATA[<p><em>TL;DR: To set up the Convert Video to GIF tool, download the &ldquo;Convert Video to GIF&rdquo; Service <a href="https://github.com/minimaxir/video-to-gif-osx">from this GitHub Repository</a> and follow the install instructions in the README.</em></p>

<p>Last weekend, I was working on making a <a href="https://github.com/minimaxir/big-list-of-naughty-strings">Big List of Naughty Strings</a> for user-data testing. (now up to 4,000+ GitHub Stars!) For the README, I needed a visual example to demonstrate why testing bad strings is important. I easily made a video of an internal server error on Twitter using <a href="http://www.apple.com/quicktime/">Quicktime</a> and its <a href="https://support.apple.com/kb/PH5882?locale=en_US">Screen Recording feature</a> to record, crop, and trim the event. But videos cannot render on GitHub; only GIFs can. And ideally I would not include a 807KB file everytime a user loads the GitHub page.</p>

<p>Websites that offer online Video-to-GIF converters are often seedy and create low quality, low frame-rate GIFs. I did some research and eventually I tried <a href="http://www.gifrocket.com">Gifrocket</a>, a recently-released tool that promises effortless drag-and-drop video-to-GIF-conversion.</p>

<p><img src="http://minimaxir.com/img/video-to-gif-osx/zerowidthedit_gifrocket.gif"  ></p>

<p>The quality is not terrible, although the ghosting is <em>weird</em>, and the framerate is choppy when the cursor moves. Unfortunately, I can&rsquo;t optimize any settings. File Size is 534KB, which is a slight compression but not great.</p>

<p>As a result, I gave <a href="http://zulko.github.io/moviepy/">MoviePy</a>, a Python API for manipulating videos another try, as I had used it for other posts on this blog to <a href="http://minimaxir.com/2014/02/moved-temporarily/">good</a> <a href="http://minimaxir.com/2014/03/hashtag-tag/">success</a>. However, I had to shrink them significantly to keep load times down; something I would prefer not to do with my internal server error GIF in order to maximize readability. On the <a href="http://zulko.github.io/blog/2014/01/23/making-animated-gifs-from-video-files-with-python/">blog post announcing GIF support</a>, the core developer <a href="http://zulko.github.io/blog/2014/01/23/making-animated-gifs-from-video-files-with-python/#comment-1216274781">relayed an interesting comment</a> from another reader that &ldquo;mencoder/ImageMagick/gifsickle is a winning trio for Gif making.&rdquo;</p>

<p>I&rsquo;ve heard of ImageMagick before: it&rsquo;s the tool that drives nearly all image manipulation on the internet (it&rsquo;s also what Gifrocket uses). But what was mencoder and gifsickle?</p>

<p>Some searching led to this <a href="https://www.reddit.com/r/reactiongifs/comments/x55z9/after_i_learned_how_to_make_large_well_compressed/c5jbq7c">2012 Reddit comment</a> in the /r/reactiongifs subreddit. That&rsquo;s where everything clicked.</p>

<h2>Shell Games</h2>

<p>The linked Reddit post conveiently uses the same three tools by the commenter on MoviePy. I ran the terminal commands on my video, with a little tweaking.</p>

<p>The first command uses the <code>mencoder</code> functionality of <a href="http://www.mplayerhq.hu/design7/news.html">mplayer</a> to render each frame of the video to PNGs:</p>

<div class="highlight"><pre><code class="language-bash" data-lang="bash">mplayer -ao null -vo png:z<span class="o">=</span>1:outdir<span class="o">=</span>gif -vf <span class="nv">scale</span><span class="o">=</span>608:454 zerowidthedit.mov</code></pre></div>


<p>I set the size to a width of 608px since it was exactly 50% of the width of the raw video, but I could set it to whatever I want, although it would be ideal to keep the aspect ratio the same.</p>

<p>The second command uses <a href="http://www.imagemagick.org/script/index.php">ImageMagick</a>&rsquo;s famously robust <code>convert</code> comand to convert the rendered frames in to a GIF, with two optimization passes for lower file size and ~16.6ms delay, which is equivalent to 60 frames per second.</p>

<pre><code>convert +repage -fuzz 1.6% -delay 1.7 -loop 0 gif/*.png -layers OptimizePlus -layers OptimizeTransparency Almost.gif
</code></pre>

<p>This results in a 156KB image; much, much better than the 534KB from Gifrocket, and at a much higher framerate too.</p>

<p>Lastly, the GIF is further optimized with <a href="http://www.lcdf.org/gifsicle/">gifsicle</a>, which also limits the color pallete to 256 colors for even lower file size.</p>

<pre><code>gifsicle -O3 --colors 256 Almost.gif &gt; Done.gif
</code></pre>

<p>This results in a 107KB KB, a 31% savings over the already-optimized GIF, without any discernable loss! Here is the final result:</p>

<p><img src="http://minimaxir.com/img/video-to-gif-osx/zerowidthedit_final.gif"  ></p>

<p>Much, <em>much</em> better. No ghosting, 60 FPS, 1/8th of the original video file size, and 1/5th of the file size of the Gifrocket GIF!</p>

<p>Then I realized; aside from the output GIF resolution in from <code>mencoder</code>, the terminal commands for making GIFs are very generic. What if I could easily automate these steps for any video on my Mac?</p>

<h2>Full Automatic</h2>

<p>I decided to make a OS X <a href="http://www.computerworld.com/article/2476298/mac-os-x/os-x-a-quick-guide-to-services-on-your-mac.html">Service</a>, which can be used to automate actions for a specified file type in the form of a right-click menu option. Services can be created using <a href="https://en.wikipedia.org/wiki/Automator_%28software%29">Automator</a>, which is included in all OS X installations. In my case, I want to create a Service for movie files; and when the Service runs, it should Run a Shell Script with the above three commands.</p>

<p><img src="http://minimaxir.com/img/video-to-gif-osx/service.png"  ></p>

<p>After writing the script, which incorporates a maximium GIF width of 480px and will resize larger GIFs to that size while maintaining the aspect ratio, the Service is complete and I can right-click any Movie file and get an optimized GIF!</p>

<p><img src="http://minimaxir.com/img/video-to-gif-osx/convert_to_gif.gif"  ></p>

<p>Yes, even a 41.6MB 720p video file, which most GIF converter sites would never let you upload, becomes a <a href="http://i.imgur.com/0dU3A6o.gif">relatively reasonable 1.3MB GIF</a>!</p>

<p>This approach also has an unexpected benefit; the script support batch conversion, meaning you can convert as many videos as you want at the same time!</p>

<p>As usual, my code is <a href="https://github.com/minimaxir/video-to-gif-osx">open sourced on GitHub</a> with a MIT license, with instructions on how to set up the tool. The repository also includes two bonus utilities; a Shell Script to run the conversion from the command line and an Application which prompts you for movie files instead of needing to right-click. Install the three command-line applications and convert away! Or fork the repository and make an even <em>better</em> Video-to-GIF conversion tool!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Plotting a Map of New York City Using Only Taxi Location Data]]></title>
    <link href="http://minimaxir.com/2015/08/nyc-map/"/>
    <updated>2015-08-07T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2015/08/nyc-map</id>
    <content type="html"><![CDATA[<p>Recently, the New York City Taxi and Limousine Commission <a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml">released a dataset</a> of all Yellow Taxi and Green Taxi trips in 2014, and year-to-date in 2015, which follows the <a href="http://chriswhong.com/open-data/foil_nyc_taxi/">2013 data set</a> which was obtained to a FOIL request for the data last year. The dataset contains fun statistics, such as the location where the taxi picked up and dropped off its fare, the speed the taxi is moving, and the total fare at the end of the ride.</p>

<p>In the <a href="https://news.ycombinator.com/item?id=10003118">Hacker News thread</a> announcing the data set release, user eck posted an interesting, minimalistic visualization of the taxi location data:</p>

<p><img src="http://minimaxir.com/img/nyc-map/ov6K6mt.jpg"  ></p>

<p>eck made the visualization using a &ldquo;few hundred lines of C++&rdquo;. That seemed overkill to me. So I tried to reverse-engineer his visualization using my favorite plotting tool, <a href="http://ggplot2.org/">ggplot2</a>. In theory, plotting a million little points in close proximity should simulate the lines of the streets of New York City.</p>

<p>The dataset is large (2 GB per month of data), although not &ldquo;big data&rdquo; large. It would take an afternoon to set up a local database, and I wanted to make pretty visualizations <em>immediately</em>.</p>

<p>Google BigQuery Developer Advocate Felipe Hoffa <a href="https://www.reddit.com/r/bigquery/comments/3fo9ao/nyc_taxi_trips_now_officially_shared_by_the_nyc/">created a BigQuery interface</a> for the data. BigQuery allows easy and fast access to the entire dataset for rapid processing. In my case, I need to compress the data set by truncating the latitude and longitude of the GPS coordinates to 4 digits; this allows <a href="http://gis.stackexchange.com/a/8674">precision to 11 meters</a> on the coordinates, which is sufficient for estimating. Running this BigQuery query:</p>

<pre><code>SELECT ROUND(pickup_latitude, 4) as lat,
ROUND(pickup_longitude, 4) as long,
COUNT(*) as num_pickups
FROM [nyc-tlc:yellow.trips_2014]
GROUP BY lat, long
</code></pre>

<p>Gives me data for about 1 Million GPS coordinates; more than enough to make a full map. This also has the benefit of fitting into memory, which is necessary for use with ggplot2.</p>

<p>My first attempt, where I plot 1 million very small white points on a black map bounded to the latitude/longitude coordinates of NYC, turned out pretty well, and with less than 10 lines of code.</p>

<p><img src="http://minimaxir.com/img/nyc-map/nyc_old.png"  ></p>

<p>On Reddit, my submission of the <a href="https://www.reddit.com/r/dataisbeautiful/comments/3fvg8i/map_of_new_york_city_plotted_using_locations_of/">data visualization</a> received about 3,500 points, and a large amount of social media buzz on Facebook and Twitter. There were a few comments however; why were there random points in the Hudson River? Why are highways indicated as taxi pickup spots? Why does the map say &ldquo;2015&rdquo; when your query says &ldquo;2014&rdquo;? (guilty on the last one; the map was made using the 2014 dataset by accident!)</p>

<p>At the least, the streets of <a href="https://en.wikipedia.org/wiki/Manhattan">Manhattan</a> were not discernable at all, unlike eck&rsquo;s diagram. As a result, I made a few refinements to remove some logical outliers with impossible vehicle speeds, removed noisy points which were completely isolated, <em>used the correct 2015 data set</em>, and also added a color weighting to the data, where the most-taxi-dense areas will appear colored (scaling logarithmically) to differentate those areas from less taxi-prone areas.</p>

<p><img src="http://minimaxir.com/img/nyc-map/nyc_yellow_pickup.png"  ></p>

<p>The map, while less bright, became more precise. The streets in Manhattan are now visible, and the purple color shows how <a href="https://en.wikipedia.org/wiki/Times_Square">Times Square</a> and the <a href="https://en.wikipedia.org/wiki/Financial_District,_Manhattan">Financial District</a> in particular are popular taxi pickup spots in Manhattan. <a href="https://en.wikipedia.org/wiki/LaGuardia_Airport">LaGuardia Airport</a> and <a href="https://en.wikipedia.org/wiki/John_F._Kennedy_International_Airport">John F. Kennedy International Airport</a> appear purple as well.</p>

<p>However, this map only looks at places where people picked up Taxis. Could there be a difference when plotting Taxi dropoffs instead?</p>

<p>I reran the query and visualization scripts on the dropoff location data instead, and as it turns out, there is a significant difference!</p>

<p><img src="http://minimaxir.com/img/nyc-map/nyc_yellow_dropoff.png"  ></p>

<p>While Taxi pickups were isolated more in residential areas, taxi dropoffs can happen anywhere in the Tristate area. Additionally, the map more closely matches eck&rsquo;s original visualization.</p>

<p>Setting up ggplot2 in this way will also allow me to perform other fun analyses in the future. For example, since we know where taxis drop off, we can determine the average speed for the trip for each significant location in NYC geography. Would the average trip speed be higher for trips that drop off at an airport due to the highway? Conversely, would the average speed be lower in Manhattan? How would fares be affected? Those are questions for another blog post. :)</p>

<p>Although, I still have no guesses why the highways are highlighted in <em>both</em> maps.</p>

<hr />

<p><em>You can download a <a href="https://dl.dropboxusercontent.com/u/2017402/nyc_yellow_pickup.pdf">PDF of my purple pickup map</a> (4.14 MB) and a <a href="https://dl.dropboxusercontent.com/u/2017402/nyc_yellow_dropoff.pdf">PDF of my blue dropoff map</a> (7.68 MB) sans text, both of which are resolution-independent and sutable for making physical prints.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Scrape Data From Facebook Page Posts for Statistical Analysis]]></title>
    <link href="http://minimaxir.com/2015/07/facebook-scraper/"/>
    <updated>2015-07-20T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2015/07/facebook-scraper</id>
    <content type="html"><![CDATA[<p>One of the first data scrapers I wrote for the purpose of statistical analysis was a Facebook Graph API scraper, in order to determine <a href="http://minimaxir.com/2013/06/big-social-data/">which words are the most important</a> in a Facebook Page status update. However, the v2.0 update to the Facebook API unsurprisingly broke the scraper.</p>

<p>Now that <a href="https://developers.facebook.com/blog/post/2015/07/08/graph-api-v2.4/">v2.4 of the Graph API is released</a>, I gave the Facebook Graph API another look. Turns out, it&rsquo;s pretty easy to scrape and make into a spreadsheet for easy analysis, although like with any other scrapers, there are a large number of gotchas.</p>

<h1>Feasibility</h1>

<p><img src="http://minimaxir.com/img/facebook-scraper/nyt_sample.png"  ></p>

<p>In order to determine if I can sanely scrape a website, I have to do a bit of research. How much data from a Facebook status update can we actually scrape?</p>

<p>Fortunately, Facebook&rsquo;s <a href="https://developers.facebook.com/docs/graph-api/reference">Graph API documentation</a> is pretty good. We need data from the <a href="https://developers.facebook.com/docs/graph-api/reference/page">/page</a> node, and from there, we can access data from the <a href="https://developers.facebook.com/docs/graph-api/reference/v2.4/page/feed">/feed</a> edge.</p>

<p>Between the two nodes, we have access to <code>id</code>, which is a unique identifer that can be used to create a link back to the update itself (e.g. <a href="https://www.facebook.com/5281959998_10150628170209999">https://www.facebook.com/5281959998_10150628170209999</a>) <code>message</code>, the text of the update; <code>link</code>, the URL which the update is linking; <code>name</code>, the title of the webpage of the link, <code>type</code>, an identifier if the update is text, a photo, or a video; and <code>created_time</code>, when the update is published.</p>

<p>Accessing the numerical counts of <code>likes</code>, <code>comments</code>, and <code>shares</code> is less explicit in the documentation. Fortunately, <a href="http://stackoverflow.com/questions/6984526/facebook-graph-api-get-like-count-on-page-group-photos">StackOverflow</a> has the answer: you need to request <code>likes.limit(1).summary(true)</code> instead of normal <code>likes</code>.</p>

<p>There&rsquo;s no indication that there&rsquo;s a Rate Limit, oddly. Since we can query 100 updates at a time, the scraper will be efficient enough that it&rsquo;s unlikely to hit any extreme API limits.</p>

<p>Now that we know we can get all the relevant data from the sample status update, we can build a Facebook post scraper.</p>

<h1>Data Scrappy</h1>

<p><img src="http://minimaxir.com/img/facebook-scraper/def_test.png"  ></p>

<p><em>I have created an <a href="https://github.com/minimaxir/facebook-page-post-scraper/blob/master/how_to_build_facebook_scraper.ipynb">IPython notebook hosted on GitHub</a> with detailed code, code comments, and sample output for each step of the scraper development. I strongly recommend giving it a look.</em></p>

<p>First, we need to see how to actually access the API. It&rsquo;s no longer a public API, and it requires user authentication via <a href="https://developers.facebook.com/docs/facebook-login/access-tokens">access tokens</a>. Users can get Short-Term tokens, but as their name suggests, they expire quickly, so they are not recommended. The Graph API allows a neat trick; by concatenating the App ID from a user-created App and the App Secret, you create an access token which never expires. Of course, this is a major security risk, so create a separate app for the sole purpose of scraping, and reset your API Secret if it becomes known.</p>

<p>Let&rsquo;s say we want to scrape the New York Times' Facebook page. We would send a request to <a href="https://graph.facebook.com/v2.4/nytimes?access_token=XXXXX">https://graph.facebook.com/v2.4/nytimes?access_token=XXXXX</a> and we would get:</p>

<div class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span>
        <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;5281959998&quot;</span><span class="p">,</span> 
        <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;The New York Times&quot;</span>
<span class="p">}</span></code></pre></div>


<p>i.e., the page metadata. Sending a request to /nytimes/feed results in what we want:</p>

<div class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span>
        <span class="nt">&quot;data&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="nt">&quot;created_time&quot;</span><span class="p">:</span> <span class="s2">&quot;2015-07-20T01:25:01+0000&quot;</span><span class="p">,</span> 
                <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;5281959998_10150628157724999&quot;</span><span class="p">,</span> 
                <span class="nt">&quot;message&quot;</span><span class="p">:</span> <span class="s2">&quot;The planned megalopolis, a metropolitan area that would be about 6 times the size of New York\u2019s, is meant to revamp northern China\u2019s economy and become a laboratory for modern urban growth.&quot;</span>
            <span class="p">},</span> 
            <span class="p">{</span>
                <span class="nt">&quot;created_time&quot;</span><span class="p">:</span> <span class="s2">&quot;2015-07-19T22:55:01+0000&quot;</span><span class="p">,</span> 
                <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;5281959998_10150628161129999&quot;</span><span class="p">,</span> 
                <span class="nt">&quot;message&quot;</span><span class="p">:</span> <span class="s2">&quot;\&quot;It\u2019s safe to say that federal agencies are not where we want them to be across the board,\&quot; said President Barack Obama&#39;s top cybersecurity adviser. \&quot;We clearly need to be moving faster.\&quot;&quot;</span>
            <span class="p">}</span>
         <span class="p">]</span>

            
<span class="p">}</span></code></pre></div>


<p>Now we get the post data. But not much of it. In Graph API v2.4, the default behavior is to return very, very little metadata for statuses in order to reduce bandwidth, with the expectation that the user will request the necessary fields.</p>

<p>So let&rsquo;s request <em>all</em> the fields we want. This results in a very long URL not shown here which causes the posts feed to have all the data we need:</p>

<div class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span>
    <span class="nt">&quot;comments&quot;</span><span class="p">:</span> <span class="p">{},</span>
    <span class="nt">&quot;summary&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;order&quot;</span><span class="p">:</span> <span class="s2">&quot;ranked&quot;</span><span class="p">,</span>
        <span class="nt">&quot;total_count&quot;</span><span class="p">:</span> <span class="mi">31</span>
    <span class="p">},</span>
    <span class="nt">&quot;created_time&quot;</span><span class="p">:</span> <span class="s2">&quot;2015-07-20T01:25:01+0000&quot;</span><span class="p">,</span>
    <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;5281959998_10150628157724999&quot;</span><span class="p">,</span>
    <span class="nt">&quot;likes&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;data&quot;</span><span class="p">:</span> <span class="p">{},</span>
        <span class="nt">&quot;summary&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="nt">&quot;total_count&quot;</span><span class="p">:</span> <span class="mi">278</span>
        <span class="p">}</span>
    <span class="p">},</span>
    <span class="nt">&quot;link&quot;</span><span class="p">:</span> <span class="s2">&quot;http://nyti.ms/1Jr6LhU&quot;</span><span class="p">,</span>
    <span class="nt">&quot;message&quot;</span><span class="p">:</span> <span class="s2">&quot;The planned megalopolis, a metropolitan area that would be about 6 times the size of New York’s, is meant to revamp northern China’s economy and become a laboratory for modern urban growth.&quot;</span><span class="p">,</span>
    <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;China Molds a Supercity Around Beijing, Promising to Change Lives&quot;</span><span class="p">,</span>
    <span class="nt">&quot;shares&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;count&quot;</span><span class="p">:</span> <span class="mi">50</span>
    <span class="p">},</span>
    <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;link&quot;</span>
<span class="p">}</span></code></pre></div>


<h1>Post Processing</h1>

<p>Great! Now we just have to process each post. Which is easier said than done.</p>

<p>If you&rsquo;re an avid Facebook user, you know that not all of these attributes are not guaranteed to exist. Status updates may not have text or links. Since we&rsquo;re making a spreadsheet with an enforced schema, we need to validate that a field exists before attempting to process it.</p>

<p>The &ldquo;\u2019"s in the message correspond to a <a href="http://smartquotesforsmartpeople.com/">smart quote</a> apostrophe. Since this a possibility, along with other unicode characters, the message and link names must be encoded <a href="https://en.wikipedia.org/wiki/UTF-8">in UTF-8</a> to prevent errors.</p>

<p>The time format is another issue. The date follows the <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO 8601</a> standard for UTC times. However, most spreadsheet programs will not able to parse it as a Date value. Also, since the NYT is based in the USA (specifically, New York), it may be helpful for time-based statistical analysis to convert the time to Eastern Standard Time while fixing the date format.</p>

<p>There&rsquo;s also an unexpected precaution that must be taken whenever scraping data sets. These APIs do not expect users to be accessing very, very old data. As a result, there&rsquo;s a high probability of the API server actually hitting an error sometime during the scrape, such as a <a href="http://www.checkupdown.com/status/E500.html">HTTP Status 500</a> or <a href="http://www.checkupdown.com/status/E502.html">HTTP Status 502</a>. These server errors are temporary, so a helper function must be used to attempt to retrieve data until it is actually successful.</p>

<h1>Putting it All Together</h1>

<p>Now we have a full plan for scraping, we query each page of Facebook Page Statuses (100 statuses maximum per page), process all statuses on that page and writing the output to a CSV file, and navigate to the next page, and repeat until no more statuses left.</p>

<p>This can be done with a for-loop within a while loop. In addition, I also recommend counting the number of posts processed and taking a timestamp every-so-often to ensure that the program has not stalled.</p>

<p><img src="http://minimaxir.com/img/facebook-scraper/cnnwoo.png"  ></p>

<p>And that&rsquo;s it! You can access the complete scraper in this GitHub repository, along with all other scripts mentioned in this article. Once you have the CSV file, you can import it into nearly every statistical program and have fun with it. <em>(You can download a .zip of the NYTimes data <a href="https://dl.dropboxusercontent.com/u/2017402/nytimes_facebook_statuses.zip">here</a> [4.6MB])</em></p>

<p>Say, for example, what would happen if we compared the Median Likes of the New York Times with a certain other  journalistic website that&rsquo;s the master of social media?</p>

<p><img src="http://minimaxir.com/img/facebook-scraper/nytimes_buzz_fb.png"  ></p>

<p>There may be more practical reasons for analyzing data on Facebook Posts, such as quantifying the growth and success of your own page, or that of your competitors. But the data is easy to get and is very useful.</p>

<p>Although, in fairness, the scraper is not perfect and still has room for improvement. With CNN&rsquo;s Facebook Page post data, for example, somehow the scraper skips all posts from 2013. Although in that case, I blame Facebook.</p>

<hr />

<p><em>You can access all resources used in this blog post at this <a href="https://github.com/minimaxir/facebook-page-post-scraper">GitHub repository</a></em>.</p>

<p><em>If you haven&rsquo;t, I strongly recommend looking at the <a href="https://github.com/minimaxir/facebook-page-post-scraper/blob/master/how_to_build_facebook_scraper.ipynb">IPython Notebook</a> for more detailed coding methodology.</em></p>

<p><em>And, as an experiment, I&rsquo;ve made an <a href="https://github.com/minimaxir/facebook-page-post-scraper/blob/master/fb_page_data_analysis.ipynb">IPython notebook with the R kernel</a> showing how I made the NYT-BuzzFeed chart!</em></p>
]]></content>
  </entry>
  
</feed>
