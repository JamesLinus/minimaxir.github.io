<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>minimaxir | Max Woolf&#39;s Blog</title>
    <description>A blog by Max Woolf about startups, technology, and blogging. It&#39;s so meta, even this acronym.</description>
    <link>http://minimaxir.com/</link>
    <atom:link href="http://minimaxir.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 26 Mar 2016 21:28:41 -0700</pubDate>
    <lastBuildDate>Sat, 26 Mar 2016 21:28:41 -0700</lastBuildDate>
    <generator>Jekyll v3.1.2</generator>
    
      <item>
        <title>Facebook Reactions and the Problems With Quantifying Likes Differently</title>
        <description>&lt;p&gt;Last week, Facebook added &lt;a href=&quot;http://newsroom.fb.com/news/2016/02/reactions-now-available-globally/&quot;&gt;Facebook Reactions&lt;/a&gt;, allowing users to do more than just &amp;ldquo;Like&amp;rdquo; posts and statuses as they have done for the past decade. Likes were the universal symbol of approval on social media. Now, Facebook users can apply more granular responses, from positive emotions like &lt;strong&gt;Love&lt;/strong&gt;, to negative emotions such as &lt;strong&gt;Angry&lt;/strong&gt;. This was widely believed to be Facebook&amp;rsquo;s compromise instead of adding a Dislike button.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/facebook_react.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, there&amp;rsquo;s an ulterior motive. The use of reactions provides organic data on the sentiment of a status, which is helpful for numerous marketing and statistical applications. As &lt;a href=&quot;http://www.buzzfeed.com/alexkantrowitz/facebook-reactions-launch-today&quot;&gt;BuzzFeed notes&lt;/a&gt;, Facebook ads may be able &amp;ldquo;to write one product message for someone who mostly uses &lt;strong&gt;Sad&lt;/strong&gt; and another who mostly uses &lt;strong&gt;Wow&lt;/strong&gt; or &lt;strong&gt;Love.&lt;/strong&gt;&amp;rdquo;&lt;/p&gt;

&lt;p&gt;However, this isn&amp;rsquo;t the first time a big social network has tried implementing reactions alongside Likes/Dislikes. Four years ago, YouTube added &lt;a href=&quot;http://googlesystem.blogspot.com/2011/06/youtube-reactions.html&quot;&gt;Reaction buttons&lt;/a&gt; to their comments section:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/youtube-reactions.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;and removed them sometime after without fanfare, replacing it with the simple Like/Dislike bar.&lt;/p&gt;

&lt;p&gt;Presumably, YouTube implemented the buttons for the similar reason as Facebook. What makes things different now, if anything?&lt;/p&gt;

&lt;h2&gt;A Quantitative Approach to Feeling&lt;/h2&gt;

&lt;p&gt;Even after YouTube&amp;rsquo;s failure, another data-driven website implemented reaction buttons: BuzzFeed (who else?). At the end of each article (in most categories), registered users can select a quirky reaction to indicate how they felt about the article.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/buzzfeedreactions.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The heart represents &lt;strong&gt;Love&lt;/strong&gt; internally and is by-far the most-used reaction on BuzzFeed posts. When I started scraping BuzzFeed data in 2014 &lt;a href=&quot;http://minimaxir.com/2015/01/linkbait/&quot;&gt;to analyze clickbait&lt;/a&gt;, I made sure to grab the reaction data of other reactions as well to see if there are any interesting trends or correlations between reactions. A cursory glance at the scraped reaction data revealed a problem that forced me to disregard it.&lt;/p&gt;

&lt;p&gt;An important part of variable selection for analysis and modeling is avoiding &lt;em&gt;redundant&lt;/em&gt; features, as that can cause issues such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Multicollinearity&quot;&gt;multicollinearity&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Overfitting&quot;&gt;overfitting&lt;/a&gt;. For Facebook, avoiding adding redundant Reactions was an &lt;a href=&quot;https://medium.com/facebook-design/reactions-not-everything-in-life-is-likable-5c403de72a3f&quot;&gt;explicit design goal&lt;/a&gt; of the feature, but the positive emotions such as &lt;strong&gt;Like&lt;/strong&gt; and &lt;strong&gt;Wow&lt;/strong&gt; might be overly similar regardless (I believe it fair to compare the behavior of BuzzFeed users with the average Facebook user, given that they hit the same demographics). Do BuzzFeed readers use specific positive reactions differently? Did they use specific negative reactions?&lt;/p&gt;

&lt;p&gt;I rechecked my 2014 data in light of Facebook Reactions. The scraped dataset contains reaction data from 9,883 BuzzFeed articles in the Celebrity, Animals, Books, Longform, and Business categories. From that, I made a &lt;a href=&quot;http://vita.had.co.nz/papers/gpp.pdf&quot;&gt;pairs plot&lt;/a&gt; for the counts of all the &lt;em&gt;positive&lt;/em&gt; reactions on the articles to illustrate all bivariate relationships:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The lower half of the pairs plot is a scatterplot for the two reactions; the axes represent the number of votes for a given reaction on a BuzzFeed article (both axes are scaled logarithmically), color intensity indicates the number of articles at that X/Y combo, and the line is a linear trendline of least-squares.&lt;/li&gt;
&lt;li&gt;The diagonal of the pairs plot represents the density distribution of reaction vote counts for that reaction. (also logarithmically scaled on the X axis)&lt;/li&gt;
&lt;li&gt;The upper half of the pairs plot illustrates the Pearson correlation between the non-log quantities of the two reaction variables. The stars represent statistical significance of the correlation test; since the data set is large, all correlations are statistically significant (rejection of null hypothesis of no correlation) at p &amp;lt; 0.001.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/buzzfeed-pos.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All of the bivariate correlations of positive reactions are &lt;em&gt;moderately or strongly positively correlated&lt;/em&gt;, which is problematic for analysis (except one: apparently, there is little statistical relationship between things that are cute and things that make you go YAAASS). So why not just use the &lt;strong&gt;Love&lt;/strong&gt; reaction, since articles tend to get about 100 Loves, while other reactions get around 10?&lt;/p&gt;

&lt;p&gt;Does the same hold for negative reactions? Relatedly, we would also expect a negative correlation between the number of &lt;strong&gt;Love&lt;/strong&gt; reactions and negative reactions, right?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/buzzfeed-neg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All negative reactions are positively correlated, as expected, but there is a weak &lt;em&gt;positive&lt;/em&gt; correlation between &lt;strong&gt;Love&lt;/strong&gt; and &lt;strong&gt;Hate&lt;/strong&gt;, which is definitely not right. There isn&amp;rsquo;t an ideal &amp;ldquo;negative&amp;rdquo; reaction, since all have similar distributions.&lt;/p&gt;

&lt;p&gt;Why does Facebook have 6 different responses to gauge positivity or negativity when one reaction for each would be both more accurate and more intuitive for the user?&lt;/p&gt;

&lt;h2&gt;Conceal, Don&amp;rsquo;t Feel&lt;/h2&gt;

&lt;p&gt;There are other qualitative issues with Facebook&amp;rsquo;s current implementation of Reactions. Apparently, Likes and Reactions are treated &lt;em&gt;differently internally&lt;/em&gt;. As a result, you get separate notifications for Likes and Reactions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/facebook_react2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Why? No idea. There is enough Notification spam on Facebook, I don&amp;rsquo;t need &lt;em&gt;double notifications&lt;/em&gt; in my Notification feed for every status I make.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s important to note is that a user cannot both Like and React to a status; only one or the other. As a result, the number of Likes on statuses overall will drop, and this is a &lt;em&gt;major&lt;/em&gt; problem for businesses who are dependent on measuring the number of Likes for engagement.&lt;/p&gt;

&lt;p&gt;I took a look at the Facebook Graph API endpoint for &lt;a href=&quot;https://developers.facebook.com/docs/graph-api/reference/v2.5/post&quot;&gt;Facebook Page Posts&lt;/a&gt; (same endpoint I use for my &lt;a href=&quot;https://github.com/minimaxir/facebook-page-post-scraper&quot;&gt;Facebook Page Data Scraper&lt;/a&gt;), and I can confirm that the API can only report the number of Likes on a status; not the number of Likes + Reactions, or number of Likes + number of each Reaction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/cnn_fb.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is no way currently to automate the retrieval of Reactions data from Facebook posts, which is an unfortunate oversight (especially considering how Twitter &lt;a href=&quot;https://blog.twitter.com/2015/hearts-for-developers&quot;&gt;handled the transition&lt;/a&gt; from Favorites to Likes easily).&lt;/p&gt;

&lt;p&gt;The example &lt;a href=&quot;https://www.facebook.com/cnn/posts/10154506885211509&quot;&gt;CNN story&lt;/a&gt; I used for that screenshot is anecdotally one of the very few examples I&amp;rsquo;ve noticed where the number of Likes is &lt;em&gt;almost equal&lt;/em&gt; to negative emotions, a relationship which should be weakly correlated and therefore this knowledge may be useful to isolate the story as unusual (and serve ads accordingly). At Facebook&amp;rsquo;s immense scale, identifying a relatively small proportion of unusual stories might be enough to justify adding Reactions.&lt;/p&gt;

&lt;p&gt;Or maybe this feature is just the harbinger of a new generation of emotionally-charged linkbait. Perhaps there is more to this Facebook Reactions data than what meets the eye, and I&amp;rsquo;ll update my scripts and do further statistical analysis when able. But given what has happened with Reactions data before with YouTube, I am unconvinced and I still believe the functionality as a whole is a usability regression that won&amp;rsquo;t last.&lt;/p&gt;

&lt;p&gt;A Dislike button would have been better, just saying.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;You can view the code and data used to generate the BuzzFeed Reaction data visualizations &lt;a href=&quot;https://github.com/minimaxir/facebook-reactions/blob/master/buzzfeed_reactions.ipynb&quot;&gt;in this Jupyter notebook&lt;/a&gt;, &lt;a href=&quot;https://github.com/minimaxir/facebook-reactions&quot;&gt;open-sourced on GitHub&lt;/a&gt;, or you can &lt;a href=&quot;https://github.com/minimaxir/facebook-reactions/raw/master/reactions_pdf.pdf&quot;&gt;view as a PDF&lt;/a&gt;, which is better if you are on a mobile device.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Feb 2016 08:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2016/02/facebook-reactions/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/02/facebook-reactions/</guid>
        
        
        <category>Rant</category>
        
      </item>
    
      <item>
        <title>You&#39;re Not Allowed to Criticize Startups, You Stupid Hater</title>
        <description>&lt;p&gt;A couple weeks ago, &lt;a href=&quot;https://itunes.apple.com/us/app/peach-a-space-for-friends/id1067891186?mt=8&quot;&gt;Peach&lt;/a&gt;, a messaging app, was released on iOS. Not only did Peach trend on Twitter on the day of release, but there was also an &lt;em&gt;unusual&lt;/em&gt; amount of media coverage for app, with both &lt;a href=&quot;http://techcrunch.com/2016/01/08/peach-is-a-slick-new-messaging-app-from-the-founder-of-vine/&quot;&gt;TechCrunch&lt;/a&gt; and &lt;a href=&quot;http://www.buzzfeed.com/katienotopoulos/do-i-dare-to-post-on-peach&quot;&gt;BuzzFeed&lt;/a&gt; posting glowing reviews for it, noting that the app is &amp;ldquo;slick&amp;rdquo; and &amp;ldquo;blowing up&amp;rdquo; respectively (commenters in the BuzzFeed article thought it was paid advertising). Usually, reports for apps from both sites are more neutral, which made me curious.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/startup-haters/peach-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are &lt;em&gt;already&lt;/em&gt; an absurd amount of messaging apps out there, and as a result, product differentiation is the primary value proposition. (e.g. security and &lt;a href=&quot;https://itunes.apple.com/us/app/telegram-messenger/id686449807?mt=8&quot;&gt;Telegram Messenger&lt;/a&gt;) What is Peach&amp;rsquo;s differentiation? &amp;ldquo;Magic Words&amp;rdquo;, apparently, which let you perform contextual actions with words! These Magic Words are implemented in a manner similar to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Command-line_interface&quot;&gt;command-line interface&lt;/a&gt; (CLI). &lt;a href=&quot;http://nymag.com/following/2016/01/how-the-command-line-became-mainstream-again.html&quot;&gt;NYMag&lt;/a&gt; goes into extreme detail about the feature. Nerds love CLIs, so you can tell the product was made by smart people!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/startup-haters/peach-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://news.ycombinator.com/item?id=10883698&quot;&gt;Hacker News thread&lt;/a&gt; about the NYMag article was interesting. One commenter (jobu) that the Magic Words are similar to those of &lt;a href=&quot;https://slack.com&quot;&gt;Slack&lt;/a&gt;, which allow for &amp;ldquo;/&amp;rdquo; commands (again, messaging app differentiation). A HN user (pbreit) argued in response:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You&amp;rsquo;re missing the whole thing. For starters, Slack is for work and Peach is for personal. But I think before you dismiss something as &amp;ldquo;isn&amp;rsquo;t it just x plus y?&amp;rdquo; you need to take a little bit more time and thinking to try and figure out what the product designers are trying to achieve and how.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Wait, what? Are the users of Peach supposed to be mind readers? Are people stupid for not understanding the purpose of Peach?&lt;/p&gt;

&lt;p&gt;Another user (thwarted) replied to that response with such:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe the product designers and product providers should be more explicit about how their product should be used and who their target demographic is, if they want a successful product, rather than making everyone &amp;ldquo;figure out&amp;rdquo; what they are trying to achieve and how.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;pbreit replied:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&amp;ldquo;Figuring it out&amp;rdquo; might be the point. You do realize that you are referring to some of the very few people who have actually already built an amazingly successful product.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I should have mentioned earlier that Peach was made by &lt;a href=&quot;http://byte.co&quot;&gt;Byte&lt;/a&gt;, which is ran by &lt;a href=&quot;https://twitter.com/dhof&quot;&gt;Dom Hofmann&lt;/a&gt;, a founder of Vine which was purchased by Twitter before launch (it also appears Peach is a pivot from the failed Byte app; something left out by the blog articles). Yes, if a founder has had a previously successful startup, the previous success implies a higher probability for future startup success than for a startup run by an unknown founder. However, it&amp;rsquo;s still not a 100% guarantee, and startups should not be immune to criticism because of asymmetric information between the users and the startup itself. Peach is impeachable. (pun intended)&lt;/p&gt;

&lt;p&gt;TechCrunch did a &lt;a href=&quot;http://techcrunch.com/2016/01/11/hype-or-not-peach-hit-the-top-10-social-networking-app-list-fast/&quot;&gt;follow-up article&lt;/a&gt; three days after release stating (paraphrased) &amp;ldquo;Peach is #9 in the App Store in the Social Networking category, despite the haters! Now they will be a success! Haters gonna hate!&amp;rdquo; Social networking is a double-edged sword in terms of network effects: they can grow incredibly fast as friends-of-friends register, and they can &lt;em&gt;die&lt;/em&gt; incredibly fast once they all realize the app is a fad. As of publishing, Peach is ranked #110 in the Social Networking category, and off-the-charts in Overall. And &lt;a href=&quot;https://www.appannie.com/apps/ios/app/peach-a-space-for-friends/rank-history/&quot;&gt;still dropping&lt;/a&gt;, with no apparent recovery.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/startup-haters/peach-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I seriously wish the tech media would admit they were wrong, but they never will. The startup world is in dire need of cautionary tales as valuations inflate to absurd levels.&lt;/p&gt;

&lt;p&gt;But hey, isn&amp;rsquo;t any startup ran by a previously-funded founder a sure thing? They know what it takes to create a company, even if it&amp;rsquo;s in a different and highly competitive environment years later than their first success! One success is all a founder needs.&lt;/p&gt;

&lt;p&gt;A startup&amp;rsquo;s release is unpolished and buggy? It&amp;rsquo;s just an MVP, and startups are expected to have issues out of the gate, so you have to forgive them! They don&amp;rsquo;t need QA engineers.&lt;/p&gt;

&lt;p&gt;A startup uses growth hacking to get early traction? It&amp;rsquo;s hustle, and they should be applauded for their creativity! Whoever complains is just in the minority, anyways.&lt;/p&gt;

&lt;p&gt;A startup gets a lot of points on Product Hunt? That&amp;rsquo;s validation, especially since it was submitted by one of the friends/investors of the startup for all their network to see and upvote! A strong personal network is the only thing you need for business success.&lt;/p&gt;

&lt;p&gt;A startup raises a Series A? That means venture capitalists did due diligence, and if the idea sucked, they would not have invested! Millions of dollars is a lot of money to those people.&lt;/p&gt;

&lt;p&gt;A startup loses traction and buzz? It&amp;rsquo;s not dead, it can come back as long as it has money in the bank! Your lack of faith demotivates entrepreneurs.&lt;/p&gt;

&lt;p&gt;A startup &lt;em&gt;dies&lt;/em&gt;? They can just try again! It was bad luck anyways and they likely did nothing wrong.&lt;/p&gt;

&lt;p&gt;A startup is mercy-killed through an acquisition/acquihire? Just another step in their incredible journey! The founders still win, and now they have the &amp;ldquo;exited&amp;rdquo; label to leverage for future employment.&lt;/p&gt;

&lt;p&gt;Sarcastic rhetorical questions aside, it is very annoying that it is considered socially unacceptable in Silicon Valley to criticize a startup. Even though the majority of startups fail, actually &lt;em&gt;saying so&lt;/em&gt; is sacrilegious. In Peach&amp;rsquo;s particular case, the fact that the app is from a previous exited founder should invite &lt;em&gt;more&lt;/em&gt; scrutiny instead of giving it a free pass.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: Fixed joke so that it is funny.&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Jan 2016 08:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2016/01/startup-haters/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/01/startup-haters/</guid>
        
        
        <category>Rant</category>
        
      </item>
    
      <item>
        <title>Video Games and Charity: Analyzing Awesome Games Done Quick 2016 Donations</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://gamesdonequick.com&quot;&gt;Awesome Games Done Quick&lt;/a&gt;, and its sister event Summer Games Done Quick, are a fundraising events that livestreams video game speedruns &lt;a href=&quot;http://www.twitch.tv/gamesdonequick/profile&quot;&gt;live on Twitch&lt;/a&gt; for charity. Beginning in January 2011, before Twitch was launched out from Justin.tv, &lt;a href=&quot;https://en.wikipedia.org/wiki/Awesome_Games_Done_Quick_and_Summer_Games_Done_Quick#List_of_marathons&quot;&gt;AGDQ was very small&lt;/a&gt; and only raised $52,519.83 for the &lt;a href=&quot;http://preventcancer.org&quot;&gt;Prevent Cancer Foundation&lt;/a&gt;; now, in 2016, from January 3rd to January 10th, AGDQ &lt;a href=&quot;https://gamesdonequick.com/tracker/index/agdq2016&quot;&gt;successfully raised&lt;/a&gt; about $1.2 &lt;em&gt;million&lt;/em&gt; for the charity.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/Speedrun&quot;&gt;speedrun&lt;/a&gt;, as the name suggests, is the process of completing a video game as fast as possible, optionally with self-imposed challenges to make things more interesting. Speedruns can emphasize extreme player skill and/or clever glitch abuse. And unexpected mistakes which make the results hilarious.&lt;/p&gt;

&lt;p&gt;One of the first runs of AGDQ 2016, &lt;a href=&quot;https://www.youtube.com/watch?v=jLlian3g7Gg&quot;&gt;Super Monkey Ball&lt;/a&gt;, demonstrates all of these. (run starts at 5:57)&lt;/p&gt;

&lt;div class=&quot;responsive-video&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/jLlian3g7Gg &quot; style=&quot;width: 100%; height: 100%; border: none; padding-bottom: 20px&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;


&lt;p&gt;AGDQ 2016 also has fun with the concept of speedrunning. One of the best events of AGDQ 2016 was a blind speedrun of user-created &lt;a href=&quot;https://www.youtube.com/watch?v=8qC584MWXO4&quot;&gt;Super Mario Maker&lt;/a&gt; levels from top designers, in which hilarity ensued. (run starts at 27:41)&lt;/p&gt;

&lt;div class=&quot;responsive-video&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/8qC584MWXO4 &quot; style=&quot;width: 100%; height: 100%; border: none; padding-bottom: 20px&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;


&lt;p&gt;It might be interesting to know &lt;em&gt;which&lt;/em&gt; video games lead to the achievement of over $1M donated to charity and the nature of the donations in general.&lt;/p&gt;

&lt;h2&gt;Gaming Data&lt;/h2&gt;

&lt;p&gt;With a few quick scripts on Kimono to scrape data from the &lt;a href=&quot;https://gamesdonequick.com/tracker/donations/agdq2016&quot;&gt;AGDQ 2016 donation page&lt;/a&gt; (+ a &lt;em&gt;lot&lt;/em&gt; of postprocessing in R!), I obtained a dataset of all 30,528 donations, their donors, when they donated, during what speedrun they donated, and &lt;em&gt;why&lt;/em&gt; they donated. (&lt;a href=&quot;https://docs.google.com/spreadsheets/d/1yyfkS0jvRK1cWrQesYiBn1TMGC93lo1MqahcU3XeGIU/edit?usp=sharing&quot;&gt;Google Sheets link&lt;/a&gt; for all the data)&lt;/p&gt;

&lt;p&gt;Here are the cumulative donations during AGDQ, color coded by day:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cumulative donations were strong the entire run. On the second-to-last day, the donations rallied and increased exponentially, clearing $1M handily on the last day.&lt;/p&gt;

&lt;p&gt;The donation amount minimum is $5, but the average is significantly higher at $39.62. What is the distribution of donations?&lt;/p&gt;

&lt;p&gt;Here is a distribution of donations from $5 to $100 (for ease of visualization/interpretation), which account for 97% of all donations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The median donation amount is $20. What&amp;rsquo;s interesting is that donations occur at clear break points: not only are there many donations at multiple of $10, but there are many donations at $25 and $75 as well. The $50 and $75 points also potentially benefited for being the threshold for entry into a &lt;a href=&quot;https://gamesdonequick.com/tracker/prizes/agdq2016&quot;&gt;grand prize raffle&lt;/a&gt;. I&amp;rsquo;ll note off-chart that there is a spike in $1,000 donations, the threshold for the audience-clapping in celebration and the &lt;a href=&quot;https://gamesdonequick.com/tracker/donation/212588&quot;&gt;top single donation&lt;/a&gt; was made by an AGDQ sponsor, &lt;a href=&quot;https://www.theyetee.com/&quot;&gt;The Yetee&lt;/a&gt;, at $18,225. The &lt;a href=&quot;https://gamesdonequick.com/tracker/donation/209613&quot;&gt;top donation by a non-sponsor&lt;/a&gt; is from Minecraft creator Notch at $8,000, which he &lt;a href=&quot;https://gamesdonequick.com/tracker/donation/234071&quot;&gt;did twice&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Which games are the most popular and generated the most amount of money for the Prevent Cancer Foundation?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unsurprisingly, Nintendo games are the most popular due to the nostalgia factor. In fairness, the top runs on this chart occur during the last two days of AGDQ 2016, which as mentioned previously may have been affected by a rally, so we cannot assert causality. The appearance of &lt;a href=&quot;http://yachtclubgames.com/shovel-knight/&quot;&gt;Shovel Knight&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Bloodborne&quot;&gt;Bloodborne&lt;/a&gt; as leading donation games, both relatively recently released, shows that speedrunning has more appeal than just retro games.&lt;/p&gt;

&lt;p&gt;A popular technique in charity drives is donation incentives, which help bolster the number of donations total. The &lt;a href=&quot;https://gamesdonequick.com/tracker/bids/agdq2016&quot;&gt;AGDQ bid incentives&lt;/a&gt; can include bonus game segments, or certain game decisions, such as what name to give to a main character.&lt;/p&gt;

&lt;p&gt;Any donation can optionally be assigned as a donation toward an incentive. Which run received the most money toward incentives?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Super Metroid donations incentives accounted for nearly &lt;em&gt;&amp;frac14;th&lt;/em&gt; of all the money raised at AGDQ. Final Fantasy IV accounted for a large amount as well, however, in both cases, the rally effect may apply.&lt;/p&gt;

&lt;p&gt;Speaking of the Super Metroid donation incentives, it should be noted that this particular incentive is one of the most culturally-important incentives in the show. Super Metroid has an optional objective to Save The Animals from planetary destruction, but this costs time, and time is important for a speedrun. Or the speedruner can Kill The Animals through inaction for efficiency, &amp;ldquo;Saving the Frames.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;What is the split of these incentive choices?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yes, the animals were killed (specifically, they were &lt;strong&gt;REKT&lt;/strong&gt;, in the words of a last-minute donator). Both bonus games and vanity naming were popular, but nothing compared to the Save the Animals / Kill the Animals bid war.&lt;/p&gt;

&lt;p&gt;Lastly, people can leave comments with donations, and these comments are usually read on-stream when possible, as you&amp;rsquo;ve likely noticed if you&amp;rsquo;re watched the videos above.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a fun, nonscientific word cloud of those comments:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lots of positivity, aside from the whole &amp;ldquo;Kill the Animals&amp;rdquo; thing. After all, the event is all about preventing cancer.&lt;/p&gt;

&lt;p&gt;While this post isn&amp;rsquo;t an academic analysis, it&amp;rsquo;s neat to see to see what kinds of things drive donation to charity. This model of livestreaming and charitable applications is very successful, and important given the renewed attention toward livestreaming with the rise of Twitch to mainstream attention, alongside the rise of personal streaming with apps like Periscope. Donation incentives are a &lt;em&gt;very&lt;/em&gt; successful technique for facilitating donations.&lt;/p&gt;

&lt;p&gt;It will be interesting to see if Twitch and events like AGDQ can leverage charitable livestreaming, or if another startup/organization beats them first. Bidding-Wars-for-Deciding-the-Fate-of-Fictional-Animals-as-a-Service has a nice ring to it.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;You can access a Jupyter notebook with the data processing and chart processing code &lt;a href=&quot;https://github.com/minimaxir/agdq-2016&quot;&gt;in this GitHub repository&lt;/a&gt;. If you use the processed donation data or visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks! :)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note for donation incentive statistics: the user can theoretically split their donation among multiple incentives; unfortunately I assumed at time of scrape that all the money could only go toward one bid. All donations I investigated from the source data were toward a single incentive except two donations from The Yetee which I fixed manually. If there are any data discrepancies, that is the likely cause.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 08:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2016/01/agdq-2016/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/01/agdq-2016/</guid>
        
        
        <category>Code</category>
        
      </item>
    
      <item>
        <title>Movie Review Aggregator Ratings Have No Relationship with Box Office Success</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://www.rottentomatoes.com&quot;&gt;Rotten Tomatoes&lt;/a&gt; has become synonymous with movie quality in recent years. The Rotten Tomatoes Tomatometer aggregates all reviews written by movie critics for a given movie on the internet, determines whether each reviewer rates the movie as &amp;ldquo;Fresh&amp;rdquo; or &amp;ldquo;Rotten&amp;rdquo; and calculates an average. If the proportion of Fresh reviews for a given movie is greater than or equal to 60%, the movie itself is considered &amp;ldquo;Fresh&amp;rdquo; and receives a special icon.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-revenue-ratings/examples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Top Movies like Christopher Nolan&amp;rsquo;s &lt;a href=&quot;http://www.rottentomatoes.com/m/the_dark_knight/&quot;&gt;The Dark Knight&lt;/a&gt; received a 94% Rotten Tomatoes rating, and generated $533.3 million in domestic box office revenue. But other movies, like Michael Bay&amp;rsquo;s &lt;a href=&quot;http://www.rottentomatoes.com/m/transformers_revenge_of_the_fallen/&quot;&gt;Transformers: Revenge of the Fallen&lt;/a&gt;, received a 19% Tomatometer rating, but still generated $402.1 million in domestic box office revenue.&lt;/p&gt;

&lt;p&gt;How strong is the relationship between Tomatometer scores and box office success, anyways? Or are other, better metrics? Time to make some pretty charts.&lt;/p&gt;

&lt;p&gt;I obtained a large amount of movie data from the &lt;a href=&quot;http://www.omdbapi.com&quot;&gt;OMDb API&lt;/a&gt;, which provides easy access to movie metadata from IMDb and Rotten Tomatoes. This data contains Rotten Tomatoes Tomatometer scores, Rotten Tomatoes Audience Scores, IMDb User Rankings, and Metacritic Scores. If you want to know how I processed the data in R and plotted the charts using ggplot2, I have &lt;a href=&quot;https://www.youtube.com/watch?v=F5Hjlkxw_2A&quot;&gt;prepared a screencast&lt;/a&gt; for your viewing pleasure.&lt;/p&gt;

&lt;div class=&quot;responsive-video&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/F5Hjlkxw_2A &quot; style=&quot;width: 100%; height: 100%; border: none; padding-bottom: 20px&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;


&lt;p&gt;For this analysis, we will be looking at the &lt;a href=&quot;http://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/&quot;&gt;log-transformation&lt;/a&gt; of domestic box office revenue, since the values are skewed by mega-blockbusters like the ones mentioned previously. Revenues are not inflation-adjusted since the rating data is only present for recent years and due to the log-transformation already present, inflation correction would not impact this particular analysis much.&lt;/p&gt;

&lt;h2&gt;Rotten Tomatoes Tomatometer&lt;/h2&gt;

&lt;p&gt;After processing, I have a data subset of 4,863 movies with both Tomatometer and Box Office Gross values. Let&amp;rsquo;s plot all those movies on a scatterplot of log(BoxOffice) vs. Meter with each point having a slight transparency; that way, clusters of points will be come apparent where the areas are darker on the chart.&lt;/p&gt;

&lt;p&gt;We expect a positive linear relationship: movies with high Tomatometer scores to have high box office revenue, and inversely movies with low score to have low box office revenue.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-revenue-ratings/box-office-rating-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Wait, why does the trendline have a &lt;em&gt;negative&lt;/em&gt; slope?&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient&quot;&gt;Pearson correlation&lt;/a&gt; between the Tomatometer scores and log(BoxOffice) is &lt;strong&gt;-0.18&lt;/strong&gt;, implying a weak &lt;em&gt;negative&lt;/em&gt; linear relationship between the two variables. Not what I expected.&lt;/p&gt;

&lt;p&gt;There do appear to be clusters in the data. There is a group of points between $10M and $100M revenue and 0% to 20% Tomatometer rating.  Another group is present between $1,000 and $1M revenue and 80% to 100% RT rating. Both of these areas are outside of a linear relationship: perhaps these clusters are skewing trends too?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try another visualization of the data using &lt;a href=&quot;https://en.wikipedia.org/wiki/Contour_line&quot;&gt;contour maps&lt;/a&gt;, which allow the data to become 3D, so-to-speak. Using a 2D &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_density_estimation&quot;&gt;kernel density estimator&lt;/a&gt;, we can identify and color areas on the plot according to the number of points present in that area; the greater the color saturation, the more points present in the given area.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-revenue-ratings/box-office-rating-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The two clusters mentioned previously are now much more apparent. It appears there are two distinct sets of movies: blockbusters which critics hate, and limited-appeal films which critics loves. Incidentally, there is no discernible difference between movies which are Fresh (&gt;60%) and Rotten.&lt;/p&gt;

&lt;h2&gt;Metacritic&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;http://www.metacritic.com&quot;&gt;Metacritic&lt;/a&gt; score is also &lt;a href=&quot;http://www.metacritic.com/about-metascores&quot;&gt;derived from review data&lt;/a&gt; by critics; however, instead of calculating a binary review sentiment and calculating a proportion from that sentiment, Metacritic gives a quantification from 0 to 100 to each critic review and averages them together.&lt;/p&gt;

&lt;p&gt;Does that change the results for 4,479 movies?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-revenue-ratings/box-office-rating-7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Correlation between Metacritic score and log(BoxOffice) is &lt;strong&gt;-0.13&lt;/strong&gt;, which puts the analysis in a similar state as the Rotten Tomatoes data. However, the blockbuster cluster has shifted right, and the lesser-appeal cluster has shifted left.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-revenue-ratings/box-office-rating-8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clusters are much closer together.&lt;/p&gt;

&lt;p&gt;Perhaps a review metric by non-critics will tell a different story.&lt;/p&gt;

&lt;h2&gt;Rotten Tomatoes Audience Score&lt;/h2&gt;

&lt;p&gt;The Audience Score is calculated in a similar way to the Rotten Tomatoes Tomatometer score: user to the site rate a movie from 0 to 5 stars in half-star increments (i.e. effectively a scale from 0-10) and the proportion of reviews with 3.5 star ratings or higher becomes the Audience Score.&lt;/p&gt;

&lt;p&gt;This also presents a cognitive bias in ratings: the &lt;a href=&quot;http://tvtropes.org/pmwiki/pmwiki.php/Main/FourPointScale&quot;&gt;Four Point Scale&lt;/a&gt;, where having a discrete form of ranking may cause people to tend to rate toward the top of the scale and make the entire metric skewed or misleading.&lt;/p&gt;

&lt;p&gt;How does the Audience Score compare for 5,163 movies? After all, the audience is the group of people who determine how much money a movie makes at the Box Office.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-revenue-ratings/box-office-rating-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Correlation between the Audience score and log(BoxOffice) is &lt;strong&gt;0.05&lt;/strong&gt;, which is a positive linear correlation, but representative of barely any practical correlation.&lt;/p&gt;

&lt;p&gt;Speaking of the Four Point Scale, notice how, like with Metacritic score, there are barely any movies between 0% and 20% Audience Score. Is there really a skew? Let&amp;rsquo;s look at the contours:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-revenue-ratings/box-office-rating-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The locations of the clusters are much different than that of Tomatometer clusters. Both clusters are closer together, with the blockbuster cluster between 50% and 60% audience score and the lesser-appeal cluster between 70% and 80%. Hence, the low correlation.&lt;/p&gt;

&lt;h2&gt;IMDb&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.imdb.com&quot;&gt;IMDb&lt;/a&gt; works &lt;a href=&quot;http://www.imdb.com/help/show_leaf?votestopfaq&quot;&gt;almost the same way&lt;/a&gt; as the Metacritic for non-critics: ratings from IMDb users between 1-10 (note that 0 is missing!) are averaged to get a final score.&lt;/p&gt;

&lt;p&gt;How do 5,167 movies fare?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-revenue-ratings/box-office-rating-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What?!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The point groupings are at the &lt;em&gt;same&lt;/em&gt; positions of ratings, and the correlation between IMDb ratings and log(BoxOffice) is &lt;strong&gt;0.00&lt;/strong&gt;. Yes, there&amp;rsquo;s &lt;em&gt;zero&lt;/em&gt; correlation!&lt;/p&gt;

&lt;p&gt;Checking the contour map confirms it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-revenue-ratings/box-office-rating-6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That is &lt;em&gt;literally&lt;/em&gt; a Four Point Scale between 5 and 8!&lt;/p&gt;

&lt;p&gt;The Rotten Tomatoes metric is the only metric that actually &lt;em&gt;uses&lt;/em&gt; the entire rating scale. None of the other potential metrics provide more insight into a potential reason for high box-office revenue. Perhaps the movie rating system itself is broken.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s not to say that movies need high box-office revenues to be considered successful. However, working with movie profitability, and by extension movie budget, is opening another can-of-worms with respect to data integrity. (that said, on Reddit, /u/chartmkr recently &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/comments/3zpp3w/movie_budgets_and_box_office_success_19552015_oc/&quot;&gt;posted a visualization&lt;/a&gt; of Gross vs. Budget which is interesting).&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;ll still be fun to point to a Rotten Tomatoes Tomatometer rating as a kneejerk reaction to whether a movie rocks/sucks. Although, the reasons for movie financial success at the box office definitely warrant further investigation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UPDATE 1/11/15&lt;/strong&gt;: On a &lt;a href=&quot;https://news.ycombinator.com/item?id=10872076&quot;&gt;discussion on Hacker News&lt;/a&gt;, it was suggested that the blockbuster movies and the indie movies cancel each other out, i.e. blockbusters have a positive correlation and indies have a negative correlation.&lt;/p&gt;

&lt;p&gt;For the blockbuster cluster alone, the log-correlation is &lt;strong&gt;0.23&lt;/strong&gt; (not weak but not great positive correlation). For the indie cluster alone, the log-correlation is &lt;strong&gt;-0.12&lt;/strong&gt; (same as original analysis).&lt;/p&gt;

&lt;p&gt;For future analysis, it may be worthwhile to split these two clusters. I stand by the original analysis for this post: very frequently I&amp;rsquo;ve heard the question &amp;ldquo;is this a good movie?&amp;rdquo; and the response is &amp;ldquo;what does the RT score say?&amp;rdquo; Both Box Office revenues and RT scores are important measures of quality (depending on perspective), and users who want to see or purchase a movie may not necessarily care if it&amp;rsquo;s indie or a blockbuster.&lt;/p&gt;

&lt;p&gt;User cwyers &lt;a href=&quot;https://news.ycombinator.com/item?id=10878019&quot;&gt;suggested&lt;/a&gt; that Simpson&amp;rsquo;s Paradox may be in play since the number of theaters showing a movie is positively correlated to box office revenue, adding a potentially-confounding affect. I will see if I can obtain that data for future analysis.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;You can access the open-sourced Jupyter notebook and high-resolution charts from this article in &lt;a href=&quot;https://github.com/minimaxir/movie-revenue-ratings&quot;&gt;this GitHub repository&lt;/a&gt;. If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Unfortunately, I cannot redistribute the data itself due to licensing concerns.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Jan 2016 08:30:00 -0800</pubDate>
        <link>http://minimaxir.com/2016/01/movie-revenue-ratings/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/01/movie-revenue-ratings/</guid>
        
        
        <category>Code</category>
        
      </item>
    
      <item>
        <title>Let&#39;s Code an Analysis and Visualizations of Yelp Data using R and ggplot2</title>
        <description>&lt;p&gt;One of the reasons I have open-sourced the code for my complicated data visualizations is transparency for the creation process. 2015 was a &lt;a href=&quot;http://qz.com/580859/the-most-misleading-charts-of-2015-fixed/&quot;&gt;year of misleading and incorrect data visualizations&lt;/a&gt;, and I don&amp;rsquo;t want to help contribute to the misconception that data can be used for trickery. &amp;ldquo;Big data&amp;rdquo; in particular is a area where the steps to reproduce results are rarely released publicly in a step-by-step manner, often in an attempt to make the resulting analysis unimpeachable.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s time to take things to the next level of transparency by recording &lt;a href=&quot;https://en.wikipedia.org/wiki/Screencast&quot;&gt;screencasts&lt;/a&gt; of my data analysis and visualizations.&lt;/p&gt;

&lt;p&gt;Last week, ggplot2 author Hadley Wickham released &lt;a href=&quot;http://blog.rstudio.org/2015/12/21/ggplot2-2-0-0/&quot;&gt;a surprise update&lt;/a&gt; for my favorite R package, bumping the version to 2.0.0. Why not celebrate by playing around with ggplot2 and making some pretty charts?&lt;/p&gt;

&lt;h2&gt;Let&amp;rsquo;s Code!&lt;/h2&gt;

&lt;p&gt;I have recorded a screencast of myself coding in R to play around with data from &lt;a href=&quot;http://www.yelp.com/dataset_challenge&quot;&gt;Yelp Dataset Challenge&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/watch?v=Emt9bn0D5ZI&quot;&gt;uploaded it to YouTube&lt;/a&gt;. Additionally, the video can be played at an unusually high quality for screencasting: 1440p on supported browsers, at 60 frames per second.&lt;/p&gt;

&lt;div class=&quot;responsive-video&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/Emt9bn0D5ZI &quot; style=&quot;width: 100%; height: 100%; border: none; padding-bottom: 20px&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;


&lt;p&gt;This particular screencast is also my first significant attempt at working with audio/video editing and voice-over. Feel free to provide suggestions for future videos.&lt;/p&gt;

&lt;p&gt;Since the screencast is 40 minutes long (inadvertently!), I&amp;rsquo;ve written an abridged summary of the screencast, along with some clarification of points made.&lt;/p&gt;

&lt;h2&gt;Yelp Data v2&lt;/h2&gt;

&lt;p&gt;A year ago I made a &lt;a href=&quot;http://minimaxir.com/2014/09/one-star-five-stars/&quot;&gt;blog post analyzing the same Yelp data&lt;/a&gt;. Now that the data set contains 1.6 million reviews (as opposed to just 1.1 million back then), it might be interesting to look at it again to see if anything has changed. The data is formatted as by-line JSON: I wrote a pair of Python scripts to convert it to CSV for easy import into R.&lt;/p&gt;

&lt;p&gt;The screencast centralizes on three R packages: readr, dplyr, and ggplot2. (all authored by Hadley Wickham)&lt;/p&gt;

&lt;p&gt;Loading the dataset into R is easy and fast with &lt;code&gt;read_csv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_reviews &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; read_csv&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;yelp_reviews.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Since dplyr was loaded beforehand, read_csv loads the data into a tbl_df instead of a normal data.frame. When you call a normal data.frame by itself, &lt;em&gt;all data is printed to console&lt;/em&gt;, which is a problem when you have 1.6M rows (yes, that happened during a test recording). Calling a tbl_df results in a very descriptive overview of the data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/overview.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most columns are self-explanatory. &lt;code&gt;review_length&lt;/code&gt; is approximate number of words in the review, &lt;code&gt;pos_words&lt;/code&gt; is the number of positive words in the review, &lt;code&gt;neg_words&lt;/code&gt; is what you expect, &lt;code&gt;net_sentiment&lt;/code&gt; is pos_words - neg_words.&lt;/p&gt;

&lt;p&gt;A quick way to analyze the distribution of numerical data is to perform a summary on the data frame, which returns a by-column &lt;a href=&quot;https://en.wikipedia.org/wiki/Five-number_summary&quot;&gt;five-number summary&lt;/a&gt; + mean:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/summary.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ratings are biased toward 4 and 5 star reviews. There is a lot of skew for review length.&lt;/p&gt;

&lt;p&gt;dplyr makes it easy to add columns in-line with the &lt;code&gt;mutate&lt;/code&gt; command. Let&amp;rsquo;s normalize the pos_words column:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_reviews &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_reviews &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;pos_norm &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; pos_words &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; review_length&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;And we could do similar steps for the neg_words column too. Or use mutate to transform the data of an existing column.&lt;/p&gt;

&lt;p&gt;Onto ggplot2. If you want a quick histogram of univariate data, qplot does just that. Let&amp;rsquo;s visualize the distribution of stars.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;qplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;df_reviews&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; stars&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/lc1-qplot-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Definitely a skew toward 4 and 5 star reviews.&lt;/p&gt;

&lt;p&gt;We can do that for other variables too, like review length.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/lc1-qplot-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What about bivariate data? If you give two variables to qplot, it will create a scatter plot. Perhaps there is a relationship between the number of stars and the number of positive words?&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;qplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;df_reviews&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; stars&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; pos_words&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;and then we run into a problem. In this case, ggplot2 has to plot 1.6M points to screen, which can take awhile, especially if you are simultaneously using your GPU for video recording. Eventually, we get this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/lc1-qplot-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At first glance, there appears to be a positive correlation between star rating and number of positive words, but that&amp;rsquo;s misleading: since we don&amp;rsquo;t have alpha transparency on the points, the density is ambiguous. (fixing it requires working outside of a qplot).&lt;/p&gt;

&lt;h2&gt;Serious Business Data&lt;/h2&gt;

&lt;p&gt;We load the Yelp Businesses data into R through the same way as the reviews data. Here&amp;rsquo;s an overview of the data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/businesses.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Both data frames have a &lt;code&gt;business_id&lt;/code&gt; column. We can merge them with a &lt;code&gt;left_join&lt;/code&gt;, a la SQL. If both data frames have a column with the same name, it will merge on that column by default.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_reviews &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_reviews &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; left_join&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df_businesses&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Then the R console helpfully points out that both dataframes also have a &amp;ldquo;stars&amp;rdquo; column. Uh-oh.&lt;/p&gt;

&lt;p&gt;We reset the df_reviews data frame from scratch and merge again, explicitly stating the &amp;ldquo;by&amp;rdquo; column for merging. Now we know &lt;em&gt;where&lt;/em&gt; reviews were made, and that might provide helpful information.&lt;/p&gt;

&lt;h2&gt;Aggregation Station&lt;/h2&gt;

&lt;p&gt;It might be interesting to know the average star rating by city. dplyr allows for &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarize&lt;/code&gt; operations in a similar manner as SQL.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_cities &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_reviews &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;city&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; summarize&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;avg_stars &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;stars.x&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/cities.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;that&amp;rsquo;s not good. The original Yelp Dataset Challenge page mentioned that the dataset is only from specific cities, not &amp;ldquo;1023 E Frye Rd.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/Dataset_Map.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hmrph.&lt;/p&gt;

&lt;p&gt;From the map, it appears there is no overlap between any of the cities with geographic states, so let&amp;rsquo;s use &lt;code&gt;state&lt;/code&gt; instead. Additionally, we can add a count of reviews from that state, and sort by that count descending.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_states &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_reviews &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; summarize&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;avg_stars &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;stars.x&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; count&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; arrange&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;desc&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/states.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looks good enough, but that&amp;rsquo;s tempting fate.&lt;/p&gt;

&lt;h2&gt;ggplot All the Things&lt;/h2&gt;

&lt;p&gt;We can plot state vs. avg_stars with ggplot2. Setting it up is easy:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;df_states&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; avg_stars&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/lc1-ggplot-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The blank plot is actually new to 2.0.0: running the code without any layers would normally throw an error. The axis values appear valid. Let&amp;rsquo;s add columns via &lt;code&gt;geom_bar&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;df_states&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; avg_stars&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_bar&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;and this results in an error. geom_bar by itself does histograms on raw values, as shown in the qplots. The correct fix is to add a &lt;code&gt;stat=&quot;identity&quot;&lt;/code&gt; parameter to geom_bar, which tells it to scale the bars by the given value of the aesthetic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/lc1-ggplot-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Better. But the x-axis is cluttered and the States would look better on the y-axis. Time for a &lt;code&gt;coord_flip&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/lc1-ggplot-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Better. Now time to fix the order. You may notice that the order of the states is alphabetical going from the bottom of the axis to the top, and R will always set this order for any character vector. We want the sort the labels by their average star rating, descending. To do that we change the internal factor labels of state volume to the specified order.&lt;/p&gt;

&lt;p&gt;In the recording, this took awhile due to several brain farts (which happen often when dealing with factor ordering). First, we need to remove a few states with few reviews using a filter The easiest way to do this is to sort the original data frame by avg_stars descending, then set the factor order by using the new state order &lt;em&gt;in reverse&lt;/em&gt;. (Ok, ok, it might be easier to just sort ascending and not reverse, but it makes the overview harder to visualize)&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_states &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_states &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; arrange&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;desc&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;avg_stars&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; levels&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;rev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/states-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rerunning the plot code afterward yields:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/lc1-ggplot-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Good! Why not add labels for each point? This can be done with geom_text, along with adding &lt;code&gt;hjust=1&lt;/code&gt; to offset the label, changing the size, and setting the text to white. We can round the avg_star values to 2 decimal places as well.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;df_states&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; avg_stars&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_bar&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;stat&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;identity&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; coord_flip&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_text&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;label&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;avg_stars&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; hjust&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/lc1-ggplot-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;3.7&amp;rdquo; label requires using the &lt;code&gt;sprintf&lt;/code&gt; function instead of &lt;code&gt;round&lt;/code&gt; to print &amp;ldquo;3.70&amp;rdquo;, which is not fun. Otherwise, these labels are nice so far. Why not add a theme and axis labels?&lt;/p&gt;

&lt;p&gt;I go to my &lt;a href=&quot;http://minimaxir.com/2015/02/ggplot-tutorial/&quot;&gt;previous ggplot2 tutorial&lt;/a&gt; and copy-paste the FiveThirtyEight-inspired theme from there because I am efficient. (The theme required loading the RColorBrewer package, though). The axis labels are added through the &lt;code&gt;labs&lt;/code&gt; function. (note that since the axes are flipped, the labels must be flipped too!)&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;df_states&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; avg_stars&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_bar&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;stat&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;identity&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; coord_flip&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_text&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;label&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;avg_stars&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; hjust&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; fte_theme&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Average Star Rating by State&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;State&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; title&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Average Yelp Review Star Ratings by State&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/lc1-ggplot-6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Why not add 95% confidence intervals for each average? (Note that the normality assumptions for the confidence interval may not be entirely valid). We can calculate the standard error of the mean and rebuild the dataframe and reorder factors again.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_states &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_reviews &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; summarize&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;avg_stars &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;stars.x&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; count&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; se_mean&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;sd&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;stars.x&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; arrange&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;desc&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;avg_stars&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; levels&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;rev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Time to add a &lt;code&gt;geom_errorbar&lt;/code&gt; (not a &lt;code&gt;geom_crossbar&lt;/code&gt;!)&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;df_states&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;state&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; avg_stars&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_bar&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;stat&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;identity&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; coord_flip&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_text&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;label&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;avg_stars&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; hjust&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; fte_theme&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Average Star Rating by State&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;State&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; title&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Average Yelp Review Star Ratings by State&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; geom_errorbar&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;ymin&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;avg_stars &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.96&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt; se_mean&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; ymax&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;avg_stars &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.96&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt; se_mean&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lets-code-1/lc1-ggplot-7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Averages are very stable for all cities due to the large sample size.&lt;/p&gt;

&lt;p&gt;At this point I realized the recording is too long and I end it there. For a normal blog post, I&amp;rsquo;d add more theming, adjust colors so they don&amp;rsquo;t clash, and add annotations, such as a line representing the true review average from the population. And ideally, performing statistical tests to determine if any averages are different from the population average.&lt;/p&gt;

&lt;p&gt;Hopefully this gives some insight into the mechanical process of creating simple data visualizations with R and ggplot2 (the &amp;ldquo;abridged summary&amp;rdquo; ended up being as long as a typical blog post!). As my screencast shows, programming is a recurring process of saying &amp;ldquo;this is easy to do!&amp;rdquo; then failing miserably for stupid reasons. Even after the 40 minute screencast, there&amp;rsquo;s still much, much more polish needed for the data visualization. My blog posts take a very long time to produce for those reasons; the clear, clean code from the finished product is not indicative of the unexpected errors that occur when writing it.&lt;/p&gt;

&lt;p&gt;I did this recording &amp;ldquo;blind&amp;rdquo; to test whether or not it&amp;rsquo;s feasible for me to &lt;em&gt;stream&lt;/em&gt; the coding of data visualization on services like &lt;a href=&quot;http://www.twitch.tv&quot;&gt;Twitch&lt;/a&gt;. It&amp;rsquo;s definitely possible, but has more logistical challenges. (namely, that &lt;a href=&quot;https://obsproject.com&quot;&gt;OBS&lt;/a&gt; is fussy outside of Windows and I still need to figure out how to configure it optimally). I admit the code in this screencast may not be the highest-quality code (in retrospect I should have put the code in an editor instead of directly in the console, and reuse dataframe/ggplot objects), but the transparent process for coding data visualizations is important. If there is enough interest, I may revisit Yelp data again, or even more advanced datasets.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;You can access the R code used for the data visualizations and the Python scripts used to process the raw Yelp dataset &lt;a href=&quot;https://github.com/minimaxir/lets-code-1&quot;&gt;in this GitHub repository&lt;/a&gt;. However, the raw data itself cannot be redistributed.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For those wondering what I used for recording the screencast:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Computer: &lt;em&gt;Late 2013 13&quot; Retina MacBook Pro running OS X 10.11.2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Recording Software: &lt;em&gt;Screenflow 4.5&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Microphone: &lt;em&gt;Shure MV5 Digital Condenser&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Music: &lt;em&gt;Various artists from the &amp;ldquo;No Attribution Required&amp;rdquo; section of the YouTube Audio Library&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 28 Dec 2015 09:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2015/12/lets-code-1/</link>
        <guid isPermaLink="true">http://minimaxir.com/2015/12/lets-code-1/</guid>
        
        
        <category>Code</category>
        
      </item>
    
      <item>
        <title>Mapping Where Arrests Frequently Occur in San Francisco Using Crime Data</title>
        <description>&lt;p&gt;In my previous post, &lt;a href=&quot;http://minimaxir.com/2015/12/sf-arrests/&quot;&gt;Analyzing San Francisco Crime Data to Determine When Arrests Frequently Occur&lt;/a&gt;, I found out that there are trends where SF Police arrests occur more frequently than others. By processing the &lt;a href=&quot;https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry&quot;&gt;SFPD Incidents dataset&lt;/a&gt; from the &lt;a href=&quot;https://data.sfgov.org&quot;&gt;SF OpenData portal&lt;/a&gt;, I found that arrests typically occur on Wednesdays at 4-5 PM, and that the type of crime is relevant to the frequency of the crime. (e.g. DUIs happen late Friday/Saturday night).&lt;/p&gt;

&lt;p&gt;However, I could not understand &lt;em&gt;why&lt;/em&gt; Wednesday/4-5PM is a peak time for arrests. In addition to analyzing &lt;em&gt;when&lt;/em&gt; arrests occur, I also looked at &lt;em&gt;where&lt;/em&gt; arrests occur. For example, perhaps more crime happens as people are leaving work; in that case, we would expect to see crimes downtown.&lt;/p&gt;

&lt;h2&gt;Making a Map of SF Arrests&lt;/h2&gt;

&lt;p&gt;Continuing from the previous analysis, I have a data frame of all police arrests that have occurred in San Francisco from 2003 - 2015 (587,499 arrests total).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrest-map/arrests.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What is the most efficient way to make a map for the data? There are too many data points for rendering each point in a tool like &lt;a href=&quot;http://www.tableau.com&quot;&gt;Tableau&lt;/a&gt; or &lt;a href=&quot;https://www.google.com/maps&quot;&gt;Google Maps&lt;/a&gt;. I can use &lt;code&gt;ggplot2&lt;/code&gt; again as I did &lt;a href=&quot;http://minimaxir.com/2015/11/nyc-ggplot2-howto/&quot;&gt;to make a map of New York City&lt;/a&gt; manually, but as noted in that article, the abstract nature of the map may hide information.&lt;/p&gt;

&lt;p&gt;Enter &lt;code&gt;ggmap&lt;/code&gt;. ggmap, an &lt;a href=&quot;https://github.com/dkahle/ggmap&quot;&gt;R package by David Kahle&lt;/a&gt;, is a tool that allows the user to retrieve a map image from a number of map data providers, and integrates seamlessly with ggplot2 for simple visualization creation. Kahle and Hadley Wickham (the creator of ggplot2) &lt;a href=&quot;https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf&quot;&gt;coauthored a paper&lt;/a&gt; describing practical applications of ggmap.&lt;/p&gt;

&lt;p&gt;I will include most of the map generation code in-line. &lt;em&gt;For more detailed code and output, a &lt;a href=&quot;https://github.com/minimaxir/sf-arrests-when-where/blob/master/crime_data_sf.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt; containing the code and visualizations used in this article is available open-source on GitHub.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By default, you can ask ggmap just for a location using &lt;code&gt;get_map()&lt;/code&gt;, and it will give you an approximate map around that location. You can configure the zoom level on that point as well. Optionally, if you need precise bounds for the map, you can set the bounding box manually, and the &lt;a href=&quot;http://boundingbox.klokantech.com&quot;&gt;Bounding Box Tool&lt;/a&gt; works extremely well for this purpose, with the CSV coordinate export already being in the correct format.&lt;/p&gt;

&lt;p&gt;ggmap allows maps from sources such as &lt;a href=&quot;https://www.google.com/maps&quot;&gt;Google Maps&lt;/a&gt; and &lt;a href=&quot;http://www.openstreetmap.org/#map=5/51.500/-0.100&quot;&gt;OpenStreetMap&lt;/a&gt;, and the maps can be themed, such as a color map of a black-and-white map. A black-and-white minimalistic map would be best for readability. A &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/comments/3ule41/mapping_restaurants_in_san_francisco_by_health/&quot;&gt;Reddit submission by /u/all_genes_considered&lt;/a&gt; used &lt;a href=&quot;http://maps.stamen.com/#terrain/12/37.7706/-122.3782&quot;&gt;Stamen maps&lt;/a&gt; as a source with the toner-lite theme, and that worked well.&lt;/p&gt;

&lt;p&gt;Since we&amp;rsquo;ve identified the map parameters, now we can request an appropriate map:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;bbox &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-122.516441&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;37.702072&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-122.37276&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;37.811818&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;map &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; get_map&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;location &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; bbox&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;stamen&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; maptype &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;toner-lite&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Plotting the map by itself results in something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrest-map/sf-arrest-where-0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the right track (aside from two Guerrero Streets), but obviously it&amp;rsquo;ll need some aesthetic adjustments.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s plot all 587,499 arrests on top of the map for fun and see what happens.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggmap&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;map&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            geom_point&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; df_arrest&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;X&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Y&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; color &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;#27AE60&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; size &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; alpha &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            fte_theme&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            theme&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;&amp;hellip;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;title &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Locations of Police Arrests Made in San Francisco from 2003 – 2015&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ggmap()&lt;/code&gt; sets up the base map.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;geom_point()&lt;/code&gt; plots points. The data for plotting is specified at this point. &amp;ldquo;color&amp;rdquo; and &amp;ldquo;size&amp;rdquo; parameters do just that. An alpha of 0.01 causes each point to be 99% transparent; therefore, addresses with a lot of points will be more opaque.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fte_theme()&lt;/code&gt; is my theme based on the FiveThirtyEight style.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;theme()&lt;/code&gt; is needed for a few additional theme tweaks to remove the axes/margins&lt;/li&gt;
&lt;li&gt;&lt;code&gt;labs()&lt;/code&gt; is for labeling the plot (&lt;em&gt;always&lt;/em&gt; label!)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Rendering the plot results in:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/Xu8wXzc.png&quot; target=_blank&gt;&lt;img src=&quot;/img/sf-arrest-map/sf-arrest-where-1.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;NOTE: All maps in this article are embedded at a lower size to ensure that the article doesn&amp;rsquo;t take days to load. To load a high-resolution version of any map, click on it and it will open in a new tab.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Additionally, since ggmap forces plots to a fixed ratio, this results in the &amp;ldquo;random white space&amp;rdquo; problem mentioned in the NYC article, for which I still have not found a solution, but have minimized the impact.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s clear to see where arrests in the city occur. A large concentration in the Tenderloin and the Mission, along with clusters in Bayview and Fisherman&amp;rsquo;s Wharf. However, point-stacking is not helpful when comparing high-density areas, so this visualization can be optimized.&lt;/p&gt;

&lt;p&gt;How about faceting by type of crime again? We can render a map of San Francisco for each type of crime, and then we can see if the clusters for a given type of crime are different from others.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggmap&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;map&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            geom_point&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; df_arrest &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Category &lt;span class=&quot;o&quot;&gt;%in%&lt;/span&gt; df_top_crimes&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;Category&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;X&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Y&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Category&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; alpha&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            fte_theme&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            theme&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;&amp;hellip;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;title &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Locations of Police Arrests Made in San Francisco from 2003 – 2015, by Type of Crime&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            facet_wrap&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; Category&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; nrow &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Again, only one line of new code for the facet, although the source data needs to be filtered as it was in the previous post.&lt;/p&gt;

&lt;p&gt;Running the code yields:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/6NgzV3k.jpg&quot; target=_blank&gt;&lt;img src=&quot;/img/sf-arrest-map/sf-arrest-where-2.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is certainly more interesting (and pretty). Some crimes, such as Assaults, Drugs/Narcotics, and Warrants, occur all over the city. Other crimes, such as Disorderly Conduct and Robbery, primarily have clusters in the Tenderloin and in the Mission close to the 16th Street BART stop. (Prostitution notably has a cluster in the Mission and a cluster &lt;em&gt;above&lt;/em&gt; the Tenderloin.)&lt;/p&gt;

&lt;p&gt;Again, we can&amp;rsquo;t compare high-density points, so now we should probably normalize the data by facet. One way to do this is to weight each point by the reciprocal of the number of points in the facet (e.g. if there are 5,000 Fraud arrests, assign a weight of 1/5000 to each Fraud arrest), and aggregate the sums of the weights in a geographical area.&lt;/p&gt;

&lt;p&gt;We can reuse the normalization code from the previous post, and the hex overlay code from my NYC taxi plot post as well:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;sum_thresh &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; threshold &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; threshold&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;))}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggmap&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;map&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            stat_summary_hex&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; df_arrest &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Category &lt;span class=&quot;o&quot;&gt;%in%&lt;/span&gt; df_top_crimes&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;Category&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Category&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;w&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;()),&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;X&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Y&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; z&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;w&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; fun&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;sum_thresh&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; alpha &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;#CCCCCC&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            fte_theme&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            theme&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;&amp;hellip;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_fill_gradient&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;low &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;#DDDDDD&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; high &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;#2980B9&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;title &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Locations of Police Arrests Made in San Francisco from 2003 – 2015, Normalized by Type of Crime&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            facet_wrap&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; Category&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; nrow &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sum_thresh()&lt;/code&gt; is a helper function that aggregates the sums of weights, but will not plot the corresponding hex if there is not enough data at that location.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scale_fill_gradient()&lt;/code&gt; sets the gradient for the chart. If there are few arrests, the hex will be gray; if there are many arrests, it will be deep blue.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Putting it all together:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/LXKPseq.jpg&quot; target=_blank&gt;&lt;img src=&quot;/img/sf-arrest-map/sf-arrest-where-3.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This confirms the interpretations mentioned above.&lt;/p&gt;

&lt;p&gt;Since the code base is already created, it is very simple to facet on any variable. So why not create a faceted map for &lt;em&gt;every conceivable variable&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;How about checking arrest locations by Police Districts?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/i82wsIZ.png&quot; target=_blank&gt;&lt;img src=&quot;/img/sf-arrest-map/sf-arrest-where-4.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The map shows that the hex plotting works correctly, at the least. Notably, the Central, Northern, and Southern Police Districts end up making a large proportion of their arrests nearby the Tenderloin/Market Street instead of anywhere else in their area of perview.&lt;/p&gt;

&lt;p&gt;Is the location of arrests seasonal? Does it vary by the month the arrest occured?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/u2eQMZf.png&quot; target=_blank&gt;&lt;img src=&quot;/img/sf-arrest-map/sf-arrest-where-5.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nope. Still Tenderloin and Mission.&lt;/p&gt;

&lt;p&gt;Maybe the locations of arrests have changed over time, as legal polices changed. Let&amp;rsquo;s facet by year.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/x4SRnkU.png&quot; target=_blank&gt;&lt;img src=&quot;/img/sf-arrest-map/sf-arrest-where-6.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here things are &lt;em&gt;slightly&lt;/em&gt; different across each facet; Tenderloin had a much higher concentration of arrests peaking in 2009-2010, and the concentration of yearly arrests in the Tenderloin has decreased relative to everywhere else in the city.&lt;/p&gt;

&lt;p&gt;Does the location of arrests vary by the time of day? As noted earlier, there could be more arrests downtown during working hours.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/cDKo8Lt.png&quot; target=_blank&gt;&lt;img src=&quot;/img/sf-arrest-map/sf-arrest-where-7.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Higher relative concentration in Tenderlion/Mission during work hours, lesser during the night.&lt;/p&gt;

&lt;p&gt;Last try. Perhaps the day of week leads to different locations, especially as people tend to go out to bars all across the city.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/tNBRilL.png&quot; target=_blank&gt;&lt;img src=&quot;/img/sf-arrest-map/sf-arrest-where-8.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Zero&lt;/em&gt; difference. Eh.&lt;/p&gt;

&lt;p&gt;We did learn that there is certainly a lot of arrests in the Tenderloin and 16th Street/Mission BART stop, though. However, that doesn&amp;rsquo;t necessarily mean there is more &lt;em&gt;crime&lt;/em&gt; in those areas (correlation does not imply causation), but it is something worth noting when traveling around the San Francisco.&lt;/p&gt;

&lt;h2&gt;Bonus: Do Social Security payments lead to an increase in arrests?&lt;/h2&gt;

&lt;p&gt;In response to my previous article, &lt;a href=&quot;https://www.reddit.com/r/sanfrancisco/comments/3vfgg2/analyzing_san_francisco_crime_data_to_determine/cxn29wd&quot;&gt;Redditor /u/NowProveIt hypothesizes&lt;/a&gt; that the spike in Wednesday arrests could be attributed to &lt;a href=&quot;https://www.ssa.gov/kc/rp_paybenefits.htm&quot;&gt;Social Security disability&lt;/a&gt; (RDSI) payments. The &lt;a href=&quot;https://www.socialsecurity.gov/pubs/EN-05-10031-2015.pdf&quot;&gt;Social Security Benefit Payments schedule&lt;/a&gt; is typically every second, third, and fourth Wednesday of a month.&lt;/p&gt;

&lt;p&gt;Normally, you would expect that the arrest behavior for any Wednesday in a given month to be independent from each other. Therefore, if the arrest behavior for the &lt;em&gt;first&lt;/em&gt; Wednesday is different than that for the secord/third/fourth Wednesday (presumably, the First Wednesday has fewer arrests overall), then we might have a lead.&lt;/p&gt;

&lt;p&gt;Through more &lt;code&gt;dplyr&lt;/code&gt; shenanigans, I am able to filter the dataset of arrests to Wednesday arrests only, and classify each Wednesday as the first, second, third, or fourth of the month. (there are occasionally fifth Wednesdays but no one cares about those).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrest-map/ordinal.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can plot a single line chart for each ordinal of the number of arrests over the day. We are looking to see if the First Wednesday has different behavior than the other Wednesdays.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrest-map/ssi-crime-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;and it doesn&amp;rsquo;t.&lt;/p&gt;

&lt;p&gt;Looking at locations data doesn&amp;rsquo;t help either.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/wTDKjOm.png&quot; target=_blank&gt;&lt;img src=&quot;/img/sf-arrest-map/ssi-crime-2.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Oh well, it was worth a shot.&lt;/p&gt;

&lt;p&gt;As always, all the code and raw images are available &lt;a href=&quot;https://github.com/minimaxir/sf-arrests-when-where&quot;&gt;in the GitHub repository&lt;/a&gt;. Not many more questions were answered by looking at the location data of San Francisco crimes. But that&amp;rsquo;s OK. There&amp;rsquo;s certainly other cool things to do with this data. Kaggle, for instance, is creating &lt;a href=&quot;https://www.kaggle.com/c/sf-crime/scripts&quot;&gt;a repository of scripts&lt;/a&gt; which play around with the Crime Incident dataset.&lt;/p&gt;

&lt;p&gt;But for now, at least I made a few pretty charts out of it.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Dec 2015 08:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2015/12/sf-arrest-maps/</link>
        <guid isPermaLink="true">http://minimaxir.com/2015/12/sf-arrest-maps/</guid>
        
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Analyzing San Francisco Crime Data to Determine When Arrests Frequently Occur</title>
        <description>&lt;p&gt;The &lt;a href=&quot;https://data.sfgov.org&quot;&gt;SF OpenData portal&lt;/a&gt; is a good source for detailed statistics about San Francisco. One of the most popular datasets on the portal is the &lt;a href=&quot;https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry&quot;&gt;SFPD Incidents dataset&lt;/a&gt;, which contains a tabular list of 1,842,050 reports (at time of writing) from 2003 to present.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/incident-data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The data can be exported into a 377.9 MB CSV; not large enough to be considered &amp;ldquo;big data,&amp;rdquo; but still too heavy for programs like Excel to process efficiently. Let&amp;rsquo;s take a look at the data using &lt;a href=&quot;https://www.r-project.org&quot;&gt;R&lt;/a&gt; and see if there&amp;rsquo;s anything interesting.&lt;/p&gt;

&lt;h2&gt;Processing the Data&lt;/h2&gt;

&lt;p&gt;For this article, I&amp;rsquo;m going to do something different and illustrate the data processing step-by-step, both as a teaching tool, and to show that I am not using vague methodology to generate a narratively-convenient conclusion. &lt;em&gt;For more detailed code and output, a &lt;a href=&quot;https://github.com/minimaxir/sf-arrests-when-where/blob/master/crime_data_sf.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt; containing the code and visualizations used in this article is available open-source on GitHub.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Loading a 1.9 million row file into R can take awhile, even on modern computers with a SSD. Enter &lt;code&gt;readr&lt;/code&gt;, &lt;a href=&quot;https://github.com/hadley/readr&quot;&gt;another R package&lt;/a&gt; by &lt;code&gt;ggplot2&lt;/code&gt; author Hadley Wickham, which grants access to a &lt;code&gt;read_csv()&lt;/code&gt; function that has nearly 10x the speed of the base &lt;code&gt;read.csv()&lt;/code&gt; R function, with more sensible defaults too.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;path &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;~/Downloads/SFPD_Incidents&lt;em&gt;-&lt;/em&gt;from_1_January_2003.csv&amp;quot;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;df &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; read_csv&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;path&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;In memory, the data set is 180.9 MB, and removing a few useless columns (e.g. IncidentNum) further reduces the size to 126.9 MB. Since there are many redundancies in the row data (e.g. only 10 distinct PdDistrict values), R can perform memory optimizations.&lt;/p&gt;

&lt;p&gt;You may have noticed in the first article image that the text data in some of the columns is in ALL CAPS, which would look ugly if the text was used in a data visualization. We can create a helper function to convert a column of text values into proper case through the use of &lt;a href=&quot;http://stackoverflow.com/questions/15776732/how-to-convert-a-vector-of-strings-to-title-case&quot;&gt;regular expression shenanigans&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;proper_case &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;gsub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;\b([A-Z])([A-Z]+)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;\U\1\L\2&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; x&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; perl&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Now we can do more through processing using &lt;code&gt;dplyr&lt;/code&gt;, &lt;a href=&quot;https://github.com/hadley/dplyr&quot;&gt;&lt;em&gt;another&lt;/em&gt; Hadley Wickham R package&lt;/a&gt;. dplyr is a utility that makes R easier to use: it provides a new syntax that allows data manipulation with intuitive function names, the functions can be chained using the &lt;code&gt;%&amp;gt;%&lt;/code&gt; operator for efficiency, and all data processing is &lt;em&gt;significantly&lt;/em&gt; faster due to a C++ code base. (Fun fact: before the release of dplyr, I intended to quit using R for data analysis in favor of Python. Base R syntax is &lt;em&gt;that&lt;/em&gt; difficult to use.)&lt;/p&gt;

&lt;p&gt;In dplyr, &lt;code&gt;mutate&lt;/code&gt; allows the creation and transformation of columns. We will transform the text columns by running the columns through the &lt;code&gt;proper_case&lt;/code&gt; function earlier:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Category &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; proper_case&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Category&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 Descript &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; proper_case&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Descript&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 PdDistrict &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; proper_case&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;PdDistrict&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 Resolution &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; proper_case&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Resolution&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;After all that, the data looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/processed-table.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Much better!&lt;/p&gt;

&lt;p&gt;However, many of the records have a &amp;ldquo;None&amp;rdquo; value for Resolution. This implies that the police appeared at the incident but did no action, which isn&amp;rsquo;t that helpful for analysis. How about we look at incidents which resulted in an arrest?&lt;/p&gt;

&lt;p&gt;dplyr&amp;rsquo;s  &lt;code&gt;filter&lt;/code&gt; command does that, and we can use &lt;code&gt;grepl()&lt;/code&gt; to do a text search for each Resolution value for the presence of &amp;ldquo;Arrest&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_arrest &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;grepl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Arrest&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; Resolution&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s it! There are 587,499 arrests total in the dataset.&lt;/p&gt;

&lt;h2&gt;Arrests Over Time&lt;/h2&gt;

&lt;p&gt;One of the most simple data visualizations is a line chart, and it&amp;rsquo;s a good starting point to use for analyzing arrests. Has the number of daily arrests been changing over time? dplyr and ggplot2 make this very easy to visualize in R.&lt;/p&gt;

&lt;p&gt;First, the Date column must be formatted as a Date internally in R instead of text. Then we &lt;code&gt;group_by&lt;/code&gt; the Date, and then use &lt;code&gt;summarize&lt;/code&gt; to perform an aggregate on each group; in this case, count how many entries for the group. (&lt;code&gt;n()&lt;/code&gt; is a convenient shortcut). We can also ensure that the dates are in ascending order.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_arrest_daily &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_arrest &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Date &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;as.Date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Date&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;%m/%d/%Y&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Date&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    summarize&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; n&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    arrange&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Date&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/date-table.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nifty! However, keep in mind that there are thousands of days in this dataset.&lt;/p&gt;

&lt;p&gt;Now we can make a pretty line chart in ggplot2. Here&amp;rsquo;s the code, and I will explain what everything does afterward:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df_arrest_daily&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Date&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; count&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    geom_line&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;color &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;#F2CA27&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; size &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    geom_smooth&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;color &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;#1A1A1A&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    fte_theme&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    scale_x_date&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;breaks &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; date_breaks&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;2 years&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; labels &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; date_format&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;%Y&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Date of Arrest&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;# of Police Arrests&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; title &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Daily Police Arrests in San Francisco from 2003 – 2015&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ggplot()&lt;/code&gt; sets up the base chart and axes.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;geom_line()&lt;/code&gt; creates the line for the line chart. &amp;ldquo;color&amp;rdquo; and &amp;ldquo;size&amp;rdquo; parameters do just that.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;geom_smooth()&lt;/code&gt; adds a smoothing spline on top of the chart to serve as a trendline, which is helpful since there are a lot of points.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fte_theme()&lt;/code&gt; is my theme based on the FiveThirtyEight style.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scale_x_date()&lt;/code&gt; explicitly sets the x-axis to scale with date values. However, there are a few extremely useful formatting parameters with this function: &amp;ldquo;breaks&amp;rdquo; lets you set the chart breaks in plain English, and &amp;ldquo;labels&amp;rdquo; lets you format the dates at this breaks; in this case, there are breaks every 2 years, and only the year will be displayed for minimalism.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;labs()&lt;/code&gt; is a quick shortcut for labeling your axes and plot (&lt;em&gt;always&lt;/em&gt; label!)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Running the code and saving the output results in this image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/sf-arrest-when-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The line chart has high variation due to the number of points (in retrospect, a 30-day moving average of arrests would work better visually). As the trendline indicates, the trend is actually &lt;em&gt;multimodal&lt;/em&gt;, with daily arrest peaks in 2009 and 2014. Definitely interesting. The number of arrests appears to be on a downward trend since then.&lt;/p&gt;

&lt;p&gt;The next step is to look into possible answers for the day-by-day variation.&lt;/p&gt;

&lt;h2&gt;When Do Arrests Happen?&lt;/h2&gt;

&lt;p&gt;One of my go-to data visualizations is a heat map of times of week; in this case, we can find which day-of-week and time-of-day when the most Arrests occur in San Francisco, and compare that with other time slots at a glance.&lt;/p&gt;

&lt;p&gt;This requires the Hour and Day-of-Week to be present in separate columns: we have a DOY column already, but we need to parse the Hour component out of the HH:MM values in the Time column.&lt;/p&gt;

&lt;p&gt;This requires another helper function which uses &lt;code&gt;strsplit()&lt;/code&gt; to split a single time value to Hour and Minute components, take the first value (Hour), and convert that value to a numeric value (instead of text) For example, &amp;ldquo;09:40&amp;rdquo; input returns 9.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;get_hour &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;as.numeric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;strsplit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]][&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, this will not work for an entire column. Using &lt;code&gt;sapply()&lt;/code&gt; applies a specified function to each element in a column, which accomplishes the same goal.&lt;/p&gt;

&lt;p&gt;The goal is to count how many Arrests occur for a given day-of-week and hour combination. In dplyr, we &lt;code&gt;group_by&lt;/code&gt; both &amp;ldquo;DayOfWeek&amp;rdquo; and &amp;ldquo;Hour&amp;rdquo;, and then use &lt;code&gt;summarize&lt;/code&gt; again.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_arrest_time &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_arrest &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Hour &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Time&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; get_hour&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;DayOfWeek&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; Hour&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    summarize&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; n&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/dow-table.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A few more tweaks are done (off camera) to convert the Hours to representations like &amp;ldquo;12 PM&amp;rdquo; and get everything in the correct order.&lt;/p&gt;

&lt;p&gt;Now, it&amp;rsquo;s time to make the heatmap using &lt;code&gt;ggplot2&lt;/code&gt;. Here&amp;rsquo;s the code, and I will explain what the new functions do:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df_arrest_time&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Hour&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; DayOfWeek&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; fill &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; count&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    geom_tile&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    fte_theme&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    theme&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;&amp;hellip;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Hour of Arrest (Local Time)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Day of Week of Arrest&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; title &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;# of Police Arrests in San Francisco from 2003 – 2015, by Time of Arrest&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    scale_fill_gradient&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;low &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; high &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;#27AE60&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; labels &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; comma&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;geom_title()&lt;/code&gt; creates tiles. (instead of lines)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;theme()&lt;/code&gt; is needed for a few additional theme tweaks to get the gradient bar to render (tweaks not shown)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scale_fill_gradient()&lt;/code&gt; tells the tiles to fill on a gradient, from white as the lowest value to a green as the highest value. The &amp;ldquo;labels = comma&amp;rdquo; parameter is a hidden helpful tip to allow any values in the legend to show with commas.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Putting it all together:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/sf-arrest-when-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The heatmap is an intuitive result. Arrests don&amp;rsquo;t happen in the early morning, and arrests tend to be elevated Friday and Saturday night, when everyone is out on the town.&lt;/p&gt;

&lt;p&gt;However, the peak arrest time is apparently on Wednesdays at 4-5 PM. Wednesdays and the 4-5 PM timeslot in general have elevated arrest frequency, too. Why is that the case?&lt;/p&gt;

&lt;p&gt;This requires further analysis.&lt;/p&gt;

&lt;h2&gt;Facets of Arrest&lt;/h2&gt;

&lt;p&gt;Perhaps the odd results can be explained by another lurking variable. Logically, certain types of crime, such as DUIs, should happen primarily at night. ggplot2 has tool known as faceting that makes such analysis easy by rendering a chart for each instance of another value in another variable. In this case, with only &lt;em&gt;one&lt;/em&gt; line of ggplot2 code, we can plot a heatmap for &lt;em&gt;each&lt;/em&gt; of the top types of arrests, and see if there is any significant variation in the heatmap.&lt;/p&gt;

&lt;p&gt;After quickly using dplyr to aggregate and sort the top categories of arrest, by number of occurrences:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_top_crimes &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_arrest &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Category&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    summarize&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; n&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    arrange&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;desc&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/top-crimes.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Other Offenses&amp;rdquo; is a catch-all, so we will ignore that. Filter on the top 18 types of crime excluding Other Offenses and aggregate as usual.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_arrest_time_crime &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_arrest &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Category &lt;span class=&quot;o&quot;&gt;%in%&lt;/span&gt; df_top_crimes&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;Category&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Hour &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Time&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; get_hour&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Category&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; DayOfWeek&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; Hour&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    summarize&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; n&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Time for the heat map! The &lt;code&gt;ggplot&lt;/code&gt; code is nearly identical to the previous heatmap code, except we add &lt;code&gt;facet_wrap()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df_arrest_time_crime&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Hour&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; DayOfWeek&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; fill &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; count&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    geom_tile&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    fte_theme&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    theme&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;&amp;hellip;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Hour of Arrest (Local Time)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Day of Week of Arrest&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; title &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;# of Police Arrests in San Francisco from 2003 – 2015, by Category and Time of Arrest&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    scale_fill_gradient&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;low &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; high &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;#2980B9&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    facet_wrap&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; Category&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; nrow &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/sf-arrest-when-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Easy visualization to make, but it&amp;rsquo;s not fully correct. There can only be one scale for the whole visualization, which is why the categories with lots of arrests appear colored and others do not (however, it shows that Drugs/Narcotics arrests are a large contributor to the Wednesday emphasis of the data). We need to normalize the counts by facet. dplyr has a nice trick for normalization: group by the normalization variable (Category), then mutate to add a column based on the aggregate for each unique value.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_arrest_time_crime &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_arrest_time_crime &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                            group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Category&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                            mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;norm &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; count&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/crime-norm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Setting the &amp;ldquo;fill&amp;rdquo; to &amp;ldquo;norm&amp;rdquo; and rerunning the heatmap code yields:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/sf-arrest-when-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now things get interesting.&lt;/p&gt;

&lt;p&gt;Prostitution has the most notably unique behavior, which high concentrations of arrests at night on weekdays. Drunkenness and DUIs have high concentrations at night on weekends. And Disorderly Conduct has a high concentration of arrests at 5 AM on weekdays? That&amp;rsquo;s not intuitive.&lt;/p&gt;

&lt;p&gt;Notably, some offenses have relatively random times of arrests, such as Stolen Property and Vehicle Theft.&lt;/p&gt;

&lt;p&gt;However, this doesn&amp;rsquo;t help explain why arrests tend to happen Wednesdays/4-5PM. Maybe faceting by another variable will provide more information.&lt;/p&gt;

&lt;p&gt;Perhaps Police district? Maybe some PDs in San Francisco are more zealous than others. Since we created a code workflow earlier, we can apply it to any other variable very easily; in this case, it&amp;rsquo;s mostly just replacing instances of &amp;ldquo;Category&amp;rdquo; with &amp;ldquo;PdDistrict.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Doing thus yields this heatmap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/sf-arrest-when-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Which isn&amp;rsquo;t helpful. The charts are mostly identical to each other, and to the original heatmap. (Central Station (&lt;a href=&quot;http://www.sf-police.org/Modules/ShowDocument.aspx?documentID=27554&quot;&gt;coverage map&lt;/a&gt;), however, has activity correlated to Drunkenness arrests.)&lt;/p&gt;

&lt;p&gt;Perhaps the frequency of arrests is correlated to the time of year? How about faceting by month?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/sf-arrest-when-6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nope. Zero difference.&lt;/p&gt;

&lt;p&gt;Last try. As shown in the line chart, the # of Arrests has oscillated over the years. Perhaps there&amp;rsquo;s a specific year that&amp;rsquo;s skewing the results. Let&amp;rsquo;s facet by Year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/sf-arrest-when-7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nope&lt;sup&gt;2&lt;/sup&gt;. 2010-2012 have elevated Wednesday activity, but not by much.&lt;/p&gt;

&lt;p&gt;This is frustrating. As of this posting, I don&amp;rsquo;t have an obvious answer for the elevated arrests Wednesdays at 4-5PM. That being said, there definitely is still more to learn from looking at SF Crime data, although that&amp;rsquo;s enough analysis for the time being.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://minimaxir.com/2015/12/sf-arrest-maps/&quot;&gt;My next article&lt;/a&gt; discusses how to plot arrests on a map using the &lt;code&gt;ggmap&lt;/code&gt; R library, which hopefully will provide more answers. The &lt;a href=&quot;https://github.com/minimaxir/sf-arrests-when-where&quot;&gt;GitHub repository&lt;/a&gt; contains a Jupyter notebook with code and visualizations for both for this article, and for the upcoming ggmap visualizations (if you want a sneak peek) which will show &lt;em&gt;where&lt;/em&gt; arrests in San Francisco frequently occur.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 04 Dec 2015 08:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2015/12/sf-arrests/</link>
        <guid isPermaLink="true">http://minimaxir.com/2015/12/sf-arrests/</guid>
        
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>How to Visualize New York City Using Taxi Location Data and ggplot2</title>
        <description>&lt;p&gt;A few months ago, I had &lt;a href=&quot;http://minimaxir.com/2015/08/nyc-map/&quot;&gt;posted a visualization&lt;/a&gt; of NYC Yellow Taxis using &lt;a href=&quot;http://ggplot2.org&quot;&gt;ggplot2&lt;/a&gt;, an extremely-popular R package by Hadley Wickham for data visualization. At the time, the code used for the chart was very messy since I was eager to create something cool after seeing the &lt;a href=&quot;https://news.ycombinator.com/item?id=10003118&quot;&gt;referenced Hacker News thread&lt;/a&gt;. Due to popular demand, I&amp;rsquo;ve cleaned up the code and have &lt;a href=&quot;https://github.com/minimaxir/nyc-taxi-notebook&quot;&gt;released it open source&lt;/a&gt;, with a few improvements.&lt;/p&gt;

&lt;p&gt;Here are some tips and tutorials on how to make such visualizations.&lt;/p&gt;

&lt;h2&gt;Getting the Data&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;As usual, a &lt;a href=&quot;https://github.com/minimaxir/nyc-taxi-notebook/blob/master/nyc_taxi_map.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt; containing the code and visualizations used in this article is available open-source on GitHub.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A quick summary of the previous post: I obtained the data from &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt;, which &lt;a href=&quot;http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml&quot;&gt;was uploaded&lt;/a&gt; from the official NYC Taxi &amp;amp; Limousine Commission datasets, plotted each taxi point as a tiny white dot on a fully-black map, and colorized the dots depending on the number of taxis at that location.&lt;/p&gt;

&lt;p&gt;In September, the &lt;a href=&quot;https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips&quot;&gt;BigQuery dataset&lt;/a&gt; was updated to include all data from January 2009 to June 2015: over 1.1 &lt;em&gt;billion&lt;/em&gt; Yellow Taxi rides recorded. Here&amp;rsquo;s an updated query, which additionally calculates the total non-tip revenue for a given location, since that might be useful later, and implements a &lt;a href=&quot;https://www.reddit.com/r/bigquery/comments/3fo9ao/nyc_taxi_trips_now_officially_shared_by_the_nyc/ctqfr8h&quot;&gt;sanity check filter&lt;/a&gt; noted by Felipe Hoffa.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ROUND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pickup_latitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ROUND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pickup_longitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_pickups&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fare_amount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_revenue&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nyc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tlc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trips&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fare_amount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trip_distance&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;long&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;The resulting dataset is 4 million rows and 116MB in size! This is well over the limit for downloading from the web BigQuery client, so you must use a local client (in the attached notebook, R), and it will still take about 10-15 minutes to download (as a result, I recommend caching the results locally). Relatedly, rendering 4 million points on a single plot on screen may be computationally intensive: I strongly recommend rendering the visualization to disk by instantiating a &lt;code&gt;png&lt;/code&gt; device or by using &lt;code&gt;ggsave&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a few results from that query.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/nyc-ggplot2-howto/test-data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, the second latitude/longitude combo is blatantly &lt;em&gt;wrong&lt;/em&gt;. This isn&amp;rsquo;t the first fidelity issue with the dataset, but we will address those in due time.&lt;/p&gt;

&lt;h2&gt;Plotting the Taxis&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s do a basic ggplot2 plot to test things out. All we need to do is plot a small point for every lat/long combination, and then save the resulting plot.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;long&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;lat&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            geom_point&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.06&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;png&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;nyc-taxi-1.png&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; w&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; h&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot
dev.off&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/nyc-ggplot2-howto/nyc-taxi-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;stupid data fidelity issues.&lt;/p&gt;

&lt;p&gt;This issue is fixed by constraining the plot to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Minimum_bounding_box&quot;&gt;bounding box&lt;/a&gt; of latitude and longitude coordinates corresponding to NYC. Flickr has &lt;a href=&quot;https://www.flickr.com/places/info/2459115&quot;&gt;a good starting point&lt;/a&gt; for a NYC bounding box; I took that and edited the limits more precisely using the &lt;a href=&quot;http://boundingbox.klokantech.com&quot;&gt;Bounding Box Tool&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;min_lat &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;40.5774&lt;/span&gt;
max_lat &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;40.9176&lt;/span&gt;
min_long &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-74.15&lt;/span&gt;
max_long &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-73.7004&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;You could also enforce the bounding box during the BigQuery. Now let&amp;rsquo;s implement the bounding box in the plot:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;long&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;lat&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            geom_point&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.06&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_x_continuous&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;limits&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;min_long&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; max_long&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_y_continuous&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;limits&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;min_lat&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; max_lat&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;png&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;nyc-taxi-2.png&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; w&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; h&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot
dev.off&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/nyc-ggplot2-howto/nyc-taxi-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Much, much better! Now that the visualization generally looks like what we want it to be, we can start theming.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start small and do just a few tweaks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Filter the data slightly to reduce some erroneous points.&lt;/li&gt;
&lt;li&gt;The theme must be primarily a black background, with most of the ggplot2 theme attributes stripped out and the margins nullified. (implemented as &lt;code&gt;theme_map_dark()&lt;/code&gt;; code is in the notebook)&lt;/li&gt;
&lt;li&gt;Set the resolution of the rendering device to 300 DPI; this reduces some of the aliasing in the resulting image.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;num_pickups &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;long&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;lat&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            geom_point&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.06&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_x_continuous&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;limits&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;min_long&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; max_long&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_y_continuous&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;limits&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;min_lat&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; max_lat&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            theme_map_dark&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;png&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;nyc-taxi-3.png&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; w&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; h&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; res&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot
dev.off&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/nyc-ggplot2-howto/nyc-taxi-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Right on track! Now time to make things more professional.&lt;/p&gt;

&lt;p&gt;This requires the implementation of a few more aesthetics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Add a gradient color based on intensity of the number of pickups: since the number of pickups will logically be near streets, the coloring will be more intense near streets. Exact color doesn&amp;rsquo;t matter; I used the purple Wisteria from &lt;a href=&quot;https://flatuicolors.com&quot;&gt;Flat UI Colors&lt;/a&gt; to represent maximum intensity. Additionally, the scale should be logarithmic to make the colors stand out. (Another approach is to scale the transparency of the points instead, which is the approach &lt;a href=&quot;http://www.brianrlance.com/blog/2015/8/7/nyc-visualized-via-taxi-pickup-locations&quot;&gt;Brian Lance had done&lt;/a&gt; and that works well too)&lt;/li&gt;
&lt;li&gt;Annotate the theme with a proper title (and remove the scale legend; since the exact values on specific points will not be helpful)&lt;/li&gt;
&lt;li&gt;Force the plot to obey the dimension ratio with &lt;code&gt;coord_equal()&lt;/code&gt;, otherwise the map will stretch and distort to fill the entirety of the plotting area. (you can see a vertical stretch effect with the previous image)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Putting it all together:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;num_pickups &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;long&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;lat&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;num_pickups&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            geom_point&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.06&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_x_continuous&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;limits&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;min_long&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; max_long&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_y_continuous&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;limits&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;min_lat&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; max_lat&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            theme_map_dark&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_color_gradient&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;low&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;#CCCCCC&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; high&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;#8E44AD&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; trans&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;title &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Map of NYC, Plotted Using Locations Of All Yellow Taxi Pickups&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            theme&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;legend.position&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            coord_equal&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;png&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;nyc-taxi-4.png&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; w&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; h&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; res&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot
dev.off&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;results in this image, which is what we want! However there is a slight problem, and I will wrap the image in a red border to demonstrate.&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;style&gt;
.border img {
  border: 3px solid red;
}
&lt;/style&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;border&quot;&gt;&lt;img src=&quot;/img/nyc-ggplot2-howto/nyc-taxi-4.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Due to &lt;code&gt;coord_equal()&lt;/code&gt; enforcing the chart dimensions, the rendering device has a gap of white space at the top due to interaction with the &lt;code&gt;grid&lt;/code&gt; graphics package that ggplot2 is based upon; normally not a problem for default charts, but a waste of space for visualizations with non-white backgrounds.&lt;/p&gt;

&lt;p&gt;I attempted to fix this issue by forcing &lt;code&gt;grid&lt;/code&gt; to render a black rectangle then plot on top of it. Unfornately, that was not successful. The quickest workaround is to set the image dimensions through trial-and-error such that the issue is minimized.&lt;/p&gt;

&lt;p&gt;All things considered, that&amp;rsquo;s minor but should still be noted. The streets of Manhattan are visible! And there&amp;rsquo;s still more that can be done.&lt;/p&gt;

&lt;h2&gt;Hexing the Revenue&lt;/h2&gt;

&lt;p&gt;Hex map overlays are a popular technique for aggregating two-dimensional data on a 3rd dimension. ggplot2 has a relatively new &lt;a href=&quot;http://docs.ggplot2.org/current/stat_summary_hex.html&quot;&gt;stat_summary_hex&lt;/a&gt; function which does just that.&lt;/p&gt;

&lt;p&gt;Why not aggregate total revenue for NYC Yellow Taxi Pickups to determine where taxis generate the most money? Since we conveniently have the code to generate a map of NYC already, we can plot the hex bins on top of that map, after a few more tweaks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set the 3rd dimension, &lt;code&gt;z&lt;/code&gt;, to &lt;code&gt;total_revenue&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Set the aggregation to the &lt;code&gt;sum&lt;/code&gt; function, so it sums up all the revenues within a bin.&lt;/li&gt;
&lt;li&gt;Scale the total hex revenues with a gradient.&lt;/li&gt;
&lt;li&gt;Tweak all the aesthetics: color of the base points, the color of the hexes, the transparency of the hexes, and the name of the chart.&lt;/li&gt;
&lt;li&gt;Set the chart dimensions to avoid the &lt;code&gt;coord_equal()&lt;/code&gt; issue mention above.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;num_pickups &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;long&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;lat&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; z&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;total_revenue&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            geom_point&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.06&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;#999999&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            stat_summary_hex&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;fun &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; bins&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; alpha&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_x_continuous&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;limits&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;min_long&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; max_long&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_y_continuous&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;limits&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;min_lat&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; max_lat&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            theme_map_dark&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_fill_gradient&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;low&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;#CCCCCC&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; high&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;#27AE60&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; labels&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dollar&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;title &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Total Revenue for NYC Yellow Taxis by Pickup Location, from Jan 2009 ― June 2015&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            coord_equal&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;png&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;nyc-taxi-5.png&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; w&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;950&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; h&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;860&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; res&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot
dev.off&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/nyc-ggplot2-howto/nyc-taxi-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That wasn&amp;rsquo;t too bad. The gradient shows that &lt;a href=&quot;https://www.google.com/maps/place/penn+station+nyc/@40.750568,-73.993519,15z&quot;&gt;Penn Station&lt;/a&gt; in Manhattan, along with the two airports, are the largest revenue generators.&lt;/p&gt;

&lt;p&gt;I &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/comments/3sjmc0/total_revenue_for_nyc_yellow_taxis_by_pickup/&quot;&gt;posted the hex-overlayed map&lt;/a&gt; on Reddit to /r/dataisbeautiful as a part of my data visualization beta-testing. Although the chart received just under 200 upvotes, the comments in the Reddit thread were &lt;em&gt;unanimously negative&lt;/em&gt;. Reddit user /u/DanHeidel &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/comments/3sjmc0/total_revenue_for_nyc_yellow_taxis_by_pickup/cwxuy1e&quot;&gt;posted a long rant&lt;/a&gt; on the problems with the aesthetics of the chart. And for the most part, I agree with his assessment.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try again, and address the claims made in the Reddit comments.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Only show hex bins where there is enough valid data, which should remove the mysterious hexes over the water. This can be implemented through a helper aggregate function which does not render the hex if the total revenue of the hex is under some threshold value. (I set it to $100,000)&lt;/li&gt;
&lt;li&gt;Scale the total hex revenue logarithmically, and change the color to a Red hue (Alizarin) to make the step values more visible.&lt;/li&gt;
&lt;li&gt;Zoom the chart dimensions closer to Manhattan.&lt;/li&gt;
&lt;li&gt;Make a few more aesthetic tweaks.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Here&amp;rsquo;s take two:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;total_rev &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; threshold &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; threshold&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;))}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;plot &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;num_pickups &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;long&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;lat&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; z&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;total_revenue&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            geom_point&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.06&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;#999999&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            stat_summary_hex&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;fun &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; total_rev&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; bins&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; alpha&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_x_continuous&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;limits&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-74.0224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-73.8521&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_y_continuous&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;limits&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;40.6959&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;40.8348&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            theme_map_dark&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            scale_fill_gradient&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;low&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;#FFFFFF&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; high&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;#E74C3C&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; labels&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dollar&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; trans&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; breaks&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            labs&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;title &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Total Revenue for NYC Yellow Taxis by Pickup Location, from Jan 2009 ― June 2015&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            coord_equal&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;png&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;nyc-taxi-6.png&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; w&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;900&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; h&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;900&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; res&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot
dev.off&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/nyc-ggplot2-howto/nyc-taxi-6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A good step forward. Revenue all through Manhattan is mostly the same except for Penn Station. Meanwhile, the hexes in LaGuardia Airport are noticeably more saturated than Penn Station.&lt;/p&gt;

&lt;p&gt;Hopefully, this tutorial gave you a good look into a few interesting tricks that can be accomplished with ggplot2, even though the code can be somewhat messy. If you want more orthodox methods of plotting geographic data in ggplot2, you should look into the &lt;a href=&quot;https://cran.r-project.org/web/packages/ggmap/index.html&quot;&gt;ggmap&lt;/a&gt; R package, which I used to plot &lt;a href=&quot;http://minimaxir.com/2014/04/san-francisco/&quot;&gt;Facebook Checkin data in San Francisco&lt;/a&gt;, and look into the &lt;a href=&quot;https://cran.r-project.org/web/packages/maps/index.html&quot;&gt;maps&lt;/a&gt; R package plus shape files, which I used to plot &lt;a href=&quot;http://minimaxir.com/2015/01/tree-time/&quot;&gt;Instagram photo location data&lt;/a&gt;. Unfortunately, the code may not necessarily be less messy.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Nov 2015 08:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2015/11/nyc-ggplot2-howto/</link>
        <guid isPermaLink="true">http://minimaxir.com/2015/11/nyc-ggplot2-howto/</guid>
        
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Quantifying and Visualizing the Reddit Hivemind</title>
        <description>&lt;p&gt;In my &lt;a href=&quot;http://minimaxir.com/2015/10/reddit-bigquery/&quot;&gt;last post on Reddit data&lt;/a&gt; (I strongly suggest you read that first if you haven&amp;rsquo;t already), I noted that analyzing the words used in Reddit submissions may be useful in quantifying the relationship of those keywords in the success of a Reddit submission. Indeed, if we can find out which topics Reddit users tend to upvote, we can identify what keywords are most attractive to the Reddit &amp;ldquo;hivemind.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;First, I gathered some preliminary statistics about all submissions to the top 500 subreddits on Reddit, again using the &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt; data dump compiled by Jason Baumgartner and Felipe Hoffa, in order to establish a good base for the analysis:&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_submissions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ROUND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;AVG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;NTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower_95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;NTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;NTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;975&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upper_95&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reddit_posts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_corpus_201509&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_submissions&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;LIMIT&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Which results in &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1lyEc7-5vkREKBV8fUreyOszICgyA0CqX2YHaK2-4ndo/edit?usp=sharing&quot;&gt;this output&lt;/a&gt;. The &amp;ldquo;lower_95&amp;rdquo; and and the &amp;ldquo;upper_95&amp;rdquo; columns represent the 2.5% and the 97.5% percentile respectively, which could be used to form a 95% confidence interval around the median. For example, &lt;a href=&quot;http://reddit.com/r/funny&quot;&gt;/r/funny&lt;/a&gt; has a 2.5% percentile of 0 points, a median of 1 point, and a 97.5% percentile of &lt;em&gt;875 points&lt;/em&gt;. However, from those values, it is clear the data is heavily skewed right, which makes running statistical tests more difficult.&lt;/p&gt;

&lt;p&gt;Now we can query the top keywords for each subreddit, whose presence in a submission is related to the highest average score within that subreddit. This requires a very intricate and optimized BigQuery. Specifically, we want the query to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Get the Top 500 subreddits, by number of submissions (an abridged verson of query above).&lt;/li&gt;
&lt;li&gt;Get all submissions from these subreddits.&lt;/li&gt;
&lt;li&gt;Extract the keywords from all of these submissions, using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Regular_expression&quot;&gt;regular expression&lt;/a&gt; to remove most punctuation (unfortunately, the regular expression will remove punctuation &lt;em&gt;within&lt;/em&gt; words as well, resulting in some odd &amp;ldquo;words&amp;rdquo; in the final results) and flattening the resulting tokens into separate rows for aggregation.&lt;/li&gt;
&lt;li&gt;Aggregate the keywords by both subreddit and the word itself, and obtain the # of distinct submissions the word is present in, the average score among all submissions, etc.&lt;/li&gt;
&lt;li&gt;Keep only words which occur in atleast 1,000 distinct submissions, which is important for getting a good average.&lt;/li&gt;
&lt;li&gt;For each subreddit, rank each remaining word by their average score, descending.&lt;/li&gt;
&lt;li&gt;Keep only the Top 20 words for each subreddit. 500 subreddits x 20 words = 10,000 rows maximum, which the limit BigQuery allows for web download, so that works out well.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower_95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upper_95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ROW_NUMBER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OVER&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_score&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_rank&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;DISTINCT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ROUND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;AVG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower_95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;975&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upper_95&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FLATTEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SPLIT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;LOWER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;REGEXP_REPLACE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;[^\w&amp;amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;, &amp;#39;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;)), &amp;#39;&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reddit_posts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_corpus_201509&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reddit_posts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_corpus_201509&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LIMIT&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;EACH&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;HAVING&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_rank&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_score&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Phew! That query results in &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1MRfLR_TBO8zveKaifqVXTN0da7tf0FQOr5a2LVkZgkg/edit?usp=sharing&quot;&gt;this output&lt;/a&gt;, which we can use to plot a bar chart for each subreddit.&lt;/p&gt;

&lt;h2&gt;Plotting the Words&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;The R code used to generate the charts is available in &lt;a href=&quot;https://github.com/minimaxir/reddit-subreddit-keywords/blob/master/reddit_subreddit_words.ipynb&quot;&gt;this Jupyter notebook&lt;/a&gt;, open-sourced on GitHub. Additionally, all 500 charts with the keyword ranks for all 500 subreddits are available in &lt;a href=&quot;https://github.com/minimaxir/reddit-subreddit-keywords/tree/master/subreddit-mean&quot;&gt;the parent repository&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;After a few R tricks, I managed to chart the Top 10 keywords for each of the Top 15 subreddits:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://i.imgur.com/dWdCnMI.png&quot;&gt; &lt;img src=&quot;/img/reddit-topwords/subreddit-means-half.png&quot; alt=&quot;&quot; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Click on image for full-resolution)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As noted in the subtitle, each word appears in atleast 1,000 submissions by subreddit (which absorbs any messy outliers), and vertical line represents the true average upvotes per subreddit. One might argue that the median would be a better statistic instead of the median, due to the high skewness of the data. Thanks to the power of BigQuery, I was able to calculate the &lt;a href=&quot;http://i.imgur.com/0PBolIq.png&quot;&gt;top medians for each of the subreddits&lt;/a&gt; with a slightly-tweaked query, but the chart is not as helpful. There are still a few useful implications of the median, though, which I&amp;rsquo;ll show later. (No, I don&amp;rsquo;t need to normalize the data in the chart since I am not making an apples-to-apples comparison between the values of the words among subreddits, and no, I don&amp;rsquo;t need to remove stop words since this is visualizing an average, and not a count.)&lt;/p&gt;

&lt;p&gt;When the visualization was &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/comments/3nz3zz/average_number_of_upvotes_for_reddit_submissions/&quot;&gt;posted to Reddit&lt;/a&gt; in the &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/&quot;&gt;/r/dataisbeautiful&lt;/a&gt; subreddit, it received over 3,500 upvotes. As many commenters on that submission correctly note, there are a few especially interesting observations for the keywords in those 15 subreddits, and when looking at the remaining subreddits, many trends in their keywords are made apparent.&lt;/p&gt;

&lt;h2&gt;Politicalreddit&lt;/h2&gt;

&lt;p&gt;The most notable trend with the Top 15 subreddits is in the &lt;a href=&quot;https://www.reddit.com/r/politics/&quot;&gt;/r/politics&lt;/a&gt; subreddit, which tells users to &amp;ldquo;Vote based on quality, not opinion,&amp;rdquo; but has a high affiliation for submissions specifically involving Bernie Sanders and Elizabeth Warren.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-010-politics.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/technology/&quot;&gt;/r/technology&lt;/a&gt;, intended to be about &amp;ldquo;a broad spectrum of conversation as to the innovations, aspirations, applications and machinations,&amp;rdquo; has a high affiliation for political issues such as the net neutrality controversy involving the FCC, ISPs, and Comcast, and little affiliation for actual &lt;em&gt;technology&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-018-technology.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Outside the Top 15 Subreddits, the political affiliations of subreddits such as &lt;a href=&quot;http://reddit.com/r/Libertarian&quot;&gt;/r/Libertarian&lt;/a&gt; are less surprising and more funny as a result.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-103-Libertarian.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Storyreddit&lt;/h2&gt;

&lt;p&gt;One of the rising trends in the online publication landscape is that telling a story is more effective at generating attention. (thank BuzzFeed for that)&lt;/p&gt;

&lt;p&gt;The biggest offender is &lt;a href=&quot;https://www.reddit.com/r/aww/&quot;&gt;/r/aww&lt;/a&gt;, a subreddit about animals, has a high affinity for words &lt;em&gt;that aren&amp;rsquo;t animals&lt;/em&gt;, with only two animal words appearing in the Top 20.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-011-aww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The presence of story titles is also apparent in &lt;a href=&quot;https://www.reddit.com/r/Fitness/&quot;&gt;/r/Fitness&lt;/a&gt;, which in fairness, the atmosphere of self-improvement lends itself more to stories.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-054-Fitness.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Metareddit&lt;/h2&gt;

&lt;p&gt;The practice of upvoting submissions just because they mention a particular topic is colloquially known as &amp;ldquo;circlejerking.&amp;rdquo; &lt;a href=&quot;https://www.reddit.com/r/circlejerk/&quot;&gt;/r/circlejerk&lt;/a&gt; fits that well, with titles designed to satirize clickbaity issues in order to receive upvotes ironically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-025-circlejerk.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/Braveryjerk/&quot;&gt;/r/Braveryjerk&lt;/a&gt;, however, takes this practice to its logical conclusion.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-486-Braveryjerk.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Esotericreddit&lt;/h2&gt;

&lt;p&gt;Here are a few other infamous subreddits that people have requested.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/conspiracy/&quot;&gt;/r/conspiracy&lt;/a&gt;, which apparently have frequent instances of &amp;ldquo;TIL&amp;rdquo; (Today, I Learned) in titles, are simultaneously very &lt;em&gt;suspicious&lt;/em&gt; of the actions in the  &lt;a href=&quot;https://www.reddit.com/r/TIL/&quot;&gt;/r/TIL&lt;/a&gt; subreddit.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-064-conspiracy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The keywords in &lt;a href=&quot;https://www.reddit.com/r/teenagers/&quot;&gt;/r/teenagers&lt;/a&gt; captures the essence of modern social media.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-072-teenagers.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Some subreddits are just plain weird.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-175-me_irl.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s &lt;a href=&quot;https://www.reddit.com/r/me_irl/&quot;&gt;/r/me_irl&lt;/a&gt; for you.&lt;/p&gt;

&lt;h2&gt;Looking at the Medians&lt;/h2&gt;

&lt;p&gt;As shown in the linked median image earlier, the medians for most keywords are 1 or 2, which does not provide much visual information whether a keyword is more important than another.&lt;/p&gt;

&lt;p&gt;There are notable exceptions, however, such as with &lt;a href=&quot;https://www.reddit.com/r/Android/&quot;&gt;/r/Android&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/median-059-Android.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That subreddit has official threads for events; it would make sense for users to upvote that whenever it appears as it&amp;rsquo;s &lt;em&gt;important&lt;/em&gt; that it&amp;rsquo;s visible, but not necessarily as an agreement of the topic. That&amp;rsquo;s why I believe looking at the medians is a different approach than looking at the means.&lt;/p&gt;

&lt;p&gt;Same thing with &lt;a href=&quot;https://www.reddit.com/r/relationships/&quot;&gt;/r/relationships&lt;/a&gt;, where an &amp;ldquo;update&amp;rdquo; is important.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/median-057-relationships.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All in all, this is still just a first step for analyzing the importance of keywords in Reddit submission, and the impact of Reddit&amp;rsquo;s hivemind. The next step would be NLP techniques such as POS tagging and TDF-IF, but those require very significant and very expensive computing power. At the least, the good results with these simple analyses validates the idea for further research.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Again the R code used to generate the charts is available in &lt;a href=&quot;https://github.com/minimaxir/reddit-subreddit-keywords/blob/master/reddit_subreddit_words.ipynb&quot;&gt;this Jupyter notebook&lt;/a&gt;, open-sourced on GitHub. Additionally, all 500 charts with the keyword ranks for all 500 subreddits are available in &lt;a href=&quot;https://github.com/minimaxir/reddit-subreddit-keywords/tree/master/subreddit-mean&quot;&gt;the parent repository&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Oct 2015 08:00:00 -0700</pubDate>
        <link>http://minimaxir.com/2015/10/reddit-topwords/</link>
        <guid isPermaLink="true">http://minimaxir.com/2015/10/reddit-topwords/</guid>
        
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>How to Analyze Every Reddit Submission and Comment, in Seconds, for Free</title>
        <description>&lt;p&gt;While working on my &lt;a href=&quot;http://minimaxir.com/2014/12/reddit-statistics/&quot;&gt;statistical analysis of 142 million Reddit submissions&lt;/a&gt; last year, I had a surprising amount of trouble settings things up. It took a few hours to download the 40+ gigabytes of compressed data, and another few hours to parse the data and store in a local database. Even then, on my old-but-still-pretty-fast desktop PC, simple queries on the entire dataset took minutes to run, with complex queries occasionally taking upwards to an hour.&lt;/p&gt;

&lt;p&gt;Over the past year, I&amp;rsquo;ve had a lot of success using &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;Google&amp;rsquo;s BigQuery&lt;/a&gt; tool for quick big data analysis without having to manage the data, such as the &lt;a href=&quot;http://minimaxir.com/2015/08/nyc-map/&quot;&gt;recent NYC Taxi data dump&lt;/a&gt;. Recently, Jason Michael Baumgartner of &lt;a href=&quot;https://pushshift.io&quot;&gt;Pushshift.io&lt;/a&gt; (a.k.a &lt;a href=&quot;https://www.reddit.com/user/Stuck_In_the_Matrix&quot;&gt;/u/Stuck_In_The_Matrix&lt;/a&gt; on Reddit), who also provided me the original Reddit data, released &lt;a href=&quot;https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/&quot;&gt;new Reddit datasets&lt;/a&gt; containing all submissions and all comments until August 2015. Google BigQuery Developer Advocate Felipe Hoffa &lt;a href=&quot;https://www.reddit.com/r/bigquery/comments/3mv82i/dataset_reddits_full_post_history_shared_on/&quot;&gt;uploaded the dataset&lt;/a&gt; to a public table in BigQuery for anyone to perform analysis on the data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-bigquery/bigquery-tool.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With Reddit data in BigQuery, quantifying all the hundreds of millions of Reddit submissions and comments is trivial.&lt;/p&gt;

&lt;h2&gt;Hello Reddit!&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Although the data is retrieved and processed in seconds, making the data visualizations in this post takes slightly longer. You can view the R code needed to reproduce the visualizations in &lt;a href=&quot;https://github.com/minimaxir/reddit-bigquery/blob/master/reddit_bigquery.ipynb&quot;&gt;this Jupyter notebook&lt;/a&gt; open-sourced on GitHub.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;BigQuery allows 1 terabyte (1000 GB) of &lt;a href=&quot;https://cloud.google.com/bigquery/pricing&quot;&gt;free data processing&lt;/a&gt; per month; which is much more than it sounds like, and you&amp;rsquo;ll see why.&lt;/p&gt;

&lt;p&gt;BigQuery syntax works similar to typical SQL. If you can perform basic SQL aggregations such as &lt;code&gt;COUNT&lt;/code&gt; and &lt;code&gt;AVG&lt;/code&gt; on a tiny database, you can perform the same aggregations on a 100+ GB dataset.&lt;/p&gt;

&lt;p&gt;We can write a simple query to just calculate how many Reddit submissions are made each day, to both check the robustness of the data, and to show how much Reddit has grown. (note that the &lt;code&gt;created&lt;/code&gt; field is in seconds UTC; you will need to convert it to a timestamp, then convert to a &lt;code&gt;DATE&lt;/code&gt; which converts the timestamp to a YYYY-MM-DD string, which is the format you want since it sorts lexigraphically)&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEC_TO_TIMESTAMP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;created&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date_submission&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_submissions&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reddit_posts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_corpus_201509&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date_submission&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date_submission&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Which results in a simple timeseries, as shown above. BigQuery allows you to download the data as a CSV, and it can easily be visualized in any statistical program such as Excel.&lt;/p&gt;

&lt;p&gt;Of course, I prefer R and ggplot2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-bigquery/reddit-bigquery-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And that query only took 4.5 seconds to complete, with 1.46 GB data processed! BigQuery only counts the columns used in the query against the 1 TB quota; since the query only used one small column, the query is cheap. (if the query hits the raw submission title/comment text data, then the queries will be significantly larger data-wise)&lt;/p&gt;

&lt;h2&gt;When is the best time to post to Reddit?&lt;/h2&gt;

&lt;p&gt;One of the reasons people might look at Reddit data is for determining the best way to viral. One of the most important factors in making something go viral is timing; since Reddit has a ranking system based on both community approval and time-since-submission, along with the fact that there is more activity at certain times of the day, the time when a submission is made is especially important.&lt;/p&gt;

&lt;p&gt;So here&amp;rsquo;s another aggregation query that aggregates on the day of week a submission is made, and the hour when the submission is made; this creates a 7x24 matrix of timeslot possibilities. Both values are set to Eastern Standard Time, as U.S.-target websites tend to follow that timezone. Lastly, instead of checking the average amount of points for each submission at each timeslot (which would be skewed by the very high proportion of submissions with no upvotes), we&amp;rsquo;ll look at how many submissions go viral (&gt;3000) at each timeslot with a conditional &lt;code&gt;SUM(IF())&lt;/code&gt; statement (which is equivalent to Excel&amp;rsquo;s &lt;code&gt;COUNTIF&lt;/code&gt; conditional)&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;DAYOFWEEK&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEC_TO_TIMESTAMP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;created&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dayofweek&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;HOUR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEC_TO_TIMESTAMP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;created&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_hour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;SUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_gte_3000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reddit_posts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_corpus_201509&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dayofweek&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_hour&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dayofweek&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_hour&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;2.5 seconds, 2.39 GB processed, and a few R tricks results in this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-bigquery/reddit-bigquery-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The day-of-week mostly does not matter, but hour matters significantly: about 3x as many submissions go viral when they are posted in the morning EST than if they are posted late in the day. (and rarely anything comparatively goes viral posted late at night, which is intuitive enough)&lt;/p&gt;

&lt;h2&gt;Creating Wordclouds of Subreddit Comments&lt;/h2&gt;

&lt;p&gt;Wordclouds are always a fun representation of data, although not necessarily the most quantifiable. BigQuery can help derive word counts on large quantities of data, although the query is much more complex.&lt;/p&gt;

&lt;p&gt;Due to the amount of data, we&amp;rsquo;ll only look at the latest Reddit comment data (August 2015), and we&amp;rsquo;ll look at the &lt;a href=&quot;https://www.reddit.com/r/news&quot;&gt;/r/news&lt;/a&gt; subreddit to see if there are any linguistic trends. In a subquery, the comment text (from non-bots!) is stripped of punctuation, set to lower case, and split into individual words; each word is counted and aggregated.&lt;/p&gt;

&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AVG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_score&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FLATTEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SPLIT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;LOWER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;REGEXP_REPLACE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;[.&amp;amp;quot;,&lt;/em&gt;:()[]/|\n]&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reddit_comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;author&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;author&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reddit_comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bots_201505&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;&amp;quot;news&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;EACH&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;HAVING&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_words&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;5.0 seconds and 11.3 GB processed, along with removing a few stopwords via R results in this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-bigquery/reddit-bigquery-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;News is about &lt;strong&gt;people&lt;/strong&gt;. And &lt;strong&gt;more&lt;/strong&gt;. Makes sense. (An &lt;code&gt;avg_score&lt;/code&gt; column for each word is included to allow for emulation of quantifiable impact of a given word, as used in my &lt;a href=&quot;http://minimaxir.com/2015/01/linkbait/&quot;&gt;BuzzFeed analysis&lt;/a&gt;, although that is less useful for comments than submissions.)&lt;/p&gt;

&lt;h2&gt;Monthly Active Users&lt;/h2&gt;

&lt;p&gt;Although Reddit does provide a count of Subscribers for a given subreddit, most of those users are passive. One of the most important metrics of any startup is Monthly Active Users (MAUs), which we can get a reasonable approximation using the comment data. And not only that, we can aggregate the unique number of commenters by a given subreddit and by a given month, to see how subreddit activity changes over time relative to other subreddits.&lt;/p&gt;

&lt;p&gt;How this works in BigQuery is a little more complicated, and requires the use of window functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Aggregate by subreddit, month, and count of unique comment authors on &lt;em&gt;all&lt;/em&gt; comments (this will result in a query with a lot of data processed!)&lt;/li&gt;
&lt;li&gt;For each month, rank the subreddits by number of unique authors.&lt;/li&gt;
&lt;li&gt;Take the top 20 subreddits by rank for each given month.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_authors&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_authors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ROW_NUMBER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OVER&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_authors&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LEFT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEC_TO_TIMESTAMP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;created_utc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;UNIQUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_authors&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TABLE_QUERY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reddit_comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;&amp;quot;table_id CONTAINS &amp;#39;20&amp;#39; AND LENGTH(table_id)&amp;lt;8&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;EACH&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ASC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_authors&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;11.9 seconds and 53.3 GB (!) later, we can create a Top 20 Subreddits visualization for each month, and combine each image into a GIF with my trusty &lt;a href=&quot;https://github.com/minimaxir/frames-to-gif-osx&quot;&gt;Convert Frames to GIF&lt;/a&gt; tool.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-bigquery/subreddit-ranks.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can view the individual frames &lt;a href=&quot;https://github.com/minimaxir/reddit-bigquery/tree/master/subreddit-ranks&quot;&gt;in the project GitHub repository&lt;/a&gt;. There are many trends revealed, such how some subreddits die over time (&lt;a href=&quot;https://www.reddit.com/r/fffffffuuuuuuuuuuuu&quot;&gt;r/fffffffuuuuuuuuuuuu&lt;/a&gt;, &lt;a href=&quot;https://www.reddit.com/r/technology&quot;&gt;/r/technology&lt;/a&gt;), how some rise over time (&lt;a href=&quot;https://www.reddit.com/r/pcmasterrace&quot;&gt;/r/pcmasterrace&lt;/a&gt;), and how some subreddits have relative monthly spikes (&lt;a href=&quot;https://www.reddit.com/r/thebutton&quot;&gt;/r/thebutton&lt;/a&gt;). Note that the colors have no visual meaning but are used to help easily differentiate between subreddits.&lt;/p&gt;

&lt;p&gt;These sample queries are only a small sample of what can be done with the Reddit data and BigQuery. Although some data scientists may argue that using is BigQuery is pointless since ~200GB of data &lt;a href=&quot;http://yourdatafitsinram.com&quot;&gt;can fit in RAM&lt;/a&gt;, the quick, dirty, and &lt;em&gt;cheap&lt;/em&gt; option is much more pragmatic for the majority of potential data analysis on this Reddit dataset.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Again, you can view the R code needed to reproduce the visualizations in &lt;a href=&quot;https://github.com/minimaxir/reddit-bigquery/blob/master/reddit_bigquery.ipynb&quot;&gt;this Jupyter notebook&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Oct 2015 08:00:00 -0700</pubDate>
        <link>http://minimaxir.com/2015/10/reddit-bigquery/</link>
        <guid isPermaLink="true">http://minimaxir.com/2015/10/reddit-bigquery/</guid>
        
        
        <category>Data</category>
        
      </item>
    
  </channel>
</rss>
