<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>minimaxir | Max Woolf&#39;s Blog</title>
    <description>A blog by Max Woolf about startups, technology, and blogging. It&#39;s so meta, even this acronym.</description>
    <link>http://minimaxir.com/</link>
    <atom:link href="http://minimaxir.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 20 Jun 2016 08:22:10 -0700</pubDate>
    <lastBuildDate>Mon, 20 Jun 2016 08:22:10 -0700</lastBuildDate>
    <generator>Jekyll v3.1.2</generator>
    
      <item>
        <title>Methods for Finding Related Reddit Subreddits with Simple Set Theory</title>
        <description>&lt;p&gt;I recently &lt;a href=&quot;http://minimaxir.com/2016/05/reddit-graph/&quot;&gt;wrote a post&lt;/a&gt; on how to visualize &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;&gt;network graphs&lt;/a&gt; of &lt;a href=&quot;https://www.reddit.com&quot;&gt;Reddit&lt;/a&gt; subreddits.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/group-001.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;One of the reasons I&amp;rsquo;ve been researching the topic is to find a good way to facilitate discovery of lesser-known subreddits, as Reddit is doing a terrible job at it (although they have been trying a &lt;a href=&quot;https://www.reddit.com/r/changelog/comments/4o4qjh/more_small_tests_to_improve_user_experience_live/d49leyu?context=2&quot;&gt;few new experiments&lt;/a&gt; &lt;em&gt;very recently&lt;/em&gt;). As it turns out, invoking graph theory is overkill. Even fancy machine learning approaches like &lt;a href=&quot;https://en.wikipedia.org/wiki/Collaborative_filtering&quot;&gt;collaborative filtering&lt;/a&gt;, while powerful, may not be required to help Redditors discover new things.&lt;/p&gt;

&lt;h2&gt;Finding Related Subreddits&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s say we have two sets: Set &lt;em&gt;A&lt;/em&gt;, where &lt;em&gt;A&lt;/em&gt; represents the number of active users in a given subreddit, and set &lt;em&gt;B&lt;/em&gt;, where &lt;em&gt;B&lt;/em&gt; is the set of active users in a subreddit. The intersection of Sets &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; (A ∩ B) represents users who are active in &lt;em&gt;both&lt;/em&gt; subreddits.&lt;/p&gt;

&lt;p&gt;Using &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt;, I can get the comment data from &lt;strong&gt;ALL&lt;/strong&gt; public Reddit subreddits, as otherwise this technique would not work well using any smaller subset. The network graph edgelist conveniently gives (A ∩ B), obtained &lt;a href=&quot;http://minimaxir.com/2016/05/reddit-graph/&quot;&gt;as described in my previous post&lt;/a&gt;, which calculates the number of active users for all pairs of subreddits (defining &amp;ldquo;active users&amp;rdquo; as users who have made a comment in at least 5 unique threads in a given subreddit within the past 6 months).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/active-edge.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;In this case, we can filter the edgelist to only allow intersections where there are at least 10 active users; this prevents including dead and personal subreddits.&lt;/p&gt;

&lt;p&gt;We can run another similar query to get the number of active users for each subreddit.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/active-users.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;After that, for a given subreddit &lt;em&gt;A&lt;/em&gt;, find:&lt;/p&gt;

&lt;p&gt;(A ∩ B) / (B)&lt;/p&gt;

&lt;p&gt;for all subreddits &lt;em&gt;B&lt;/em&gt; where (A ∩ B) &amp;gt; 0 (i.e. only neighbors of &lt;em&gt;A&lt;/em&gt;). This computation takes less than a second. Additionally, the output is always a percentage between 0% and 100%. For the visualizations, we plot the Top 15 subreddits with the highest overlap of the specified subreddit &lt;em&gt;A&lt;/em&gt; (and color the bars with a nice &lt;a href=&quot;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&quot;&gt;viridis palette&lt;/a&gt; to provide another easy way to perceive relative magnitude of relatedness).&lt;/p&gt;

&lt;p&gt;The methodology may sound arbitrary, but the results are very interesting. Here&amp;rsquo;s a chart of the top related subreddits for &lt;a href=&quot;https://www.reddit.com/r/aww&quot;&gt;/r/aww&lt;/a&gt;, one of the most popular places on the internet for cat pictures.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/aww-related.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;I have honestly &lt;em&gt;never&lt;/em&gt; heard of any of these subreddits before. But yet, by analyzing public user activity alone, I found a few new places to get more cute pics.&lt;/p&gt;

&lt;p&gt;This methodology is excellent for finding subreddit-specific subsubreddits which may not be documented. The related subreddits for &lt;a href=&quot;https://www.reddit.com/r/buildapc&quot;&gt;/r/buildapc&lt;/a&gt; offer more places to get PC building advice.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/buildapc-related.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Related subreddits for sport-specific subreddits, like &lt;a href=&quot;https://www.reddit.com/r/cfb&quot;&gt;/r/cfb&lt;/a&gt; (college football) include the corresponding teams.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/cfb-related.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/food&quot;&gt;/r/food&lt;/a&gt; related subreddits list a surprising number of subreddits dedicated to specific foods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/food-related.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;There is a surprising amount of depth to the &lt;a href=&quot;https://www.reddit.com/r/me_irl&quot;&gt;/r/me_irl&lt;/a&gt; network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/me_irl-related.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The chart for &lt;a href=&quot;https://www.reddit.com/r/programming&quot;&gt;/r/programming&lt;/a&gt; can tell you which subreddits exist for specific programming languages and technologies.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/programming-related.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The methodology can also reveal a &lt;em&gt;lack&lt;/em&gt; of related subreddits, by the large contrast between subreddits with high relatedness and low relatedness. For example, while /r/cfb may have large numbers of obviously-related subreddits as a sports subreddit, &lt;a href=&quot;https://www.reddit.com/r/golf&quot;&gt;/r/golf&lt;/a&gt; has only 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/golf-related.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;You can view Related Subreddit charts for the Top 200 Subreddits &lt;a href=&quot;https://github.com/minimaxir/subreddit-related/tree/master/related&quot;&gt;in this GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Finding Similar Subreddits&lt;/h2&gt;

&lt;p&gt;Another method for finding related subreddits would be to find subreddits with similar communities. An academic approach to finding similarity between sets is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard Index&lt;/a&gt;. Using the same set A and set B definitions above, the formula now becomes:&lt;/p&gt;

&lt;p&gt;(A ∩ B) / [(A) + (B) - (A ∩ B)]&lt;/p&gt;

&lt;p&gt;which outputs the Jaccard Index, between 0 and 1. This formula only requires a few tweaks to the original code. The results from this computation tell a different story.&lt;/p&gt;

&lt;p&gt;Here are the most-similar subreddits to /r/aww:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/aww-jaccard-nondefault.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;In this implementation, the &lt;a href=&quot;https://www.reddit.com/r/defaults/comments/4l3svc/list_of_default_subreddits_usa_26_may_2016/&quot;&gt;default Reddit subreddits&lt;/a&gt; must be removed from the results, as the communities of default subreddits are largely similar to most others by design. Even former defaults like &lt;a href=&quot;https://www.reddit.com/r/adviceanimals&quot;&gt;/r/adviceanimals&lt;/a&gt; and &lt;a href=&quot;https://www.reddit.com/r/technology&quot;&gt;/r/technology&lt;/a&gt; still have large amounts of holdout users which skew the results. As &lt;a href=&quot;https://www.reddit.com/r/aww&quot;&gt;/r/aww&lt;/a&gt; is a mass-appeal subreddit, it makes sense that the communities are similar to other mass-appeal subreddits. &lt;/p&gt;

&lt;p&gt;The magnitude of the Jaccard Index measures the strength of the similarity. Most subreddit relationships have a low Jaccard Index, but the relative magnitude between all subreddit neighbors illustrate comparisons for potential related subreddits regardless (this is also the reason why the x-axis is not fixed across plots). The subreddit relationship with the highest absolute similarity is &lt;a href=&quot;https://www.reddit.com/r/arrow&quot;&gt;/r/arrow&lt;/a&gt; and &lt;a href=&quot;https://www.reddit.com/r/flashtv&quot;&gt;/r/flashtv&lt;/a&gt; at 0.345, which make sense given the massive overlap between the two CW television shows.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/arrow-jaccard-nondefault.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The Jaccard Index is more useful for finding similar subreddits to niche subreddits. Let&amp;rsquo;s try a few of the subreddits mentioned previously and see how the results changed.&lt;/p&gt;

&lt;p&gt;/r/buildapc is a niche, and the output identifies well-established subreddits, unlike with the previous related-subreddit methodology.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/buildapc-jaccard-nondefault.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The subreddit most similar to /r/cfb (college football) is &lt;a href=&quot;https://www.reddit.com/r/collegebasketball&quot;&gt;/r/collegebasketball&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/cfb-jaccard-nondefault.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The subreddit most similar to /r/food is &lt;a href=&quot;https://www.reddit.com/r/cooking&quot;&gt;/r/cooking&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/food-jaccard-nondefault.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The subreddit most similar to /r/programming is &lt;a href=&quot;https://www.reddit.com/r/linux&quot;&gt;/r/linux&lt;/a&gt;! (of course)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-related-subreddits/programming-jaccard-nondefault.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;You can view the Similar Subreddit charts for the Top 200 Subreddits &lt;a href=&quot;https://github.com/minimaxir/subreddit-related/tree/master/similar&quot;&gt;in this GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Again, Reddit has significantly better internal data for identifying user activity between subreddits, such as voting patterns and clickthrough tracking. But the results shown using these two set methodologies are pretty good for using public data. In fact, these two set approaches can theoretically work with &lt;em&gt;any&lt;/em&gt; set of categorized, settable data, which may give me a few ideas for new blog posts in the future.&lt;/p&gt;

&lt;p&gt;And there&amp;rsquo;s still the fancy machine learning approaches to try.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;As always, the full code used to process the comment data and generate the visualizations is available in &lt;a href=&quot;https://github.com/minimaxir/subreddit-related/blob/master/find_related_subreddits.ipynb&quot;&gt;this Jupyter notebook&lt;/a&gt;, open-sourced &lt;a href=&quot;https://github.com/minimaxir/subreddit-related&quot;&gt;on GitHub&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you do find any other interesting trends in the related/similar charts of other subreddits and write about it, it would be greatly appreciated if proper attribution is given back to this post and/or myself. Thanks!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 20 Jun 2016 08:20:00 -0700</pubDate>
        <link>http://minimaxir.com/2016/06/reddit-related-subreddits/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/06/reddit-related-subreddits/</guid>
        
        
        <category>Visualization</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Interactive Salary/Equity Chart for Jobs Offered in the San Francisco Bay Area</title>
        <description>&lt;p&gt;There has been a lot of talk about the high salaries paid to engineers in Silicon Valley and the San Francisco Bay Area. Why not take a quick look at the amount of money being offered to prospective employees?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://angel.co&quot;&gt;AngelList&lt;/a&gt; is a social network for startups and startup employees to network professionally. There is &lt;a href=&quot;https://angel.co/jobs&quot;&gt;also a section&lt;/a&gt; where startups can &lt;a href=&quot;https://angel.co/david-174/jobs/72943-engineer&quot;&gt;make job postings&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sfba-compensation/senior.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Unusually for job sites, AngelList requires posting compensation information for jobs, with a range of offered salary and equity for the position. (Note: due to their cash-strapped nature, early-stage startups may underpay candidates relative to market/experience).&lt;/p&gt;

&lt;p&gt;Using the &lt;a href=&quot;https://angel.co/api&quot;&gt;AngelList API&lt;/a&gt;, I scraped all publicly-available full-time jobs located in the San Francisco Bay Area. I retrieved 5,941 jobs total, with half of those for Engineer/Developer positions.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s plot Salary vs. Equity to get a 2D range of the different types compensation (by inferring a coordinate by taking the midpoint of each offered range; e.g. for an $80k—$125k salary offer, we plot Y=$102.5k). We can also control on Engineers vs. Non-Engineers since those salaries will likely be different in distribution.&lt;/p&gt;

&lt;p&gt;Putting it all together:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sfba-compensation/engineer-sfba-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;This is the same chart as the interactive one at the beginning of the article; if you haven&amp;rsquo;t, navigate around that chart, particularly looking at the  changes in job titles as you move upward with increasing salary/equity for a given facet.&lt;/p&gt;

&lt;p&gt;A few quick observations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The black horizontal lines represent the &lt;strong&gt;median salary&lt;/strong&gt; offered for each facet. For Engineers, the median salary offered is &lt;strong&gt;$115k&lt;/strong&gt;, and for Non-Engineers, the median salary offered is &lt;strong&gt;$90k&lt;/strong&gt;. (as mentioned before, early-stage startups pay lower and are more prominent on AngelList, so these results may be affected by selection bias).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Median equity&lt;/strong&gt;, on the other hand, is negligible and not worth plotting (&lt;strong&gt;0.10%&lt;/strong&gt; for Engineers, &lt;strong&gt;0.02%&lt;/strong&gt; for Non-Engineers).&lt;/li&gt;
&lt;li&gt;Speaking of equity, startups do not give much, with the distribution topping out at about 1.0% in both cases. (Engineers appear to get a little more equity, though).&lt;/li&gt;
&lt;li&gt;However, for serious roles, 7.5% equity (from offering 0%—15% or 5%—10%) is a figure that appears relatively often.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;As always, the code used to create the visualizations is available in &lt;a href=&quot;https://github.com/minimaxir/sfba-compensation/blob/master/angelist_sfbayarea_jobs.ipynb&quot;&gt;this Jupyter notebook&lt;/a&gt;, and the code used to scrape the AngelList data is open-sourced &lt;a href=&quot;https://github.com/minimaxir/sfba-compensation&quot;&gt;on GitHub&lt;/a&gt;. This post is my first test of using &lt;a href=&quot;https://plot.ly&quot;&gt;plot.ly&lt;/a&gt; to &lt;a href=&quot;http://moderndata.plot.ly/ggplot2-docs-completely-remade-in-d3-js/&quot;&gt;automatically convert ggplot2 plots&lt;/a&gt; into fully-interactive &lt;a href=&quot;https://d3js.org&quot;&gt;D3.js&lt;/a&gt; charts. The results were better than I expected (aside from the font-size issues specific to faceting), so expect to see more interactive charts in my future posts!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you do find any other interesting trends in the chart and write about it, it would be greatly appreciated if proper attribution is given back to this post and/or myself. Thanks!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 31 May 2016 08:00:00 -0700</pubDate>
        <link>http://minimaxir.com/2016/05/sfba-compensation/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/05/sfba-compensation/</guid>
        
        
        <category>Visualization</category>
        
        <category>Data</category>
        
        <category>Interactive</category>
        
      </item>
    
      <item>
        <title>How to Create a Network Graph Visualization of Reddit Subreddits</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;&gt;Network graphs&lt;/a&gt; are pretty data visualizations, and I like pretty data visualizations. Recently, &lt;a href=&quot;https://www.reddit.com&quot;&gt;Reddit&lt;/a&gt; user CuriousGnu &lt;a href=&quot;http://www.curiousgnu.com/reddit-comments&quot;&gt;posted a network graph&lt;/a&gt; of the comment patterns of the top 50 Reddit subreddits:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/rd_comments_net_hd.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/comments/4fsrjd/oc_redditors_who_commented_in_rx_also_commented/&quot;&gt;visualization&lt;/a&gt; was made with &lt;a href=&quot;https://gephi.org&quot;&gt;Gephi&lt;/a&gt;, a very popular free and open-source network graph tool.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/gephi.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Gephi is &lt;em&gt;extremely&lt;/em&gt; difficult to use, and most blog posts  about the software are in the form of Step 1: Gephi, Step 2: ???, Step 3: Profit. Even if you know &lt;em&gt;do&lt;/em&gt; how to use it, most of the network design customizations must be done manually, which is not helped by software slowness even on high-end machines. My own attempts to use Gephi for nice-looking networks have had &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/comments/3z60z6/network_of_reddit_commenting_patterns_for_the_top/&quot;&gt;mixed&lt;/a&gt; &lt;a href=&quot;https://www.reddit.com/r/magicTCG/comments/401hdq/graph_network_of_magic_the_gathering_creature/&quot;&gt;results&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Additionally, there is very little discussion on how to gather the data for large-scale network graph visualizations, and how to make them in a &lt;em&gt;reproducible&lt;/em&gt; manner. It is time to fix that and create a Reddit network graph visualization with many more nodes, step by step.&lt;/p&gt;

&lt;h2&gt;Getting Reddit Edge Data&lt;/h2&gt;

&lt;p&gt;Network graphs are typically formed by getting the relationship data between two entities (the edges), then extrapolating the vertices of the graph (the nodes) from that data.&lt;/p&gt;

&lt;p&gt;There are two common data structures for representing edge data. One is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Adjacency_matrix&quot;&gt;adjacency matrix&lt;/a&gt;, which is a 2D matrix where the rows/columns represent the entities, and the value at the intersection between a row/column represents the &lt;em&gt;weight&lt;/em&gt; of the relationships. For the visualization above, CuriousGnu made an adjacency matrix by querying the relationships from &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt; for each subreddit manually. That requires adding a line of SQL for &lt;em&gt;each&lt;/em&gt; subreddit you want to plot, which is time-consuming and I am lazy.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try option #2: an &lt;a href=&quot;https://reference.wolfram.com/language/ref/EdgeList.html&quot;&gt;edge list&lt;/a&gt;, which is a tabular dataset where each row contains the two entities and a weight. With clever use of BigQuery, we can query the edges for &lt;em&gt;every single subreddit&lt;/em&gt; at the same time. And we can query on real-time Reddit data from approximately the past 6 months using Jason Baumgartner&amp;rsquo;s &lt;a href=&quot;https://pushshift.io/using-bigquery-with-reddit-data/&quot;&gt;Reddit dataset&lt;/a&gt; on BigQuery. &lt;/p&gt;

&lt;p&gt;The process works like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Determine active users of a subreddit by identifying the subreddits where a user has &lt;strong&gt;commented&lt;/strong&gt; on at least &lt;strong&gt;5 different submissions&lt;/strong&gt; within the past 6 months.&lt;/li&gt;
&lt;li&gt;Perform a &lt;a href=&quot;http://stackoverflow.com/questions/3362038/what-is-self-join-and-when-would-you-use-it&quot;&gt;self-join&lt;/a&gt; by joining the table on itself: this will create &lt;strong&gt;links&lt;/strong&gt; between all subreddits where a given user is active. (e.g. an active user of /r/askreddit, /r/pics, and /r/gifs will form 9 links: askreddit → askreddit, askreddit → pics, askreddit → gifs, pics → askreddit, etc.)&lt;/li&gt;
&lt;li&gt;Aggregate the counts of the number of links between two subreddits; this will become the edge &lt;strong&gt;Weight&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Filter the resulting dataset by removing self-loops and reverse-edges. (e.g. since we have askreddit → pics, remove pics → askreddit). Additionally, we should only retain edges with &lt;strong&gt;at least 200 active users&lt;/strong&gt; to keep the resulting dataset a manageable size for this analysis.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Putting it all together results in this query:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l_subreddit&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l_subreddit&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Weight&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LOWER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l_subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;DISTINCT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;link_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_threads&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pushshift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rt_reddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l_subreddit&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;HAVING&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;JOIN&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LOWER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l_subreddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;DISTINCT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;link_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_threads&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pushshift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rt_reddit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l_subreddit&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;HAVING&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;author&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;author&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Target&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;HAVING&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Source&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Target&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Weight&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Only 13 lines of code, with 3 of those lines repeated. Running the query only takes a few minutes. (which is actually &lt;em&gt;forever&lt;/em&gt; in BigQuery time: when people talk about &amp;ldquo;big data,&amp;rdquo; this is &lt;em&gt;actually big data&lt;/em&gt;!)&lt;/p&gt;

&lt;p&gt;That query (at the time of analysis) returns &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1MFHno-sYR3MkWgntnieWobWQ2e3x4CAcNUdVIFjmlQI/edit?usp=sharing&quot;&gt;this dataset&lt;/a&gt; of 7,498 edges; more than enough. Now for the fun part.&lt;/p&gt;

&lt;h2&gt;Visualizing the Reddit Data&lt;/h2&gt;

&lt;p&gt;The edge list linked above can actually be imported into Gephi as-is. &lt;strong&gt;Don&amp;rsquo;t&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Instead, let&amp;rsquo;s use R and my favorite data visualization tool &lt;code&gt;ggplot2&lt;/code&gt;, with a twist.&lt;/p&gt;

&lt;p&gt;First, we load the edge list into R, and create an undirected network graph using the &lt;code&gt;igraph&lt;/code&gt; package.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;net &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; graph.data.frame&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; directed&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/igraph.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The imported edge list results in a network with 1,131 nodes/subreddits. After pruning nodes with only a few neighbors and removing the subsequently-orphaned edges, we get a network of 517 nodes/subreddits with 6,732 edges.&lt;/p&gt;

&lt;p&gt;We can then add summary statistics for the nodes, such as the group/community each node belongs to, and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Centrality&quot;&gt;eigenvector centrality&lt;/a&gt; of the node.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;V&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;net&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;group &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; membership&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;cluster_walktrap&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;net&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; weights&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;E&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;net&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;Weight&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
V&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;net&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;centrality &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; eigen_centrality&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;net&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; weights&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;E&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;net&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;Weight&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;vector&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Convert the network to a dataframe suitable for plotting using the &lt;code&gt;ggnetwork&lt;/code&gt; library.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_net &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ggnetwork&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;net&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; layout &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;fruchtermanreingold&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; weights&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Weight&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; niter&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;50000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now time for ggplot2/ggnetwork fun. In this case, we will color the nodes whether or not they are a default subreddit (orange if default, blue otherwise) and color the lines accordingly (orange if either end is a default subreddit, blue otherwise).&lt;/p&gt;

&lt;p&gt;Yes, writing and optimizing all of this code is &lt;em&gt;significantly&lt;/em&gt; easier than using Gephi, believe it or not.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;default_colors&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;#3498db&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;#e67e22&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
default_labels&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Not Default&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Default&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df_net&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; x&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; y&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; xend &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; xend&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; yend &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; yend&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; size &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; centrality&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    geom_edges&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;color &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; connectDefault&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    geom_nodes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;fill &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; defaultnode&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; shape &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; stroke&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;black&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    geom_nodelabel_repel&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;df_net&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;color &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; defaultnode&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; label &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; vertex.names&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                          fontface &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;bold&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; box.padding &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; unit&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;lines&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                          label.padding&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; unit&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;lines&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; segment.size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; label.size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    scale_color_manual&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;values&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default_colors&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; labels&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default_labels&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; guide&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    scale_fill_manual&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;values&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default_colors&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; labels&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default_labels&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    ggtitle&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Network Graph of Reddit Subreddits (by @minimaxir)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    scale_size&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;range&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; 
    theme_blank&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;object data=&quot;/img/reddit-graph/subreddit-1.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;400px&quot;&gt;&lt;/object&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;hidden-lg&quot;&gt;&lt;em&gt;If you are on a smartphone or tablet, tap &lt;a href=&quot;/img/reddit-graph/subreddit-1.pdf&quot; target=&quot;_blank&quot;&gt;this link&lt;/a&gt; to view the network in a zoomable format.&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The large networks in the blog post are rendered as a PDF, which allows for easy pan/zooming at a very low file size (284KB!), while SVG/&lt;a href=&quot;https://d3js.org&quot;&gt;d3&lt;/a&gt;/&lt;a href=&quot;http://sigmajs.org&quot;&gt;sigma.js&lt;/a&gt; approaches have very poor performance at large numbers of nodes/edges.&lt;/p&gt;

&lt;p&gt;As we expect, the default subreddits are in the center of the network graph and have high centrality (although /r/art and /r/earthporn are oddly far separated from the other defaults). The large amounts of orange graph-wide illustrate the breadth of the defaults.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s color the nodes and edges by group, just as you saw in the introductory visualization:&lt;/p&gt;

&lt;p&gt;&lt;object data=&quot;/img/reddit-graph/subreddit-2.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;400px&quot;&gt;&lt;/object&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;hidden-lg&quot;&gt;&lt;em&gt;If you are on a smartphone or tablet, tap &lt;a href=&quot;/img/reddit-graph/subreddit-2.pdf&quot; target=&quot;_blank&quot;&gt;this link&lt;/a&gt; to view the network in a zoomable format.&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;If an edge links to a node of the same group, the edge is colored that group. Otherwise, the edge is colored gray. (the code that implements this is not shown because it is somewhat convoluted). This color scheme helps gauge the overall impact of the communities on Reddit. But why not look at specific groups?&lt;/p&gt;

&lt;h2&gt;Subgraph Surprises&lt;/h2&gt;

&lt;p&gt;As you can see plainly in the group-colored visualization, there is a giant green group at the center which includes the default subreddits. Analyzing that is not helpful. But we can filter the network on other specific groups and their subgraphs to see if we can define any Reddit subcultures. (note that the Group number is merely an ID; the value and order are not relevant).&lt;/p&gt;

&lt;p&gt;The most notable Reddit groups are gaming groups. We have two distinct groups of gamers:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/group-006.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/group-008.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Plus Nintendo gamers? With a little Vita on the side?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/group-010.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Subreddits related to sports and sporting teams form a nice cluster:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/group-001.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;PC-building has a distinct community:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/group-011.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The British make nice triangles!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/group-022.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Relationship and female-oriented subreddits have a relationship. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/group-007.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Lastly, DC Comics has their own sector, particularly with the corresponding CW television shows. (although some Marvel shows sneak in!)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-graph/group-003.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Of course, Reddit itself has better data for identifying relationships between subreddits, as they can track user activity more intimately. Meanwhile, the output for this post turned out better than expected and I hope to include similar visualizations in future blog posts. Hopefully, it dispelled some of the mystery behind pretty network graphs. (if you do use the code or data visualization designs from this post, it would be greatly appreciated if proper attribution is given back to this post and/or myself. Thanks!).&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;As always, the full code used to process the edge list and generate the visualizations is available in &lt;a href=&quot;https://github.com/minimaxir/reddit-graph/blob/master/subreddit_network_pdf.ipynb&quot;&gt;this Jupyter notebook&lt;/a&gt;, open-sourced &lt;a href=&quot;https://github.com/minimaxir/reddit-graph&quot;&gt;on GitHub&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Additionally, thanks to Professor James P. Curley of Columbia University for providing &lt;a href=&quot;http://curleylab.psych.columbia.edu/netviz/netviz1.html#/&quot;&gt;helpful slides&lt;/a&gt; which have good code samples for getting started with igraph/ggnetwork.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 27 May 2016 08:00:00 -0700</pubDate>
        <link>http://minimaxir.com/2016/05/reddit-graph/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/05/reddit-graph/</guid>
        
        
        <category>Visualization</category>
        
        <category>Data</category>
        
        <category>Interactive</category>
        
      </item>
    
      <item>
        <title>Creating Stylish, High-Quality Word Clouds Using Python and Font Awesome Icons</title>
        <description>&lt;p&gt;You&amp;rsquo;ve probably seen word clouds around the internet. There are several popular free tools for creating them, such as &lt;a href=&quot;http://www.wordle.net&quot;&gt;Wordle&lt;/a&gt;. I myself am a fan of them, and I have made them for previous posts using the &lt;a href=&quot;http://www.inside-r.org/packages/cran/wordcloud/docs/wordcloud&quot;&gt;wordcloud package&lt;/a&gt; for R.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/gif-unlimited/reddit-gif-wordcloud-e.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Word clouds are not the most scientific type of data visualization. However, they are a very &lt;em&gt;information-dense&lt;/em&gt; representation of the frequency of all words in a given text. Word clouds are more effective than just using bar charts displaying the counts of words for large amounts of text, as the chart would be difficult to parse if there are too many bars.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/amueller/word_cloud&quot;&gt;Python word_cloud package&lt;/a&gt; by Andreas Mueller is relatively popular. A Reddit bot &lt;a href=&quot;https://github.com/Winneon/makeswordclouds&quot;&gt;makeswordclouds&lt;/a&gt; by Jesse Bryan automatically &lt;a href=&quot;https://www.reddit.com/user/makeswordcloudsagain&quot;&gt;generates a word cloud&lt;/a&gt; of comments on &lt;a href=&quot;https://www.reddit.com&quot;&gt;Reddit&lt;/a&gt; submissions using this package. However, when I first saw the example output on the package, I was not impressed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/wordclouds/constitution.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;I did more research into the &lt;a href=&quot;http://amueller.github.io/word_cloud/&quot;&gt;package documentation&lt;/a&gt;. I found that there are two important perks present the Python implementation:&lt;/p&gt;

&lt;p&gt;1) Python word_cloud allows the user to specify a mask to constrain the distribution of words.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/wordclouds/a_new_hope_1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;2) In addition to the mask, Python word_cloud allows the user to use the original colors of the image to set the colors of the words.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/wordclouds/colored_2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The masks are the more interesting aspect for creating visualizations, but &lt;em&gt;where&lt;/em&gt; do you get the masks? You can manually trace and extract objects from images, but that can be time consuming and the masks will likely be heavily aliased and at a low resolution (the size of the mask sets the size of the word cloud).&lt;/p&gt;

&lt;p&gt;Enter &lt;a href=&quot;https://fortawesome.github.io/Font-Awesome/&quot;&gt;Font Awesome&lt;/a&gt;, an icon font by Dave Gandy which is &lt;em&gt;very&lt;/em&gt; widely used throughout the Internet (including this website). Icon fonts contain a wide variety of shapes and are vectorized, and therefore they can scale to any size.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/wordclouds/fa_icons.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;So why not use Font Awesome icons as masks for the word cloud? The font icons need to be extracted and rasterized as an image in order to be usable with the Python word_cloud package: cue the Python script &lt;a href=&quot;https://github.com/Pythonity/icon-font-to-png&quot;&gt;Icon Font to PNG&lt;/a&gt; by Pythonity which does what the name implies.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/wordclouds/fa_icons_finder.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Now &lt;em&gt;every&lt;/em&gt; Font Awesome icon can be used as a word cloud mask! And the icons can be exported at any size: for this post, I render the word clouds at &lt;strong&gt;2048x2048px&lt;/strong&gt;, larger than most desktop screens! After hacking the Python scripts included with the package which were used to create the default word clouds, I managed to create a few interesting examples.&lt;/p&gt;

&lt;h2&gt;Reddit Data and Thematic Icons&lt;/h2&gt;

&lt;p&gt;Font Awesome has &lt;a href=&quot;http://fortawesome.github.io/Font-Awesome/icon/line-chart/&quot;&gt;icons for charts&lt;/a&gt;, which logically appeals to me as a data person. Why not make a word cloud which looks like a line chart?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s use the word counts of titles of submissions to the &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/&quot;&gt;/r/dataisbeautiful subreddit&lt;/a&gt; on Reddit which have scored at least 100 points (using the &lt;a href=&quot;https://bigquery.cloud.google.com/table/fh-bigquery:reddit_posts.full_corpus_201512&quot;&gt;Reddit data dump&lt;/a&gt; located on &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Additionally, we can improve on the design of the default word cloud output by forcing all-caps text and by changing the text font. For word clouds, I prefer to use condensed font families, as they can allow for more information to be displayed in the word cloud. In this example, I will be using the &lt;a href=&quot;https://www.myfonts.com/fonts/paratype/din-condensed/&quot;&gt;DIN Condensed&lt;/a&gt; font, a font native to OS X and a font you&amp;rsquo;ve likely seen in media advertisements and website logos.&lt;/p&gt;

&lt;p&gt;Putting it all together:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/dataisbeautiful_wordcloud.png&quot;&gt;&lt;img src=&quot;/img/wordclouds/dataisbeautiful_wordcloud.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(All word clouds in this post are shrunk to 600x600px to reduce loading time: click on the image for the full 2048x2048px resolution.)&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;GitHub Data and Brand Icons&lt;/h2&gt;

&lt;p&gt;Font Awesome also contains icons representing the logos of popular internet brands, such as Facebook and Twitter.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com&quot;&gt;GitHub&lt;/a&gt; is another such website. I will use BigQuery again with the &lt;a href=&quot;https://bigquery.cloud.google.com/table/githubarchive:year.2014&quot;&gt;2014 GitHub Archive dataset&lt;/a&gt; to gather word counts of git commit messages during that year, and use the modern GitHub logo as the mask. This time, I will incorporate the freeware condensed monospaced font &lt;a href=&quot;https://www.fontsquirrel.com/fonts/M-1m&quot;&gt;M+ 1m&lt;/a&gt; in order to create a more code-like aesthetic, which creates an interesting look when juxtaposed with the negative space of GitHub&amp;rsquo;s logo.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/github_wordcloud.png&quot;&gt;&lt;img src=&quot;/img/wordclouds/github_wordcloud.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Yelp Data and Sentiment Icons&lt;/h2&gt;

&lt;p&gt;One of the &lt;a href=&quot;http://minimaxir.com/2014/09/one-star-five-stars/&quot;&gt;earliest word clouds I made&lt;/a&gt; was for the &lt;a href=&quot;http://www.yelp.com&quot;&gt;Yelp&lt;/a&gt; reviews dataset from the &lt;a href=&quot;https://www.yelp.com/dataset_challenge&quot;&gt;Yelp Dataset Challenge&lt;/a&gt; to compare and contrast verbiage between 1-star reviews and 5-star reviews. Let&amp;rsquo;s remake those word clouds.&lt;/p&gt;

&lt;p&gt;At this point I should mention appropriate color palettes for word clouds since the  rainbows of the stereotypical word clouds can be distracting. I strongly recommend using the &lt;a href=&quot;http://colorbrewer2.org&quot;&gt;ColorBrewer&lt;/a&gt; palettes, helpfully provided for this use case with the &lt;a href=&quot;https://github.com/jiffyclub/palettable&quot;&gt;paletteable Python library&lt;/a&gt; by Matt Davis. I particularly like the &lt;a href=&quot;https://jiffyclub.github.io/palettable/colorbrewer/sequential/&quot;&gt;sequential palettes&lt;/a&gt;, which follow a clean gradient between white and another color (or between two or three colors), although I ignore some of the lighter colors as they may not be visible against white backgrounds.&lt;/p&gt;

&lt;p&gt;The font choice this time is &lt;a href=&quot;https://www.google.com/fonts/specimen/Open+Sans+Condensed&quot;&gt;Open Sans Condensed&lt;/a&gt;, a Google Font. &lt;a href=&quot;https://www.google.com/fonts&quot;&gt;Google Fonts&lt;/a&gt; are free and open source. I strongly recommend using them for documents/websites to add some flair over default fonts.&lt;/p&gt;

&lt;p&gt;Using the &lt;strong&gt;Greens&lt;/strong&gt; palette and a smiley-face Font Awesome icon on 5-star reviews:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/yelp_pos_wordcloud.png&quot;&gt;&lt;img src=&quot;/img/wordclouds/yelp_pos_wordcloud.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Makes sense, although the thin lines of the smiley-face causes the font sizes to become constrained. How about the inverse: &lt;strong&gt;Reds&lt;/strong&gt;, thumbs-down, and 1-star reviews?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/yelp_neg_wordcloud.png&quot;&gt;&lt;img src=&quot;/img/wordclouds/yelp_neg_wordcloud.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Facebook Data and Etc. Icons&lt;/h2&gt;

&lt;p&gt;I recently updated my &lt;a href=&quot;https://github.com/minimaxir/facebook-page-post-scraper&quot;&gt;Facebook Page Data Scraper&lt;/a&gt;, which gathers public posts made by Facebook Pages, to now retrieve total Reaction counts on those posts instead of just Likes.&lt;/p&gt;

&lt;p&gt;Why not create a word cloud of news headlines to get a zeitgeist of popular discussion? To do this, I scraped all the public posts from &lt;a href=&quot;https://www.facebook.com/cnn/&quot;&gt;CNN&amp;rsquo;s Facebook page&lt;/a&gt;, and created a word cloud of all CNN headlines to which the posts link. Let&amp;rsquo;s use the Google Font  &lt;a href=&quot;https://www.google.com/fonts/specimen/Amatic+SC&quot;&gt;Amatic SC&lt;/a&gt; which you&amp;rsquo;ve likely seen before in ads, and let&amp;rsquo;s try a &lt;a href=&quot;https://jiffyclub.github.io/palettable/colorbrewer/qualitative/&quot;&gt;qualitative palette&lt;/a&gt;, &lt;strong&gt;Dark2&lt;/strong&gt;, to get a &amp;ldquo;rainbow&amp;rdquo; effect without looking gawdy.&lt;/p&gt;

&lt;p&gt;And use a flag icon, because why not?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/cnn_wordcloud.png&quot;&gt;&lt;img src=&quot;/img/wordclouds/cnn_wordcloud.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Of course, CNN has had fun with the 2016 U.S. Presidential election.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s do one more word cloud. We have not yet done a word cloud using the colors-from-original-image technique. Using the underlying &lt;a href=&quot;http://matplotlib.org/examples/color/colormaps_reference.html&quot;&gt;matplotlib color map&lt;/a&gt; for the &lt;strong&gt;Spectral&lt;/strong&gt; palette, we can overlay a spatial rainbow which determines the color of the words displayed at that area of the mask:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/wordclouds/spectral.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Additionally, in order to estimate the importance of the presence of each word in generating Reactions, we can create a word cloud of the words where the words are sized not by count, but by the &lt;strong&gt;average number of Reactions&lt;/strong&gt; on Facebook posts referencing CNN headlines containing that word (where the word is used in at least 20 headlines for some statistical correction). And let&amp;rsquo;s try a black background.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/cnn_wordcloud_reactions.png&quot;&gt;&lt;img src=&quot;/img/wordclouds/cnn_wordcloud_reactions.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;While the word cloud cannot be output in a vectorized format using this method, creating a word cloud at a super-high resolution (even larger than 2048x2048px) is more-than-enough for making typical wall posters and t-shirts.&lt;/p&gt;

&lt;p&gt;Word clouds may not have as much explanatory value in the academic sense, but they have &lt;em&gt;persuasive&lt;/em&gt; power, which is just as important. At the least, it&amp;rsquo;s another visual technique in my fun bag of visualization tricks to spice up future blog posts. &lt;/p&gt;

&lt;p&gt;Performing postprocessing on rendered word clouds can help create especially artsy art, but that discussion best-saved for &lt;a href=&quot;https://raw.githubusercontent.com/minimaxir/stylistic-word-clouds/master/wordclouds/starry_night_cnn_weight_12_iterations_500_smooth_5.png&quot;&gt;another time&lt;/a&gt;.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;You can view the scripts to create the word clouds in this posts in this &lt;a href=&quot;https://github.com/minimaxir/stylistic-word-clouds&quot;&gt;GitHub repository&lt;/a&gt;; the code is more hacky than usual, but it should be clear enough to demonstrate how the raw data was processed in each instance and how the word clouds were rendered. In the future, I hope to create a &lt;a href=&quot;http://flask.pocoo.org&quot;&gt;Flask app&lt;/a&gt; based on these scripts to streamline the creation of word clouds.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 09 May 2016 08:00:00 -0700</pubDate>
        <link>http://minimaxir.com/2016/05/wordclouds/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/05/wordclouds/</guid>
        
        
        <category>Visualization</category>
        
      </item>
    
      <item>
        <title>Blockbuster Movies with Male Leads Earn More Than Those with Female Leads</title>
        <description>&lt;p&gt;One of the more interesting revelations discovered during the &lt;a href=&quot;https://en.wikipedia.org/wiki/Sony_Pictures_Entertainment_hack&quot;&gt;2014 Sony Pictures Entertainment hack&lt;/a&gt; was that actresses &lt;a href=&quot;https://en.wikipedia.org/wiki/Jennifer_Lawrence&quot;&gt;Jennifer Lawrence&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Amy_Adams&quot;&gt;Amy Adams&lt;/a&gt; made &lt;a href=&quot;http://www.thedailybeast.com/articles/2014/12/12/exclusive-sony-hack-reveals-jennifer-lawrence-is-paid-less-than-her-male-co-stars.html&quot;&gt;less money than their male costars&lt;/a&gt; for the movie &lt;a href=&quot;http://www.imdb.com/title/tt1800241/&quot;&gt;American Hustle&lt;/a&gt;. Specifically, Lawrence and Adams earned 7% of the profits while their male co-stars earned 9%: a 28% increase in pay.&lt;/p&gt;

&lt;p&gt;That made me curious: is the discrepancy in pay between male-leads and female-leads justifiable? Do movies with male lead actors generate more box office revenue than movies with female leads? Are movies with male leads &lt;em&gt;better&lt;/em&gt; than those with female leads?&lt;/p&gt;

&lt;p&gt;Using movie data from &lt;a href=&quot;http://www.omdbapi.com&quot;&gt;OMDb API&lt;/a&gt;, which is sourced from &lt;a href=&quot;http://www.imdb.com&quot;&gt;IMDb&lt;/a&gt; and &lt;a href=&quot;http://www.rottentomatoes.com&quot;&gt;Rotten Tomatoes&lt;/a&gt; data, I found that on average, blockbuster movies with male leads generate 22% more domestic box office revenue than those with female leads, and that this difference is statistically significant.&lt;/p&gt;

&lt;h2&gt;Setting Up the Movie Data&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-data.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve talked about processing the OMDb dataset in &lt;a href=&quot;http://minimaxir.com/2016/04/trust-but-verify/&quot;&gt;my previous post&lt;/a&gt;. For this analysis, I&amp;rsquo;ll be filtering on a specific subset of movies:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Movies with &lt;strong&gt;at least $10 million in inflation-adjusted domestic box office revenue&lt;/strong&gt;. My &lt;a href=&quot;http://minimaxir.com/2016/01/movie-revenue-ratings/&quot;&gt;first analysis&lt;/a&gt; showed that there is a distinct cluster of movies above the $10M threshold specifically. These blockbusters are also what the public knows and best reflects the perception of the industry.&lt;/li&gt;
&lt;li&gt;Movies which were &lt;strong&gt;released in 2000 or later&lt;/strong&gt;. There was &lt;a href=&quot;http://minimaxir.com/2016/04/trust-but-verify/&quot;&gt;missing box office revenue data&lt;/a&gt; I had found with earlier years so I would prefer to use more robust data to be safe. Additionally, this avoids the complicated issue of &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/comments/4bcb6x/john_goodman_is_not_the_greatest_supporting_actor/d17y82k&quot;&gt;20th-century gender politics in cinema&lt;/a&gt;, which I cannot easily address statistically.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After applying the filters and cleaning the data further to eliminate miscoded movies, I have created a dataset of 2,020 movies. No movies were removed as outliers in box office revenue (such as Star Wars VII and Avatar) since several tests failed to identify them as statistical outliers.&lt;/p&gt;

&lt;p&gt;I identified the lead actor of each movie, using the first credited actor on the IMDb cast overview (NB: this may lead to counterintuitive behavior in casts with unknown leads; the first credited actor for &lt;a href=&quot;http://www.imdb.com/title/tt2488496/&quot;&gt;Star Wars: The Force Awakens&lt;/a&gt; on IMDb is Harrison Ford, who is not the lead and I corrected it to Daisy Ridley in the data). Then I determined their gender by referencing a few gender/first-name mappings (with thanks to &lt;a href=&quot;https://twitter.com/matthew_daniels&quot;&gt;Matt Daniels&lt;/a&gt; and his great work on &lt;a href=&quot;http://polygraph.cool/films/index.html&quot;&gt;gender and film dialogue&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In all, the dataset has 467 (23%) of movies with a female lead actor, and 1,553 (77%) movies with a male lead actor. Both counts are more than enough for this analysis.&lt;/p&gt;

&lt;p&gt;You can view and download the final dataset &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1UMV-6yCjHBveyOcZwiilEm2DWRMjdzAbgdHutcjCn-E/edit?usp=sharing&quot;&gt;in this Google Sheet&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Distribution of Box Office Revenue&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with simple histograms of the box office data. What are the distributions of the data for each gender?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;On average, blockbuster movies with male lead actors generate &lt;strong&gt;$79.8M in revenue&lt;/strong&gt;. The distribution, even when log-scaled, is skewed right, with the median being much lower at $49.8M.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;On average, blockbuster movies with female lead actors generate &lt;strong&gt;$65.6M in revenue&lt;/strong&gt;. The general shape of the distribution is the same as with male lead actors.&lt;/p&gt;

&lt;p&gt;Double-checking the math:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;79.8M / 65.6M = 22% increase in average box office revenue for male-lead movies
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So it is.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s overlay the two distributions after normalizing and smoothing with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_density_estimation&quot;&gt;2D kernel destiny estimator&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Female movies have a clear mode near its average, but male movies have a flatter distribution, with significantly more movies making 9 figures.&lt;/p&gt;

&lt;p&gt;But is the difference between the two averages statistically significant? We can run two statistical tests between the box-office revenues of male-led and female-lead movies:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&quot;&gt;Kolmogorov–Smirnov test&lt;/a&gt; for determining if two populations have the same distribution. The null hypothesis is that the two are drawn from the same distribution; the alternative hypothesis is that the distributions are different. We reject the null hypothesis at the 95% level in favor of the alternative if the p-value of the test is less than 0.05. Running the test, &lt;strong&gt;p &amp;lt; 0.01&lt;/strong&gt;, so we can say the distributions are statistically different. (the p-values are the same whether the box office revenues are log-transformed or not)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&quot;&gt;Wilcoxon rank-sum test&lt;/a&gt; for determining if the means (averages) of two populations are the same (null hypothesis) or different (alternative hypothesis). This test is used instead of a &lt;em&gt;t&lt;/em&gt;-test if the populations are not Normally distributed, which is the case here. Running the test (one-sided, since only checking if a mean is greater), &lt;strong&gt;p &amp;lt; 0.01&lt;/strong&gt;, so we can say the two means are statistically different. (again, the p-values are the same whether the box office revenues are log-transformed or not)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So we have statistical evidence that male-lead movies generate more money on average than female-led movies. But this claim is very serious, and as a result, we need even more proof.&lt;/p&gt;

&lt;h2&gt;The Resampling&lt;/h2&gt;

&lt;p&gt;Although 2,020 movies is a fair sample size by statistical standards, some may argue that the movies were chosen too arbitrarily and that there is not enough data to support the conclusions I make above. Enter &lt;a href=&quot;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)&quot;&gt;bootstrap resampling&lt;/a&gt;, in which we resample the data randomly (with replacement) to generate pseudo-datasets, and then calculate aggregate statistics (e.g. the average) on that simulated dataset. Repeat a large number of times, and we can form confidence intervals for the &lt;em&gt;true average&lt;/em&gt; of a given data set.&lt;/p&gt;

&lt;p&gt;In this case, we resample the box office revenues, calculate the average box office revenue for both male-led and female-led movies, store the result away and resample the data again, and keep repeating until satisfied.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an animation of the resampling of both averages as the number of trials increases. As you can see, the shape of both distributions stabilize very quickly:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie_frames.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;And the final plot, at 10,000 repetitions:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-10.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The dot on the bottom of each distribution represents the &lt;em&gt;actual&lt;/em&gt; sample average value calculated during the analysis earlier, while the line range represents a 95% confidence interval for the true average revenue value for each gender. The distribution of male-led movie averages is more narrow than female-led movies because there are 3 times as many male-led movies in the dataset.&lt;/p&gt;

&lt;p&gt;As you can see, &lt;em&gt;the line ranges never intersect&lt;/em&gt;. Even in the most favorable scenario at the 95% confidence level, the average  domestic box office revenue for male-led movies will be greater. Specifically, of the 10,000 trials, only 2 trials had the case where female-led movies had equal or greater average revenue than the corresponding male-led movie revenue average from the same resampling; this implies &lt;strong&gt;p &amp;lt; 0.01&lt;/strong&gt; for the statistical test on whether the means are same or different.&lt;/p&gt;

&lt;p&gt;Interestingly, there&amp;rsquo;s a little overlap between in the distributions, which occurs when there are multiple instances of Star Wars VII in the resampled dataset and its high box office revenue pushes the &lt;em&gt;entire&lt;/em&gt; female-lead average up very significantly.&lt;/p&gt;

&lt;h2&gt;Gender and Quality&lt;/h2&gt;

&lt;p&gt;It is also worth checking if male-led movies are &lt;em&gt;better in quality&lt;/em&gt; than female-lead movies, as if that&amp;rsquo;s the case, it might provide a more logical explanation why male-led movies make more money.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s check out the distribution of &lt;a href=&quot;http://www.rottentomatoes.com&quot;&gt;Rotten Tomatoes&lt;/a&gt; Tomatometer scores of blockbuster movies.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s no obvious difference. Female-led movies are about 2% points lower on average, but the general distribution is the same (&lt;a href=&quot;https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)&quot;&gt;uniform&lt;/a&gt;). Overlaying the two distributions shows as such.&lt;/p&gt;

&lt;p&gt;The difference in averages is &lt;em&gt;not&lt;/em&gt; statistically significant, as both the Kolmogorov–Smirnov test and the Wilcoxon rank-sum test fail to reject the null hypothesis at the 95% level (&lt;strong&gt;p = 0.37&lt;/strong&gt; and &lt;strong&gt;p = 0.13&lt;/strong&gt; respectively).&lt;/p&gt;

&lt;p&gt;How about &lt;a href=&quot;http://www.metacritic.com&quot;&gt;Metacritic&lt;/a&gt; scores?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-8.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-gender/movie-gender-9.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Again, distributions are the same (both take on the shape of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Normal_distribution&quot;&gt;Normal distribution&lt;/a&gt;, interestingly). And again, the Kolmogorov–Smirnov test and the Wilcoxon rank-sum test fail. (&lt;strong&gt;p = 0.45&lt;/strong&gt; and &lt;strong&gt;p = 0.14&lt;/strong&gt; respectively).&lt;/p&gt;

&lt;p&gt;The quality of a movie is independent from the gender of the lead actor in determining the financial performance of a movie.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In 2014, &lt;a href=&quot;http://www.mpaa.org/wp-content/uploads/2015/03/MPAA-Theatrical-Market-Statistics-2014.pdf&quot;&gt;according to the MPAA&lt;/a&gt;, the gender breakdown of moviegoers who see blockbuster movies is about 50/50, eliminating another potential explanation for the average movie revenue discrepancy.&lt;/p&gt;

&lt;p&gt;Granted, there still can be more work done, such as controlling on movie Genre in addition to gender. Is the gender of the lead actor a &lt;em&gt;causal&lt;/em&gt; factor in a movie&amp;rsquo;s success? Not necessarily, and this analysis does not assert such. But there definitely is a revenue disparity that&amp;rsquo;s worth investigating, and it&amp;rsquo;s not just that &amp;ldquo;male-led movies are better.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;There may be more movies like Star Wars Episode VII where a movie with a female lead can hit almost a billion dollars domestically (e.g. Star Wars VIII). Things are looking upward, and it would not surprise me if the 22% revenue difference decreases and disappears in the next decade.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;You can view the code used to process the data and generate the data visualizations &lt;a href=&quot;https://github.com/minimaxir/movie-gender/blob/master/movie_gender.ipynb&quot;&gt;in this Jupyter notebook&lt;/a&gt;, &lt;a href=&quot;https://github.com/minimaxir/movie-gender&quot;&gt;open-sourced on GitHub&lt;/a&gt;, or you can &lt;a href=&quot;https://github.com/minimaxir/movie-gender/raw/master/movie_gender_pdf.pdf&quot;&gt;view as a PDF&lt;/a&gt; which is better if you are on a mobile device.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You are free to use the charts from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Apr 2016 08:00:00 -0700</pubDate>
        <link>http://minimaxir.com/2016/04/movie-gender/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/04/movie-gender/</guid>
        
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>The Importance of Sanity-Checking Datasets Before Analysis</title>
        <description>&lt;p&gt;I&amp;rsquo;ve done some cool things with movie data using a dataset from &lt;a href=&quot;http://www.omdbapi.com&quot;&gt;OMDb API&lt;/a&gt;, which is sourced from &lt;a href=&quot;http://www.imdb.com&quot;&gt;IMDb&lt;/a&gt; and &lt;a href=&quot;http://www.rottentomatoes.com&quot;&gt;Rotten Tomatoes&lt;/a&gt; data. In my &lt;a href=&quot;http://minimaxir.com/2016/01/movie-revenue-ratings/&quot;&gt;previous article&lt;/a&gt; on the dataset, I plotted the relationship between the domestic box office revenue of movies and their Rotten Tomatoes scores.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/movie-revenue-ratings/box-office-rating-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;I want to take another look at domestic Box Office Revenues with aggregate statistics such as means/medians on categorical variables such as MPAA rating and release month. For this type of analysis in particular, I&amp;rsquo;ll also need to implement code in &lt;a href=&quot;https://www.r-project.org&quot;&gt;R&lt;/a&gt; for inflation adjustment.&lt;/p&gt;

&lt;p&gt;However, I ran into a few unexpectedly silly issues.&lt;/p&gt;

&lt;h2&gt;Seeing Double&lt;/h2&gt;

&lt;p&gt;There are many similarities between data validation and the Quality Assurance process of product development, which is why this particular area appeals to me personally as a Software QA Engineer. Whenever a cool dataset is released publicly, I play around with it to look for any obvious flaws and to get a good all-around benchmark on the robustness of the data (this is a separate procedure from the traditional &amp;ldquo;data cleaning&amp;rdquo; phase necessary to begin quantification on some poorly-structured datasets).&lt;/p&gt;

&lt;p&gt;Do the extreme values in the data make sense? Is the data encoded in a sane format? Are there any obvious gaps or logical contradictions in summary representations of the data, especially when compared to other canonical sources? &lt;/p&gt;

&lt;p&gt;These concerns are also some of the reasons I&amp;rsquo;ve switched to the &lt;a href=&quot;http://jupyter.org&quot;&gt;Jupyter Notebook&lt;/a&gt; as my primary data science IDE. After each block of code which transforms data, I can print the data frame inline to immediately see the results of the code execution, and refer back to them if anything odd happens in the future.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say I have a data frame of Movies using the latest data dump (3/26/16) from OMDb. This data set contains 1,160,273 movies, including both IMDb and Rotten Tomatoes data. After cleaning the data (not shown), I can use the R package &lt;code&gt;dplyr&lt;/code&gt; by Hadley Wickham to sort the data frame by Box Office Revenue descending, and print the &lt;code&gt;head&lt;/code&gt; (top) of the data.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-R&quot; data-lang=&quot;R&quot;&gt;&lt;span class=&quot;kp&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; select&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;imdbID&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; Title&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; Year&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; BoxOffice&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; arrange&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;desc&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;BoxOffice&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; n &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/trust-but-verify/data-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Those movies being the best &lt;em&gt;makes sense&lt;/em&gt;. For &lt;a href=&quot;http://www.rottentomatoes.com/m/star_wars_episode_vii_the_force_awakens/&quot;&gt;Star Wars: The Force Awakens&lt;/a&gt;, I can compare it to the Box Office reported on the corresponding Rotten Tomatoes page, which in turn matches the &lt;a href=&quot;http://www.boxofficemojo.com/movies/?id=starwars7.htm&quot;&gt;domestic Box Office Revenue&lt;/a&gt; on &lt;a href=&quot;http://www.boxofficemojo.com&quot;&gt;Box Office Mojo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But wait, &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Dark_Knight_%28film%29&quot;&gt;The Dark Knight&lt;/a&gt; appears &lt;em&gt;twice&lt;/em&gt;? How?!&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s no way I would have missed something this obvious during the sanity-check for my previous article. In order to make sure that I&amp;rsquo;m not going insane, I double-checked the December 2015 data dump I used for that post, derived the top movies with the same methodology for the modern data dump, and the duplicate movies &lt;em&gt;were not present&lt;/em&gt;. Weird.&lt;/p&gt;

&lt;p&gt;There are 2 different IDs for
The Dark Knight, and for some other movies near the top (&lt;a href=&quot;http://www.imdb.com/title/tt4817264/&quot;&gt;Inside Out&lt;/a&gt;, &amp;ldquo;&lt;a href=&quot;http://www.imdb.com/title/tt3138972/&quot;&gt;The Gravity&lt;/a&gt;&amp;rdquo;). Fortunately, duplicate data like this is easy to debug. The second data entry for The Dark Knight has a greater IMDb ID (1774602) which means it was likely added to the site later. Let&amp;rsquo;s look up the &lt;a href=&quot;http://www.imdb.com/title/tt1774602/&quot;&gt;corresponding IMDb page&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/trust-but-verify/dark-knight.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Huh. Apparently someone put a filler movie entry with the same name and release year as a blockbuster movie in hopes that people search for it by accident (and since it received 50 ratings and an average score of 8.6, this tactic was successful).&lt;/p&gt;

&lt;p&gt;Using the Rotten Tomatoes &lt;a href=&quot;http://developer.rottentomatoes.com/docs/read/json/v10/Movie_Alias&quot;&gt;IMDb Lookup API&lt;/a&gt;, we find that &amp;ldquo;The Dark Knight&amp;rdquo; page on Rotten Tomatoes&amp;hellip;&lt;a href=&quot;http://api.rottentomatoes.com/api/public/v1.0/movie_alias.json?type=imdb&amp;amp;id=1774602&quot;&gt;doesn&amp;rsquo;t exist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We can run a safe deduplicate by removing entries with the same title (excluding the &amp;ldquo;The&amp;rdquo; if present) and release year.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_dup &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; select&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Title&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; Year&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Title &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;gsub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;The &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; Title&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
dup &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;duplicated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df_dup&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# find entry indices which are duplicates&lt;/span&gt;
&lt;span class=&quot;kp&quot;&gt;rm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df_dup&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# remove temp dataframe&lt;/span&gt;

df_dedup &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; filter&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;dup&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# keep entries which are *not* dups&lt;/span&gt;
&lt;span class=&quot;kp&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df_dedup &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; select&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;imdbID&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; Title&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; Year&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; BoxOffice&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; arrange&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;desc&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;BoxOffice&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; n &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/trust-but-verify/data-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;There we go! The de-duped dataset has 1,114,431 movies, impliying that there were 45,842 of these duplicate entries.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not sure &lt;em&gt;whose&lt;/em&gt; fault it is that duplicate movies suddenly became present in the data dump: OMDb or Rotten Tomatoes. &lt;em&gt;But it doesn&amp;rsquo;t matter&lt;/em&gt;: the wrong entries still need to be addressed, and it&amp;rsquo;s good to have a test case for the future too.&lt;/p&gt;

&lt;h2&gt;Inflation Station&lt;/h2&gt;

&lt;p&gt;A &lt;a href=&quot;http://stackoverflow.com/a/26068058&quot;&gt;Stack Overflow answer&lt;/a&gt; from &lt;a href=&quot;http://stackoverflow.com/users/1048757/brash-equilibrium&quot;&gt;Ben Hanowell&lt;/a&gt; has a good R implementation and rationale for implementing inflation adjustment using the &lt;a href=&quot;https://research.stlouisfed.org/fred2/data/CPIAUCSL.txt&quot;&gt;historical Consumer Price Index data&lt;/a&gt; from the &lt;a href=&quot;https://www.stlouisfed.org&quot;&gt;Federal Reserve Bank of St. Louis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Take the index for each year (averaging each month for simplicity) and create an adjustment factor to convert historical dollar amounts into present-day dollar amounts. Much better than plugging hundreds of thousands of values into an online calculator. Here&amp;rsquo;s the SO code made &lt;code&gt;dplyr&lt;/code&gt;-friendly for this purpose, with the requisite sanity-checks.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;inflation &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; read_csv&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;http://research.stlouisfed.org/fred2/data/CPIAUCSL.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Year &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;as.integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;substr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;DATE&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
                    summarize&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Avg_Value &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;VALUE&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# average across all months&lt;/span&gt;
                    mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Adjust &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;tail&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Avg_Value&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; Avg_Value&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# normalize by most-recent year&lt;/span&gt;

&lt;span class=&quot;kp&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;inflation &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;kp&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;inflation &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;tail&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/trust-but-verify/inf.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;For example, to get the inflation-adjusted Box Office Revenue for a movie released in 1949 in 2016 dollars, we multiply the reported revenue by 10. That sounds about right (and matches closely enough to the output of the &lt;a href=&quot;http://data.bls.gov/cgi-bin/cpicalc.pl?cost1=1&amp;amp;year1=1949&amp;amp;year2=2016&quot;&gt;Bureau of Labor Statistics inflation calculator&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Now map each inflation adjustment factor to each movie by merging the two datasets (on the &lt;code&gt;Year&lt;/code&gt; column), then multiply the Box Office revenue by the adjustment factor to get the inflation-adjusted revenue. Plus another sanity-check for good measure. &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_dedup_join &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_dedup &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; inner_join&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;inflation&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; mutate&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;AdjBoxOffice &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; BoxOffice &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; Adjust&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kp&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df_dedup_join &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; select&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Title&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; Year&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; AdjBoxOffice&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; arrange&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;desc&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;AdjBoxOffice&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; n&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/trust-but-verify/data-3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Uh-oh.&lt;/p&gt;

&lt;p&gt;I mean, &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Lorax_(TV_special)&quot;&gt;The Lorax&lt;/a&gt; probably earned $1.2 billion in VHS sales for Earth Day education &lt;em&gt;alone&lt;/em&gt;, but the TV special was never released in theaters. There was a &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Lorax_(film)&quot;&gt;CGI remake of The Lorax&lt;/a&gt; a few years ago which was reasonably popular. Could it be that someone at Rotten Tomatoes or Box Office Mojo confused the two media?&lt;/p&gt;

&lt;p&gt;That is exactly what happened. On Rotten Tomatoes, The &lt;a href=&quot;http://www.rottentomatoes.com/m/the-lorax/&quot;&gt;1972 Lorax&lt;/a&gt; was encoded with similar box office revenue as the &lt;a href=&quot;http://www.rottentomatoes.com/m/the_lorax/&quot;&gt;2012 Lorax&lt;/a&gt;; then the inflation factor sextupled it. For this type of data fidelity issue, it&amp;rsquo;s considerably more obvious whose at fault.&lt;/p&gt;

&lt;p&gt;Unfortunately, that&amp;rsquo;s not the end of problems with the dataset. I compared my results with &lt;a href=&quot;http://www.vox.com/2016/4/4/11351788/batman-v-superman-terrible-reviews#undefined&quot;&gt;Vox&amp;rsquo;s dataset&lt;/a&gt; on worldwide historical box office revenues. In the Top 200 Movies by inflation-adjusted revenue, there are noted historical movie omissions such as &lt;a href=&quot;http://www.rottentomatoes.com/m/jaws/&quot;&gt;Jaws&lt;/a&gt; and &lt;a href=&quot;http://www.rottentomatoes.com/m/star_wars/&quot;&gt;Star Wars: A New Hope&lt;/a&gt;. It turns out Rotten Tomatoes does not have Box Office Revenue data for these movies at all. &lt;/p&gt;

&lt;p&gt;That is a very serious problem which I&amp;rsquo;ll have to think about if it blocks any analysis on aggregate box office data completely. In the end, sanity-checking third party data is important because you never know &lt;em&gt;how&lt;/em&gt; the data will surprise you, until it&amp;rsquo;s too late.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;You can view the Top 200 movies by domestic box office revenue for each of the 12/15 source dataset, the 3/16 dataset, the 3/16 deduped dataset, and the 3/16 deduced inflation-adjusted data &lt;a href=&quot;https://github.com/minimaxir/movie-data-sanity-checking&quot;&gt;in this GitHub repository&lt;/a&gt;, along with the Jupyter notebook.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Apr 2016 08:00:00 -0700</pubDate>
        <link>http://minimaxir.com/2016/04/trust-but-verify/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/04/trust-but-verify/</guid>
        
        
        <category>Idea</category>
        
      </item>
    
      <item>
        <title>Unlimited Data Storage Using Image Steganography and Cat GIFs</title>
        <description>&lt;p&gt;&lt;span&gt;&lt;style&gt;
.suspicious-pregnant-pause {
  padding: 60px 0;
}
&lt;/style&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In 2016, animated GIFs are absurdly popular. &lt;a href=&quot;https://www.messenger.com&quot;&gt;Facebook Messenger&lt;/a&gt;, &lt;a href=&quot;https://twitter.com&quot;&gt;Twitter&lt;/a&gt;, and &lt;a href=&quot;https://slack.com&quot;&gt;Slack&lt;/a&gt; now natively support GIF sharing, and startups which help facilitate GIF sharing such as &lt;a href=&quot;http://giphy.com&quot;&gt;Giphy&lt;/a&gt; and &lt;a href=&quot;https://www.riffsy.com&quot;&gt;Riffsy&lt;/a&gt; have received &lt;a href=&quot;https://www.crunchbase.com/organization/giphy&quot;&gt;&lt;em&gt;tens of millions&lt;/em&gt; of dollars&lt;/a&gt; in venture capital. I even built my own tool to &lt;a href=&quot;https://github.com/minimaxir/video-to-gif-osx&quot;&gt;convert a video to a GIF&lt;/a&gt; on OSX because &lt;a href=&quot;http://minimaxir.com/2015/08/gif-to-video-osx/&quot;&gt;I wanted to be cool&lt;/a&gt; too.&lt;/p&gt;

&lt;p&gt;On &lt;a href=&quot;https://www.reddit.com&quot;&gt;Reddit&lt;/a&gt;, GIFs are also popular for reactions, often containing submission titles with the phrase MRW (&amp;ldquo;my reaction when&amp;rdquo;). Here is a wordcloud of the titles of all GIF submissions to Reddit in 2015:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/gif-unlimited/reddit-gif-wordcloud-e.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Completely unrelated, &lt;a href=&quot;https://en.wikipedia.org/wiki/Steganography&quot;&gt;image steganography&lt;/a&gt; is a technique that allows the embedding of messages into the pixels of images themselves, allowing for the transmission of secret messages. Image steganography was popular on the internet in the early 2000&amp;rsquo;s, but died out before image-sharing between peers was commonplace and information encryption became a hot political issue.&lt;/p&gt;

&lt;h2&gt;It&amp;rsquo;s a Secret to Everybody&lt;/h2&gt;

&lt;p&gt;In 2012, Zach Oakes created &lt;a href=&quot;https://sekao.net/pixeljihad/&quot;&gt;PixelJihad&lt;/a&gt;, an &lt;a href=&quot;https://github.com/oakes/PixelJihad&quot;&gt;open-source&lt;/a&gt;, client-side tool for image steganography using JavaScript and the HTML5 Canvas. This implementation embeds text messages by converting them into raw binary and modifying the least-significant bit (Aaron Miller wrote &lt;a href=&quot;http://www.aaronmiller.in/thesis/&quot;&gt;an informative paper&lt;/a&gt; on LSB encoding) of the red, blue, and green binary pixel values of the specified image to match the message (e.g. a white pixel with red, blue, and green values of &lt;code&gt;[255, 255, 255]&lt;/code&gt; can be represented in binary form as &lt;code&gt;[11111111, 11111111, 11111111]&lt;/code&gt;; LSB encoding changes the right-most bit to 0 or 1 for each pixel channel). Changing the last bit of a color channel will not result in a detectable visual change by the human eye.&lt;/p&gt;

&lt;p&gt;The image pixels to-be-modified during encoding are selected according to a semirandom schedule. The message can then be decoded by reading the LSB of the changed pixels from the schedule and reconstructing the binary message, then converting back into text.&lt;/p&gt;

&lt;p&gt;Since we can set 3 bits per physical pixel, the maximum amount of data (in bytes) that can be stored is equivalent to the width of an image (in pixels) multiplied by the height times 3, divided by 8.&lt;/p&gt;

&lt;p&gt;Take the Reddit GIF wordcloud I posted above. If you download the image and load it into PixelJihad, you can see I hid a secret message showing how to get the Reddit word data using &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt;. The visualization is 600px by 400px, therefore it can store up to:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;600 * 400 * 3 bits / 8 = 90000 Bytes / 1000 = 90 kB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The image itself is 257 kB, but a bonus 35% in data storage isn&amp;rsquo;t bad.&lt;/p&gt;

&lt;p&gt;PixelJihad only supports text messages with an artificial limit of 1,000 characters (~1 kB) for &lt;em&gt;sanity&lt;/em&gt;, but I&amp;rsquo;m not fond of being sane. There is &lt;em&gt;nothing&lt;/em&gt; from a technical perspective stopping users from encoding larger text messages, or even raw file data as pure binary into images.&lt;/p&gt;

&lt;p&gt;But embedding data into a &lt;em&gt;static&lt;/em&gt; image is boring. What happens when you embed data into &lt;em&gt;each frame of a GIF&lt;/em&gt;? Just a thought.&lt;/p&gt;

&lt;h2&gt;Meow!&lt;/h2&gt;

&lt;p&gt;Now, back to what you&amp;rsquo;re reading this article for, who cares about all this stegosaurus crap. Here is a cute cat GIF &lt;a href=&quot;https://www.reddit.com/r/gifs/comments/256bir/abduction_cat_re_flying_cat_re_so_big/&quot;&gt;from Reddit&lt;/a&gt;. Look at its cuteness.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/gif-unlimited/aliencat.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Aww.&lt;/p&gt;

&lt;p&gt;The GIF file size is 2.44 MB (sorry, readers on mobile devices!). The dimensions of the GIF are 300px by 399px. But if you open the GIF in Preview on OSX, you can see that the GIF is comprised of 72 frames.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/gif-unlimited/gif-finder-17.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Performing the same math:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;300 * 399 * 72 frames * 3 / 8 = 3231900 / 1000000 = 3.23 MB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The potential amount of encodable data is larger than the file size of the image itself! Screw secret messages, we&amp;rsquo;re literally &lt;em&gt;creating&lt;/em&gt; data storage out of nothing!&lt;/p&gt;

&lt;p&gt;You know what this means? We can &lt;em&gt;embed the cat GIF into itself&lt;/em&gt; steganographically by splitting it over all 72 frames, and get 780 kB left over to store whatever we want.&lt;/p&gt;

&lt;p&gt;And then we can &lt;em&gt;do it again&lt;/em&gt; to the newly-embedded image.&lt;/p&gt;

&lt;p&gt;Infinitely.&lt;/p&gt;

&lt;p&gt;780 kB free with each iteration.&lt;/p&gt;

&lt;p&gt;Cat GIF recursion.&lt;/p&gt;

&lt;h2&gt;Purrcursion&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s say we want to embed a 10 MB music file into a GIF. Since 10000 kB / 780 kB = 12.82, we re-embed the cat GIF 13 times, including 1/13th of the music file as sidecar data with each iteration. And now we have a cat GIF that contains some pretty sweet tunes!&lt;/p&gt;

&lt;p&gt;And not only do we have a magical cat GIF, the data is completely portable; you can simply e-mail the cat GIF to your friend and they can decode the high-fidelity music file out of the GIF using an app that implements this schema. Or you can post the cat GIF w/ music to Reddit for &lt;em&gt;double&lt;/em&gt; meaningless internet points.&lt;/p&gt;

&lt;p&gt;The only limitation to the &amp;ldquo;infinitely&amp;rdquo; part is the increasing amount of CPU processing power for each decoding, but that&amp;rsquo;s fixable since it&amp;rsquo;s 2016 and smartphones have quad-core processors. Worth every ounce of battery life, in my opinion.&lt;/p&gt;

&lt;p&gt;*&lt;em&gt;tl;dr *&lt;/em&gt; I was bored and decided to create infinite data in a way that makes people feel fuzzy inside. No big deal. Of course, I can&amp;rsquo;t show &lt;em&gt;you&lt;/em&gt; a proof-of-concept of how this works. That would be silly. But if you&amp;rsquo;re a venture capitalist who wants to invest more tens of millions of dollars into a clearly-sustainable GIF startup, feel free to email me at &lt;a href=&quot;mailto:max@minimaxir.com&quot;&gt;max@minimaxir.com&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;suspicious-pregnant-pause&quot;&gt;&amp;hellip;&lt;/div&gt;

&lt;div class=&quot;suspicious-pregnant-pause&quot;&gt;&amp;hellip;&lt;/div&gt;

&lt;div class=&quot;suspicious-pregnant-pause&quot;&gt;&amp;hellip;&lt;/div&gt;

&lt;p&gt;Yeah, infinite data via image steganography doesn&amp;rsquo;t actually work. But the reasons it doesn&amp;rsquo;t work are not obvious unless you have deep knowledge of how image compression reduces the file size of images, and conversely, how modifying images could cause them to become &lt;em&gt;larger&lt;/em&gt; in file size  (I may or may not have spent several hours forking PixelJihad and adding modern features before realizing that I am not a data storage magician).&lt;/p&gt;

&lt;p&gt;I will write a follow-up post soon detailing the steganographic gotchas. Until then, keep in mind that there is no such thing as a free cat GIF.&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Mar 2016 08:00:00 -0700</pubDate>
        <link>http://minimaxir.com/2016/03/gif-unlimited/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/03/gif-unlimited/</guid>
        
        
        <category>Idea</category>
        
        <category>Comedy</category>
        
        <category>Genius</category>
        
      </item>
    
      <item>
        <title>Facebook Reactions and the Problems With Quantifying Likes Differently</title>
        <description>&lt;p&gt;Facebook added &lt;a href=&quot;http://newsroom.fb.com/news/2016/02/reactions-now-available-globally/&quot;&gt;Facebook Reactions&lt;/a&gt;, allowing users to do more than just &amp;ldquo;Like&amp;rdquo; posts and statuses as they have done for the past decade. Likes were the universal symbol of approval on social media. Now, Facebook users can apply more granular responses, from positive emotions like &lt;strong&gt;Love&lt;/strong&gt;, to negative emotions such as &lt;strong&gt;Angry&lt;/strong&gt;. This was widely believed to be Facebook&amp;rsquo;s compromise instead of adding a Dislike button.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/facebook_react.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Of course, there&amp;rsquo;s an ulterior motive. The use of reactions provides organic data on the sentiment of a status, which is helpful for numerous marketing and statistical applications. As &lt;a href=&quot;http://www.buzzfeed.com/alexkantrowitz/facebook-reactions-launch-today&quot;&gt;BuzzFeed notes&lt;/a&gt;, Facebook ads may be able &amp;ldquo;to write one product message for someone who mostly uses &lt;strong&gt;Sad&lt;/strong&gt; and another who mostly uses &lt;strong&gt;Wow&lt;/strong&gt; or &lt;strong&gt;Love.&lt;/strong&gt;&amp;rdquo;&lt;/p&gt;

&lt;p&gt;However, this isn&amp;rsquo;t the first time a big social network has tried implementing reactions alongside Likes/Dislikes. Four years ago, YouTube added &lt;a href=&quot;http://googlesystem.blogspot.com/2011/06/youtube-reactions.html&quot;&gt;Reaction buttons&lt;/a&gt; to their comments section:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/youtube-reactions.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;and removed them sometime after without fanfare, replacing it with the simple Like/Dislike bar.&lt;/p&gt;

&lt;p&gt;Presumably, YouTube implemented the buttons for the similar reason as Facebook. What makes things different now, if anything?&lt;/p&gt;

&lt;h2&gt;A Quantitative Approach to Feeling&lt;/h2&gt;

&lt;p&gt;Even after YouTube&amp;rsquo;s failure, another data-driven website implemented reaction buttons: BuzzFeed (who else?). At the end of each article (in most categories), registered users can select a quirky reaction to indicate how they felt about the article.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/buzzfeedreactions.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The heart represents &lt;strong&gt;Love&lt;/strong&gt; internally and is by-far the most-used reaction on BuzzFeed posts. When I started scraping BuzzFeed data in 2014 &lt;a href=&quot;http://minimaxir.com/2015/01/linkbait/&quot;&gt;to analyze clickbait&lt;/a&gt;, I made sure to grab the reaction data of other reactions as well to see if there are any interesting trends or correlations between reactions. A cursory glance at the scraped reaction data revealed a problem that forced me to disregard it.&lt;/p&gt;

&lt;p&gt;An important part of variable selection for analysis and modeling is avoiding &lt;em&gt;redundant&lt;/em&gt; features, as that can cause issues such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Multicollinearity&quot;&gt;multicollinearity&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Overfitting&quot;&gt;overfitting&lt;/a&gt;. For Facebook, avoiding adding redundant Reactions was an &lt;a href=&quot;https://medium.com/facebook-design/reactions-not-everything-in-life-is-likable-5c403de72a3f&quot;&gt;explicit design goal&lt;/a&gt; of the feature, but the positive emotions such as &lt;strong&gt;Like&lt;/strong&gt; and &lt;strong&gt;Wow&lt;/strong&gt; might be overly similar regardless (I believe it fair to compare the behavior of BuzzFeed users with the average Facebook user, given that they hit the same demographics). Do BuzzFeed readers use specific positive reactions differently? Did they use specific negative reactions?&lt;/p&gt;

&lt;p&gt;I rechecked my 2014 data in light of Facebook Reactions. The scraped dataset contains reaction data from 9,883 BuzzFeed articles in the Celebrity, Animals, Books, Longform, and Business categories. From that, I made a &lt;a href=&quot;http://vita.had.co.nz/papers/gpp.pdf&quot;&gt;pairs plot&lt;/a&gt; for the counts of all the &lt;em&gt;positive&lt;/em&gt; reactions on the articles to illustrate all bivariate relationships:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The lower half of the pairs plot is a scatterplot for the two reactions; the axes represent the number of votes for a given reaction on a BuzzFeed article (both axes are scaled logarithmically), color intensity indicates the number of articles at that X/Y combo, and the line is a linear trendline of least-squares.&lt;/li&gt;
&lt;li&gt;The diagonal of the pairs plot represents the density distribution of reaction vote counts for that reaction. (also logarithmically scaled on the X axis)&lt;/li&gt;
&lt;li&gt;The upper half of the pairs plot illustrates the Pearson correlation between the non-log quantities of the two reaction variables. The stars represent statistical significance of the correlation test; since the data set is large, all correlations are statistically significant (rejection of null hypothesis of no correlation) at p &amp;lt; 0.001.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/buzzfeed-pos.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;All of the bivariate correlations of positive reactions are &lt;em&gt;moderately or strongly positively correlated&lt;/em&gt;, which is problematic for analysis (except one: apparently, there is little statistical relationship between things that are cute and things that make you go YAAASS). So why not just use the &lt;strong&gt;Love&lt;/strong&gt; reaction, since articles tend to get about 100 Loves, while other reactions get around 10?&lt;/p&gt;

&lt;p&gt;Does the same hold for negative reactions? Relatedly, we would also expect a negative correlation between the number of &lt;strong&gt;Love&lt;/strong&gt; reactions and negative reactions, right?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/buzzfeed-neg.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;All negative reactions are positively correlated, as expected, but there is a weak &lt;em&gt;positive&lt;/em&gt; correlation between &lt;strong&gt;Love&lt;/strong&gt; and &lt;strong&gt;Hate&lt;/strong&gt;, which is definitely not right. There isn&amp;rsquo;t an ideal &amp;ldquo;negative&amp;rdquo; reaction, since all have similar distributions.&lt;/p&gt;

&lt;p&gt;Why does Facebook have 6 different responses to gauge positivity or negativity when one reaction for each would be both more accurate and more intuitive for the user?&lt;/p&gt;

&lt;h2&gt;Conceal, Don&amp;rsquo;t Feel&lt;/h2&gt;

&lt;p&gt;There are other qualitative issues with Facebook&amp;rsquo;s current implementation of Reactions. Apparently, Likes and Reactions are treated &lt;em&gt;differently internally&lt;/em&gt;. As a result, you get separate notifications for Likes and Reactions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/facebook_react2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Why? No idea. There is enough Notification spam on Facebook, I don&amp;rsquo;t need &lt;em&gt;double notifications&lt;/em&gt; in my Notification feed for every status I make.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s important to note is that a user cannot both Like and React to a status; only one or the other. As a result, the number of Likes on statuses overall will drop, and this is a &lt;em&gt;major&lt;/em&gt; problem for businesses who are dependent on measuring the number of Likes for engagement.&lt;/p&gt;

&lt;p&gt;I took a look at the Facebook Graph API endpoint for &lt;a href=&quot;https://developers.facebook.com/docs/graph-api/reference/v2.5/post&quot;&gt;Facebook Page Posts&lt;/a&gt; (same endpoint I use for my &lt;a href=&quot;https://github.com/minimaxir/facebook-page-post-scraper&quot;&gt;Facebook Page Data Scraper&lt;/a&gt;), and I can confirm that the API can only report the number of Likes on a status; not the number of Likes + Reactions, or number of Likes + number of each Reaction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/facebook-reactions/cnn_fb.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;There is no way currently to automate the retrieval of Reactions data from Facebook posts, which is an unfortunate oversight (especially considering how Twitter &lt;a href=&quot;https://blog.twitter.com/2015/hearts-for-developers&quot;&gt;handled the transition&lt;/a&gt; from Favorites to Likes easily).&lt;/p&gt;

&lt;p&gt;The example &lt;a href=&quot;https://www.facebook.com/cnn/posts/10154506885211509&quot;&gt;CNN story&lt;/a&gt; I used for that screenshot is anecdotally one of the very few examples I&amp;rsquo;ve noticed where the number of Likes is &lt;em&gt;almost equal&lt;/em&gt; to negative emotions, a relationship which should be weakly correlated and therefore this knowledge may be useful to isolate the story as unusual (and serve ads accordingly). At Facebook&amp;rsquo;s immense scale, identifying a relatively small proportion of unusual stories might be enough to justify adding Reactions.&lt;/p&gt;

&lt;p&gt;Or maybe this feature is just the harbinger of a new generation of emotionally-charged linkbait. Perhaps there is more to this Facebook Reactions data than what meets the eye, and I&amp;rsquo;ll update my scripts and do further statistical analysis when able. But given what has happened with Reactions data before with YouTube, I am unconvinced and I still believe the functionality as a whole is a usability regression that won&amp;rsquo;t last.&lt;/p&gt;

&lt;p&gt;A Dislike button would have been better, just saying.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;You can view the code and data used to generate the BuzzFeed Reaction data visualizations &lt;a href=&quot;https://github.com/minimaxir/facebook-reactions/blob/master/buzzfeed_reactions.ipynb&quot;&gt;in this Jupyter notebook&lt;/a&gt;, &lt;a href=&quot;https://github.com/minimaxir/facebook-reactions&quot;&gt;open-sourced on GitHub&lt;/a&gt;, or you can &lt;a href=&quot;https://github.com/minimaxir/facebook-reactions/raw/master/reactions_pdf.pdf&quot;&gt;view as a PDF&lt;/a&gt;, which is better if you are on a mobile device.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Feb 2016 08:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2016/02/facebook-reactions/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/02/facebook-reactions/</guid>
        
        
        <category>Rant</category>
        
      </item>
    
      <item>
        <title>You&#39;re Not Allowed to Criticize Startups, You Stupid Hater</title>
        <description>&lt;p&gt;A couple weeks ago, &lt;a href=&quot;https://itunes.apple.com/us/app/peach-a-space-for-friends/id1067891186?mt=8&quot;&gt;Peach&lt;/a&gt;, a messaging app, was released on iOS. Not only did Peach trend on Twitter on the day of release, but there was also an &lt;em&gt;unusual&lt;/em&gt; amount of media coverage for app, with both &lt;a href=&quot;http://techcrunch.com/2016/01/08/peach-is-a-slick-new-messaging-app-from-the-founder-of-vine/&quot;&gt;TechCrunch&lt;/a&gt; and &lt;a href=&quot;http://www.buzzfeed.com/katienotopoulos/do-i-dare-to-post-on-peach&quot;&gt;BuzzFeed&lt;/a&gt; posting glowing reviews for it, noting that the app is &amp;ldquo;slick&amp;rdquo; and &amp;ldquo;blowing up&amp;rdquo; respectively (commenters in the BuzzFeed article thought it was paid advertising). Usually, reports for apps from both sites are more neutral, which made me curious.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/startup-haters/peach-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;There are &lt;em&gt;already&lt;/em&gt; an absurd amount of messaging apps out there, and as a result, product differentiation is the primary value proposition. (e.g. security and &lt;a href=&quot;https://itunes.apple.com/us/app/telegram-messenger/id686449807?mt=8&quot;&gt;Telegram Messenger&lt;/a&gt;) What is Peach&amp;rsquo;s differentiation? &amp;ldquo;Magic Words&amp;rdquo;, apparently, which let you perform contextual actions with words! These Magic Words are implemented in a manner similar to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Command-line_interface&quot;&gt;command-line interface&lt;/a&gt; (CLI). &lt;a href=&quot;http://nymag.com/following/2016/01/how-the-command-line-became-mainstream-again.html&quot;&gt;NYMag&lt;/a&gt; goes into extreme detail about the feature. Nerds love CLIs, so you can tell the product was made by smart people!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/startup-haters/peach-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://news.ycombinator.com/item?id=10883698&quot;&gt;Hacker News thread&lt;/a&gt; about the NYMag article was interesting. One commenter (jobu) that the Magic Words are similar to those of &lt;a href=&quot;https://slack.com&quot;&gt;Slack&lt;/a&gt;, which allow for &amp;ldquo;/&amp;rdquo; commands (again, messaging app differentiation). A HN user (pbreit) argued in response:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You&amp;rsquo;re missing the whole thing. For starters, Slack is for work and Peach is for personal. But I think before you dismiss something as &amp;ldquo;isn&amp;rsquo;t it just x plus y?&amp;rdquo; you need to take a little bit more time and thinking to try and figure out what the product designers are trying to achieve and how.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Wait, what? Are the users of Peach supposed to be mind readers? Are people stupid for not understanding the purpose of Peach?&lt;/p&gt;

&lt;p&gt;Another user (thwarted) replied to that response with such:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Maybe the product designers and product providers should be more explicit about how their product should be used and who their target demographic is, if they want a successful product, rather than making everyone &amp;ldquo;figure out&amp;rdquo; what they are trying to achieve and how.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;pbreit replied:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Figuring it out&amp;rdquo; might be the point. You do realize that you are referring to some of the very few people who have actually already built an amazingly successful product.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I should have mentioned earlier that Peach was made by &lt;a href=&quot;http://byte.co&quot;&gt;Byte&lt;/a&gt;, which is ran by &lt;a href=&quot;https://twitter.com/dhof&quot;&gt;Dom Hofmann&lt;/a&gt;, a founder of Vine which was purchased by Twitter before launch (it also appears Peach is a pivot from the failed Byte app; something left out by the blog articles). Yes, if a founder has had a previously successful startup, the previous success implies a higher probability for future startup success than for a startup run by an unknown founder. However, it&amp;rsquo;s still not a 100% guarantee, and startups should not be immune to criticism because of asymmetric information between the users and the startup itself. Peach is impeachable. (pun intended)&lt;/p&gt;

&lt;p&gt;TechCrunch did a &lt;a href=&quot;http://techcrunch.com/2016/01/11/hype-or-not-peach-hit-the-top-10-social-networking-app-list-fast/&quot;&gt;follow-up article&lt;/a&gt; three days after release stating (paraphrased) &amp;ldquo;Peach is #9 in the App Store in the Social Networking category, despite the haters! Now they will be a success! Haters gonna hate!&amp;rdquo; Social networking is a double-edged sword in terms of network effects: they can grow incredibly fast as friends-of-friends register, and they can &lt;em&gt;die&lt;/em&gt; incredibly fast once they all realize the app is a fad. As of publishing, Peach is ranked #110 in the Social Networking category, and off-the-charts in Overall. And &lt;a href=&quot;https://www.appannie.com/apps/ios/app/peach-a-space-for-friends/rank-history/&quot;&gt;still dropping&lt;/a&gt;, with no apparent recovery.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/startup-haters/peach-3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;I seriously wish the tech media would admit they were wrong, but they never will. The startup world is in dire need of cautionary tales as valuations inflate to absurd levels.&lt;/p&gt;

&lt;p&gt;But hey, isn&amp;rsquo;t any startup ran by a previously-funded founder a sure thing? They know what it takes to create a company, even if it&amp;rsquo;s in a different and highly competitive environment years later than their first success! One success is all a founder needs.&lt;/p&gt;

&lt;p&gt;A startup&amp;rsquo;s release is unpolished and buggy? It&amp;rsquo;s just an MVP, and startups are expected to have issues out of the gate, so you have to forgive them! They don&amp;rsquo;t need QA engineers.&lt;/p&gt;

&lt;p&gt;A startup uses growth hacking to get early traction? It&amp;rsquo;s hustle, and they should be applauded for their creativity! Whoever complains is just in the minority, anyways.&lt;/p&gt;

&lt;p&gt;A startup gets a lot of points on Product Hunt? That&amp;rsquo;s validation, especially since it was submitted by one of the friends/investors of the startup for all their network to see and upvote! A strong personal network is the only thing you need for business success.&lt;/p&gt;

&lt;p&gt;A startup raises a Series A? That means venture capitalists did due diligence, and if the idea sucked, they would not have invested! Millions of dollars is a lot of money to those people.&lt;/p&gt;

&lt;p&gt;A startup loses traction and buzz? It&amp;rsquo;s not dead, it can come back as long as it has money in the bank! Your lack of faith demotivates entrepreneurs.&lt;/p&gt;

&lt;p&gt;A startup &lt;em&gt;dies&lt;/em&gt;? They can just try again! It was bad luck anyways and they likely did nothing wrong.&lt;/p&gt;

&lt;p&gt;A startup is mercy-killed through an acquisition/acquihire? Just another step in their incredible journey! The founders still win, and now they have the &amp;ldquo;exited&amp;rdquo; label to leverage for future employment.&lt;/p&gt;

&lt;p&gt;Sarcastic rhetorical questions aside, it is very annoying that it is considered socially unacceptable in Silicon Valley to criticize a startup. Even though the majority of startups fail, actually &lt;em&gt;saying so&lt;/em&gt; is sacrilegious. In Peach&amp;rsquo;s particular case, the fact that the app is from a previous exited founder should invite &lt;em&gt;more&lt;/em&gt; scrutiny instead of giving it a free pass.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: Fixed joke so that it is funny.&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Jan 2016 08:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2016/01/startup-haters/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/01/startup-haters/</guid>
        
        
        <category>Rant</category>
        
      </item>
    
      <item>
        <title>Video Games and Charity: Analyzing Awesome Games Done Quick 2016 Donations</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://gamesdonequick.com&quot;&gt;Awesome Games Done Quick&lt;/a&gt;, and its sister event Summer Games Done Quick, are a fundraising events that livestreams video game speedruns &lt;a href=&quot;http://www.twitch.tv/gamesdonequick/profile&quot;&gt;live on Twitch&lt;/a&gt; for charity. Beginning in January 2011, before Twitch was launched out from Justin.tv, &lt;a href=&quot;https://en.wikipedia.org/wiki/Awesome_Games_Done_Quick_and_Summer_Games_Done_Quick#List_of_marathons&quot;&gt;AGDQ was very small&lt;/a&gt; and only raised $52,519.83 for the &lt;a href=&quot;http://preventcancer.org&quot;&gt;Prevent Cancer Foundation&lt;/a&gt;; now, in 2016, from January 3rd to January 10th, AGDQ &lt;a href=&quot;https://gamesdonequick.com/tracker/index/agdq2016&quot;&gt;successfully raised&lt;/a&gt; about $1.2 &lt;em&gt;million&lt;/em&gt; for the charity.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/Speedrun&quot;&gt;speedrun&lt;/a&gt;, as the name suggests, is the process of completing a video game as fast as possible, optionally with self-imposed challenges to make things more interesting. Speedruns can emphasize extreme player skill and/or clever glitch abuse. And unexpected mistakes which make the results hilarious.&lt;/p&gt;

&lt;p&gt;One of the first runs of AGDQ 2016, &lt;a href=&quot;https://www.youtube.com/watch?v=jLlian3g7Gg&quot;&gt;Super Monkey Ball&lt;/a&gt;, demonstrates all of these. (run starts at 5:57)&lt;/p&gt;

&lt;div class=&quot;responsive-video&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/jLlian3g7Gg &quot; style=&quot;width: 100%; height: 100%; border: none; padding-bottom: 20px&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;AGDQ 2016 also has fun with the concept of speedrunning. One of the best events of AGDQ 2016 was a blind speedrun of user-created &lt;a href=&quot;https://www.youtube.com/watch?v=8qC584MWXO4&quot;&gt;Super Mario Maker&lt;/a&gt; levels from top designers, in which hilarity ensued. (run starts at 27:41)&lt;/p&gt;

&lt;div class=&quot;responsive-video&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/8qC584MWXO4 &quot; style=&quot;width: 100%; height: 100%; border: none; padding-bottom: 20px&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;It might be interesting to know &lt;em&gt;which&lt;/em&gt; video games lead to the achievement of over $1M donated to charity and the nature of the donations in general.&lt;/p&gt;

&lt;h2&gt;Gaming Data&lt;/h2&gt;

&lt;p&gt;With a few quick scripts on Kimono to scrape data from the &lt;a href=&quot;https://gamesdonequick.com/tracker/donations/agdq2016&quot;&gt;AGDQ 2016 donation page&lt;/a&gt; (+ a &lt;em&gt;lot&lt;/em&gt; of postprocessing in R!), I obtained a dataset of all 30,528 donations, their donors, when they donated, during what speedrun they donated, and &lt;em&gt;why&lt;/em&gt; they donated. (&lt;a href=&quot;https://docs.google.com/spreadsheets/d/1yyfkS0jvRK1cWrQesYiBn1TMGC93lo1MqahcU3XeGIU/edit?usp=sharing&quot;&gt;Google Sheets link&lt;/a&gt; for all the data)&lt;/p&gt;

&lt;p&gt;Here are the cumulative donations during AGDQ, color coded by day:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Cumulative donations were strong the entire run. On the second-to-last day, the donations rallied and increased exponentially, clearing $1M handily on the last day.&lt;/p&gt;

&lt;p&gt;The donation amount minimum is $5, but the average is significantly higher at $39.62. What is the distribution of donations?&lt;/p&gt;

&lt;p&gt;Here is a distribution of donations from $5 to $100 (for ease of visualization/interpretation), which account for 97% of all donations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The median donation amount is $20. What&amp;rsquo;s interesting is that donations occur at clear break points: not only are there many donations at multiple of $10, but there are many donations at $25 and $75 as well. The $50 and $75 points also potentially benefited for being the threshold for entry into a &lt;a href=&quot;https://gamesdonequick.com/tracker/prizes/agdq2016&quot;&gt;grand prize raffle&lt;/a&gt;. I&amp;rsquo;ll note off-chart that there is a spike in $1,000 donations, the threshold for the audience-clapping in celebration and the &lt;a href=&quot;https://gamesdonequick.com/tracker/donation/212588&quot;&gt;top single donation&lt;/a&gt; was made by an AGDQ sponsor, &lt;a href=&quot;https://www.theyetee.com/&quot;&gt;The Yetee&lt;/a&gt;, at $18,225. The &lt;a href=&quot;https://gamesdonequick.com/tracker/donation/209613&quot;&gt;top donation by a non-sponsor&lt;/a&gt; is from Minecraft creator Notch at $8,000, which he &lt;a href=&quot;https://gamesdonequick.com/tracker/donation/234071&quot;&gt;did twice&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Which games are the most popular and generated the most amount of money for the Prevent Cancer Foundation?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Unsurprisingly, Nintendo games are the most popular due to the nostalgia factor. In fairness, the top runs on this chart occur during the last two days of AGDQ 2016, which as mentioned previously may have been affected by a rally, so we cannot assert causality. The appearance of &lt;a href=&quot;http://yachtclubgames.com/shovel-knight/&quot;&gt;Shovel Knight&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Bloodborne&quot;&gt;Bloodborne&lt;/a&gt; as leading donation games, both relatively recently released, shows that speedrunning has more appeal than just retro games.&lt;/p&gt;

&lt;p&gt;A popular technique in charity drives is donation incentives, which help bolster the number of donations total. The &lt;a href=&quot;https://gamesdonequick.com/tracker/bids/agdq2016&quot;&gt;AGDQ bid incentives&lt;/a&gt; can include bonus game segments, or certain game decisions, such as what name to give to a main character.&lt;/p&gt;

&lt;p&gt;Any donation can optionally be assigned as a donation toward an incentive. Which run received the most money toward incentives?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Super Metroid donations incentives accounted for nearly &lt;em&gt;&amp;frac14;th&lt;/em&gt; of all the money raised at AGDQ. Final Fantasy IV accounted for a large amount as well, however, in both cases, the rally effect may apply. &lt;/p&gt;

&lt;p&gt;Speaking of the Super Metroid donation incentives, it should be noted that this particular incentive is one of the most culturally-important incentives in the show. Super Metroid has an optional objective to Save The Animals from planetary destruction, but this costs time, and time is important for a speedrun. Or the speedruner can Kill The Animals through inaction for efficiency, &amp;ldquo;Saving the Frames.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;What is the split of these incentive choices?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Yes, the animals were killed (specifically, they were &lt;strong&gt;REKT&lt;/strong&gt;, in the words of a last-minute donator). Both bonus games and vanity naming were popular, but nothing compared to the Save the Animals / Kill the Animals bid war.&lt;/p&gt;

&lt;p&gt;Lastly, people can leave comments with donations, and these comments are usually read on-stream when possible, as you&amp;rsquo;ve likely noticed if you&amp;rsquo;re watched the videos above.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a fun, nonscientific word cloud of those comments:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/agdq-2016/agdq-7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Lots of positivity, aside from the whole &amp;ldquo;Kill the Animals&amp;rdquo; thing. After all, the event is all about preventing cancer.&lt;/p&gt;

&lt;p&gt;While this post isn&amp;rsquo;t an academic analysis, it&amp;rsquo;s neat to see to see what kinds of things drive donation to charity. This model of livestreaming and charitable applications is very successful, and important given the renewed attention toward livestreaming with the rise of Twitch to mainstream attention, alongside the rise of personal streaming with apps like Periscope. Donation incentives are a &lt;em&gt;very&lt;/em&gt; successful technique for facilitating donations.&lt;/p&gt;

&lt;p&gt;It will be interesting to see if Twitch and events like AGDQ can leverage charitable livestreaming, or if another startup/organization beats them first. Bidding-Wars-for-Deciding-the-Fate-of-Fictional-Animals-as-a-Service has a nice ring to it.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;You can access a Jupyter notebook with the data processing and chart processing code &lt;a href=&quot;https://github.com/minimaxir/agdq-2016&quot;&gt;in this GitHub repository&lt;/a&gt;. If you use the processed donation data or visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks! :)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note for donation incentive statistics: the user can theoretically split their donation among multiple incentives; unfortunately I assumed at time of scrape that all the money could only go toward one bid. All donations I investigated from the source data were toward a single incentive except two donations from The Yetee which I fixed manually. If there are any data discrepancies, that is the likely cause.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 08:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2016/01/agdq-2016/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/01/agdq-2016/</guid>
        
        
        <category>Code</category>
        
      </item>
    
  </channel>
</rss>
